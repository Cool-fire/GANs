{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying BEGAN in Faces image\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/633jal/r170310717_began_boundary_equilibrium_generative/\n",
    "\n",
    "https://arxiv.org/pdf/1703.10717.pdf\n",
    "\n",
    "1. The generator wins by generating images that the discriminator can successfully autoencode with small loss.\n",
    "\n",
    "2. The discriminator wins by autoencoding real images well and by autoencoding the generated images poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Encoder\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb\n",
    "\n",
    "awesome python library\n",
    "https://pypi.python.org/pypi/tqdm\n",
    "\n",
    "https://github.com/carpedm20/BEGAN-tensorflow/blob/master/models.py\n",
    "\n",
    "https://arxiv.org/pdf/1703.10717.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library and Utils / Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import re\n",
    "import scipy.misc\n",
    "import scipy\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        path_dir, _ = os.path.split(path)\n",
    "        if not os.path.isdir(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "\n",
    "\n",
    "def session(graph=None, allow_soft_placement=True,\n",
    "            log_device_placement=False, allow_growth=True):\n",
    "    \"\"\" return a Session with simple config \"\"\"\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=allow_soft_placement,\n",
    "                            log_device_placement=log_device_placement)\n",
    "    config.gpu_options.allow_growth = allow_growth\n",
    "    return tf.Session(graph=graph, config=config)\n",
    "\n",
    "\n",
    "def tensors_filter(tensors, filters, combine_type='or'):\n",
    "    assert isinstance(tensors, (list, tuple)), '`tensors` shoule be a list or tuple!'\n",
    "    assert isinstance(filters, (str, list, tuple)), \\\n",
    "        '`filters` should be a string or a list(tuple) of strings!'\n",
    "    assert combine_type == 'or' or combine_type == 'and', \"`combine_type` should be 'or' or 'and'!\"\n",
    "\n",
    "    if isinstance(filters, str):\n",
    "        filters = [filters]\n",
    "\n",
    "    f_tens = []\n",
    "    for ten in tensors:\n",
    "        if combine_type == 'or':\n",
    "            for filt in filters:\n",
    "                if filt in ten.name:\n",
    "                    f_tens.append(ten)\n",
    "                    break\n",
    "        elif combine_type == 'and':\n",
    "            all_pass = True\n",
    "            for filt in filters:\n",
    "                if filt not in ten.name:\n",
    "                    all_pass = False\n",
    "                    break\n",
    "            if all_pass:\n",
    "                f_tens.append(ten)\n",
    "    return f_tens\n",
    "\n",
    "\n",
    "def trainable_variables(filters=None, combine_type='or'):\n",
    "    t_var = tf.trainable_variables()\n",
    "    if filters is None:\n",
    "        return t_var\n",
    "    else:\n",
    "        return tensors_filter(t_var, filters, combine_type)\n",
    "\n",
    "\n",
    "def summary(tensor_collection, summary_type=['mean', 'stddev', 'max', 'min', 'sparsity', 'histogram']):\n",
    "    \"\"\"\n",
    "    usage:\n",
    "    1. summary(tensor)\n",
    "    2. summary([tensor_a, tensor_b])\n",
    "    3. summary({tensor_a: 'a', tensor_b: 'b})\n",
    "    \"\"\"\n",
    "\n",
    "    def _summary(tensor, name, summary_type=['mean', 'stddev', 'max', 'min', 'sparsity', 'histogram']):\n",
    "        \"\"\" Attach a lot of summaries to a Tensor. \"\"\"\n",
    "\n",
    "        if name is None:\n",
    "            # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "            # session. This helps the clarity of presentation on tensorboard.\n",
    "            name = re.sub('%s_[0-9]*/' % 'tower', '', tensor.name)\n",
    "            name = re.sub(':', '-', name)\n",
    "\n",
    "        with tf.name_scope('summary_' + name):\n",
    "            summaries = []\n",
    "            if len(tensor._shape) == 0:\n",
    "                summaries.append(tf.summary.scalar(name, tensor))\n",
    "            else:\n",
    "                if 'mean' in summary_type:\n",
    "                    mean = tf.reduce_mean(tensor)\n",
    "                    summaries.append(tf.summary.scalar(name + '/mean', mean))\n",
    "                if 'stddev' in summary_type:\n",
    "                    mean = tf.reduce_mean(tensor)\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(tensor - mean)))\n",
    "                    summaries.append(tf.summary.scalar(name + '/stddev', stddev))\n",
    "                if 'max' in summary_type:\n",
    "                    summaries.append(tf.summary.scalar(name + '/max', tf.reduce_max(tensor)))\n",
    "                if 'min' in summary_type:\n",
    "                    summaries.append(tf.summary.scalar(name + '/min', tf.reduce_min(tensor)))\n",
    "                if 'sparsity' in summary_type:\n",
    "                    summaries.append(tf.summary.scalar(name + '/sparsity', tf.nn.zero_fraction(tensor)))\n",
    "                if 'histogram' in summary_type:\n",
    "                    summaries.append(tf.summary.histogram(name, tensor))\n",
    "            return tf.summary.merge(summaries)\n",
    "\n",
    "    if not isinstance(tensor_collection, (list, tuple, dict)):\n",
    "        tensor_collection = [tensor_collection]\n",
    "    with tf.name_scope('summaries'):\n",
    "        summaries = []\n",
    "        if isinstance(tensor_collection, (list, tuple)):\n",
    "            for tensor in tensor_collection:\n",
    "                summaries.append(_summary(tensor, None, summary_type))\n",
    "        else:\n",
    "            for tensor, name in tensor_collection.items():\n",
    "                summaries.append(_summary(tensor, name, summary_type))\n",
    "        return tf.summary.merge(summaries)\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, session, var_list=None):\n",
    "    print(' [*] Loading checkpoint...')\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        ckpt_path = os.path.join(checkpoint_dir, ckpt_name)\n",
    "    try:\n",
    "        restorer = tf.train.Saver(var_list)\n",
    "        restorer.restore(session, ckpt_path)\n",
    "        print(' [*] Loading successful! Copy variables from % s' % ckpt_path)\n",
    "        return True\n",
    "    except:\n",
    "        print(' [*] No suitable checkpoint!')\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def disk_image_batch(image_paths, batch_size, shape, preprocess_fn=None, shuffle=True, num_threads=16,\n",
    "                     min_after_dequeue=100, allow_smaller_final_batch=False, scope=None):\n",
    "    \"\"\"\n",
    "    This function is suitable for bmp, jpg, png and gif files\n",
    "    image_paths: string list or 1-D tensor, each of which is an iamge path\n",
    "    preprocess_fn: single image preprocessing function\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(scope, 'disk_image_batch'):\n",
    "        data_num = len(image_paths)\n",
    "\n",
    "        # dequeue a single image path and read the image bytes; enqueue the whole file list\n",
    "        _, img = tf.WholeFileReader().read(tf.train.string_input_producer(image_paths, shuffle=shuffle, capacity=data_num))\n",
    "        img = tf.image.decode_image(img)\n",
    "\n",
    "        # preprocessing\n",
    "        img.set_shape(shape)\n",
    "        if preprocess_fn is not None:\n",
    "            img = preprocess_fn(img)\n",
    "\n",
    "        # batch datas\n",
    "        if shuffle:\n",
    "            capacity = min_after_dequeue + (num_threads + 1) * batch_size\n",
    "            img_batch = tf.train.shuffle_batch([img],\n",
    "                                               batch_size=batch_size,\n",
    "                                               capacity=capacity,\n",
    "                                               min_after_dequeue=min_after_dequeue,\n",
    "                                               num_threads=num_threads,\n",
    "                                               allow_smaller_final_batch=allow_smaller_final_batch)\n",
    "        else:\n",
    "            img_batch = tf.train.batch([img],\n",
    "                                       batch_size=batch_size,\n",
    "                                       allow_smaller_final_batch=allow_smaller_final_batch)\n",
    "\n",
    "        return img_batch, data_num\n",
    "\n",
    "\n",
    "class DiskImageData:\n",
    "\n",
    "    def __init__(self, image_paths, batch_size, shape, preprocess_fn=None, shuffle=True, num_threads=16,\n",
    "                 min_after_dequeue=100, allow_smaller_final_batch=False, scope=None):\n",
    "        \"\"\"\n",
    "        This function is suitable for bmp, jpg, png and gif files\n",
    "        image_paths: string list or 1-D tensor, each of which is an iamge path\n",
    "        preprocess_fn: single image preprocessing function\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = tf.Graph()  # declare ops in a separated graph\n",
    "        with self.graph.as_default():\n",
    "            # @TODO\n",
    "            # There are some strange errors if the gpu device is the\n",
    "            # same with the main graph, but cpu device is ok. I don't know why...\n",
    "            with tf.device('/cpu:0'):\n",
    "                self._batch_ops, self._data_num = disk_image_batch(image_paths, batch_size, shape, preprocess_fn, shuffle, num_threads,\n",
    "                                                                   min_after_dequeue, allow_smaller_final_batch, scope)\n",
    "\n",
    "        print(' [*] DiskImageData: create session!')\n",
    "        self.sess = session(graph=self.graph)\n",
    "        self.coord = tf.train.Coordinator()\n",
    "        self.threads = tf.train.start_queue_runners(sess=self.sess, coord=self.coord)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._data_num\n",
    "\n",
    "    def batch(self):\n",
    "        return self.sess.run(self._batch_ops)\n",
    "\n",
    "    def __del__(self):\n",
    "        print(' [*] DiskImageData: stop threads and close session!')\n",
    "        self.coord.request_stop()\n",
    "        self.coord.join(self.threads)\n",
    "        self.sess.close()\n",
    "\n",
    "\n",
    "def to_range(images, min_value=0.0, max_value=1.0, dtype=None):\n",
    "    \"\"\"\n",
    "    transform images from [-1.0, 1.0] to [min_value, max_value] of dtype\n",
    "    \"\"\"\n",
    "    assert \\\n",
    "        np.min(images) >= -1.0 - 1e-5 and np.max(images) <= 1.0 + 1e-5 \\\n",
    "        and (images.dtype == np.float32 or images.dtype == np.float64), \\\n",
    "        'The input images should be float64(32) and in the range of [-1.0, 1.0]!'\n",
    "    if dtype is None:\n",
    "        dtype = images.dtype\n",
    "    return ((images + 1.) / 2. * (max_value - min_value) + min_value).astype(dtype)\n",
    "\n",
    "\n",
    "def imwrite(image, path):\n",
    "    \"\"\" save an [-1.0, 1.0] image \"\"\"\n",
    "\n",
    "    if image.ndim == 3 and image.shape[2] == 1:  # for gray image\n",
    "        image = np.array(image, copy=True)\n",
    "        image.shape = image.shape[0:2]\n",
    "    return scipy.misc.imsave(path, to_range(image, 0, 255, np.uint8))\n",
    "\n",
    "\n",
    "def immerge(images, row, col):\n",
    "    \"\"\"\n",
    "    merge images into an image with (row * h) * (col * w)\n",
    "    `images` is in shape of N * H * W(* C=1 or 3)\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if images.ndim == 4:\n",
    "        img = np.zeros((h * row, w * col, images.shape[3]))\n",
    "    elif images.ndim == 3:\n",
    "        img = np.zeros((h * row, w * col))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % col\n",
    "        j = idx // col\n",
    "        img[j * h:j * h + h, i * w:i * w + w, ...] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_fully_connected(inputs,\n",
    "                            num_outputs,\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            normalizer_fn=None,\n",
    "                            normalizer_params=None,\n",
    "                            weights_initializer=slim.xavier_initializer(),\n",
    "                            weights_regularizer=None,\n",
    "                            biases_initializer=tf.zeros_initializer(),\n",
    "                            biases_regularizer=None,\n",
    "                            reuse=None,\n",
    "                            variables_collections=None,\n",
    "                            outputs_collections=None,\n",
    "                            trainable=True,\n",
    "                            scope=None):\n",
    "    with tf.variable_scope(scope, 'flatten_fully_connected', [inputs]):\n",
    "        if inputs.shape.ndims > 2:\n",
    "            inputs = slim.flatten(inputs)\n",
    "        return slim.fully_connected(inputs,\n",
    "                                    num_outputs,\n",
    "                                    activation_fn,\n",
    "                                    normalizer_fn,\n",
    "                                    normalizer_params,\n",
    "                                    weights_initializer,\n",
    "                                    weights_regularizer,\n",
    "                                    biases_initializer,\n",
    "                                    biases_regularizer,\n",
    "                                    reuse,\n",
    "                                    variables_collections,\n",
    "                                    outputs_collections,\n",
    "                                    trainable,\n",
    "                                    scope)\n",
    "\n",
    "\n",
    "def leak_relu(x, leak, scope=None):\n",
    "    with tf.name_scope(scope, 'leak_relu', [x, leak]):\n",
    "        if leak < 1:\n",
    "            y = tf.maximum(x, leak * x)\n",
    "        else:\n",
    "            y = tf.minimum(x, leak * x)\n",
    "        return y\n",
    "    \n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "slim layers:\n",
    "\n",
    "    conv2d(args):\n",
    "        inputs: \n",
    "            A 4-D tensor with dimensions [batch_size, height, width, channels]\n",
    "        num_outputs:\n",
    "            Integer, the number of output filters.\n",
    "        kernel_size:\n",
    "            Can be an int if both values are the same.\n",
    "        stride: \n",
    "            default=1\n",
    "        ...\n",
    "    \n",
    "    conv2d_transpose(args):\n",
    "        inputs:\n",
    "        num_outputs: \n",
    "            Integer, the number of output filters.\n",
    "        kernel_size:\n",
    "        stride: \n",
    "            default=1\n",
    "        ...\n",
    "        \n",
    "    batch_norm(args):\n",
    "        is_training: \n",
    "            Whether or not the layer is in training mode. In training mode\n",
    "            it would accumulate the statistics of the moments into `moving_mean` and\n",
    "            `moving_variance` using an exponential moving average with the given\n",
    "            `decay`. When it is not in training mode then it would use the values of\n",
    "            the `moving_mean` and the `moving_variance`.\n",
    "            \n",
    "'''\n",
    "\n",
    "conv = partial(slim.conv2d, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "dconv = partial(slim.conv2d_transpose, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "fc = partial(flatten_fully_connected, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "relu = tf.nn.relu\n",
    "elu = tf.nn.elu\n",
    "lrelu = partial(leak_relu, leak=0.2)\n",
    "batch_norm = partial(slim.batch_norm, decay=0.9, scale=True, epsilon=1e-5, updates_collections=None)\n",
    "ln = slim.layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, dim=128, reuse=True, training=True):\n",
    "    bn = partial(batch_norm, is_training=training)\n",
    "    dconv_bn_elu = partial(dconv, normalizer_fn=bn, activation_fn=elu, biases_initializer=None)\n",
    "\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        y = fc(z, 4 * 4 * dim * 8)\n",
    "        y = tf.reshape(y, [-1, 4, 4, dim * 8])\n",
    "        #y = dconv_bn_elu(y, dim * 4, 3, 1)\n",
    "        y = dconv_bn_elu(y, dim * 4, 3, 2)\n",
    "        #y = dconv_bn_elu(y, dim * 2, 3, 1)\n",
    "        y = dconv_bn_elu(y, dim * 2, 3, 2)\n",
    "        #y = dconv_bn_elu(y, dim * 1, 3, 1)\n",
    "        y = dconv_bn_elu(y, dim * 1, 3, 2)\n",
    "        img = tf.tanh(dconv(y, 3, 3, 2)) # Change number of out channels\n",
    "        return img\n",
    "\n",
    "\n",
    "def discriminator(img, dim=128, reuse=True, training=True):\n",
    "    bn = partial(batch_norm, is_training=training)\n",
    "    conv_bn_elu = partial(conv, normalizer_fn=bn, activation_fn=elu, biases_initializer=None)\n",
    "    dconv_bn_elu = partial(dconv, normalizer_fn=bn, activation_fn=elu, biases_initializer=None)\n",
    "    h = 64 # or 128\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        encoder = elu(conv(img, dim, 3, 2))\n",
    "        #encoder = conv_bn_elu(encoder, dim, 3, 1)\n",
    "        encoder = conv_bn_elu(encoder, dim*2, 3, 2)\n",
    "        #encoder = conv_bn_elu(encoder, dim*2, 3, 1)\n",
    "        encoder = conv_bn_elu(encoder, dim*4, 3, 2)\n",
    "        #encoder = conv_bn_elu(encoder, dim*4, 3, 1)\n",
    "        encoder = conv_bn_elu(encoder, dim*8, 3, 2)\n",
    "        #encoder = conv_bn_elu(encoder, dim*8, 3, 1)\n",
    "        latent_space = fc(encoder, h)\n",
    "        decoder = fc(latent_space, 4 * 4 * dim * 8)\n",
    "        decoder = tf.reshape(decoder, [-1, 4, 4, dim * 8])\n",
    "        #decoder = dconv_bn_elu(decoder, dim * 4, 3, 1)\n",
    "        decoder = dconv_bn_elu(decoder, dim * 4, 3, 2)\n",
    "        #decoder = dconv_bn_elu(decoder, dim * 2, 3, 1)\n",
    "        decoder = dconv_bn_elu(decoder, dim * 2, 3, 2)\n",
    "        #decoder = dconv_bn_elu(decoder, dim * 1, 3, 1)\n",
    "        decoder = dconv_bn_elu(decoder, dim * 1, 3, 2)\n",
    "        reconst = tf.tanh(dconv(decoder, 3, 3, 2))\n",
    "        return reconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] DiskImageData: create session!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" param \"\"\"\n",
    "epoch = 50000\n",
    "batch_size = 32 # 16 from paper\n",
    "lr = 0.0001\n",
    "z_dim = 64\n",
    "clip = 0.01\n",
    "n_critic = 5\n",
    "gpu_id = 0\n",
    "\n",
    "lamda = 0.001\n",
    "gamma = 0.5\n",
    "beta1 = 0.9 #0.5\n",
    "\n",
    "# The number of run\n",
    "run_num = 3\n",
    "\n",
    "''' data '''\n",
    "# you should prepare your own data in ./data/img_align_celeba\n",
    "# celeba original size is [218, 178, 3]\n",
    "\n",
    "\n",
    "def preprocess_fn(img):\n",
    "    #Transform it to be between -1 and 1\n",
    "    img = tf.to_float(img) / 127.5 - 1\n",
    "\n",
    "    return img\n",
    "\n",
    "file_paths = glob.glob(\"./Face_data/Celeba/dataset_64x64/*\")\n",
    "data_pool = DiskImageData(file_paths, batch_size, shape=[64, 64, 3], preprocess_fn=preprocess_fn)\n",
    "\n",
    "\n",
    "\"\"\" graphs \"\"\"\n",
    "with tf.device('/gpu:%d' % gpu_id):\n",
    "\n",
    "    ''' graph '''\n",
    "    # inputs\n",
    "    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "    k_t = tf.placeholder(tf.float32, shape=(), name='kt') # shape of a scalar = ()\n",
    "\n",
    "    # generate\n",
    "    fake = generator(z, reuse=False)\n",
    "\n",
    "    # discriminate\n",
    "    reconst_r = discriminator(real, reuse=False)\n",
    "    reconst_f = discriminator(fake)\n",
    "\n",
    "    # losses\n",
    "    AE_loss_r = l1_loss(real, reconst_r)\n",
    "    AE_loss_f = l1_loss(fake, reconst_f)\n",
    "    d_loss = AE_loss_r - k_t * AE_loss_f\n",
    "    g_loss = AE_loss_f\n",
    "    M_global = AE_loss_r + tf.abs(gamma * AE_loss_r - AE_loss_f)\n",
    "\n",
    "    # vars\n",
    "    d_vars = trainable_variables('discriminator')\n",
    "    g_vars = trainable_variables('generator')\n",
    "    \n",
    "    # optimizers\n",
    "    d_opt = tf.train.AdamOptimizer(lr, beta1).minimize(d_loss, var_list=d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(lr, beta1).minimize(g_loss, var_list=g_vars)\n",
    "    \n",
    "    # summaries\n",
    "    d_summary = summary({AE_loss_r: 'AE_real_reconst_loss', d_loss: 'd_loss'})\n",
    "    g_summary = summary({g_loss: 'g_loss'})\n",
    "    m_summary = summary({M_global: 'M_global'})\n",
    "\n",
    "    # sample\n",
    "    f_sample = generator(z, training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Loading checkpoint...\n",
      " [*] No suitable checkpoint!\n",
      "Epoch: (  0) (    1/ 1208) -- g_loss: 0.2654, d_loss: 0.4872 AE_r: 0.4872, AE_f: 0.2654, kt: 0.00000000, M: 0.50900128\n",
      "Epoch: (  0) (  101/ 1208) -- g_loss: 0.2617, d_loss: 0.1873 AE_r: 0.1873, AE_f: 0.2617, kt: 0.00000000, M: 0.35536502\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train \"\"\"\n",
    "''' init '''\n",
    "# session\n",
    "sess = session()\n",
    "# saver\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "# summary writer\n",
    "summary_writer = tf.summary.FileWriter('./BEGAN/summaries/run%s' % run_num, sess.graph)\n",
    "\n",
    "''' initialization '''\n",
    "ckpt_dir = './BEGAN/checkpoints/run%s' % run_num\n",
    "mkdir(ckpt_dir + '/')\n",
    "if not load_checkpoint(ckpt_dir, sess):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "''' train '''\n",
    "try:\n",
    "    #z_ipt_sample = np.random.normal(size=[100, z_dim])\n",
    "    z_ipt_sample = np.random.uniform(-1., 1., size=[100, z_dim])\n",
    "    kt = np.float32(0.)\n",
    "    \n",
    "    batch_epoch = len(data_pool) // (batch_size * n_critic)\n",
    "    max_it = epoch * batch_epoch\n",
    "\n",
    "    for it in range(max_it):\n",
    "\n",
    "        # which epoch\n",
    "        epoch = it // batch_epoch\n",
    "        it_epoch = it % batch_epoch + 1\n",
    "\n",
    "        # batch data\n",
    "        real_ipt = data_pool.batch()\n",
    "        #z_ipt = np.random.normal(size=[batch_size, z_dim])\n",
    "        z_ipt = np.random.uniform(-1., 1., size=[batch_size, z_dim])\n",
    "\n",
    "        # terms to be calculated & feed list\n",
    "        g_opt_list = [g_opt, g_loss, AE_loss_r, AE_loss_f]\n",
    "        d_opt_list = [d_opt, d_loss, d_summary, g_summary, m_summary]\n",
    "        feed_dict = {real: real_ipt, z: z_ipt, k_t: kt}\n",
    "\n",
    "        # run\n",
    "        _, g_loss_calculated, AE_loss_r_calculated, AE_loss_f_calculated = sess.run(g_opt_list, feed_dict=feed_dict)\n",
    "        _, d_loss_calculated, summary_d, summary_g, summary_m = sess.run(d_opt_list, feed_dict=feed_dict)\n",
    "        \n",
    "        # update kt, m_global -- (The range of kt is [0,1])\n",
    "        kt = kt + lamda * (gamma * AE_loss_r_calculated - AE_loss_f_calculated)\n",
    "        kt = np.maximum(np.minimum(1.,kt), 0.)\n",
    "        m_global = AE_loss_r_calculated + np.abs(gamma * AE_loss_r_calculated - AE_loss_f_calculated)\n",
    "        \n",
    "        # write train summary\n",
    "        summary_writer.add_summary(summary_d, it)\n",
    "        summary_writer.add_summary(summary_g, it)\n",
    "        summary_writer.add_summary(summary_m, it)\n",
    "\n",
    "        \n",
    "\n",
    "        # display\n",
    "        if it % 100 == 0:\n",
    "            print(\"Epoch: (%3d) (%5d/%5d) -- g_loss: %.4f, d_loss: %.4f AE_r: %.4f, AE_f: %.4f, kt: %.8f, M: %.8f\" \n",
    "                  % (epoch, it_epoch, batch_epoch, \n",
    "                     g_loss_calculated, d_loss_calculated,\n",
    "                     AE_loss_r_calculated, AE_loss_f_calculated,\n",
    "                     kt, m_global))\n",
    "\n",
    "        # save\n",
    "        if (it + 1) % 1000 == 0:\n",
    "            save_path = saver.save(sess, '%s/Epoch_(%d)_(%dof%d).ckpt' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n",
    "            print('Model saved in file: % s' % save_path)\n",
    "\n",
    "        # sample\n",
    "        if (it + 1) % 100 == 0:\n",
    "            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n",
    "\n",
    "            save_dir = './BEGAN/sample_images_while_training/run%s' % run_num\n",
    "            mkdir(save_dir + '/')\n",
    "            imwrite(immerge(f_sample_opt, 10, 10), '%s/Epoch_(%d)_(%dof%d).jpg' % (save_dir, epoch, it_epoch, batch_epoch))\n",
    "\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\" [*] Close main session!\")\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
