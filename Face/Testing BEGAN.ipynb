{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy from https://github.com/Heumi/BEGAN-tensorflow/tree/master/src/function\n",
    "\n",
    "To test the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf\n",
    "\n",
    "https://github.com/nightrome/really-awesome-gan\n",
    "\n",
    "https://arxiv.org/pdf/1705.09966.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1703.05192.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1606.03657.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1704.02166.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1709.03842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = scm.imread(img_path)/255. - 0.5\n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "\n",
    "def inverse_image(img):\n",
    "    img = (img + 0.5) * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv2d(x, filter_shape, bias=True, stride=1, padding=\"SAME\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "    pad_size = (kw - 1) / 2\n",
    "\n",
    "    if padding == \"VALID\":\n",
    "        x = tf.pad(x, [[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], \"SYMMETRIC\")\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=filter_shape, initializer=initializer)\n",
    "        x = tf.nn.conv2d(x, weight, [1, stride, stride, 1], padding=padding)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=filter_shape[-1], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"./Face_data/Celeba/dataset_64x64\" #args.data_dir #./Data\n",
    "        self.dataset = \"celeba\" #args.dataset  # celeba\n",
    "        self.data_size = 64 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 50 #args.niter\n",
    "        self.niter_snapshot = 2440 #args.nsnapshot\n",
    "        self.max_to_keep = 5 #args.max_to_keep\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 0.5 #args.gamma\n",
    "        self.filter_number = 64 #args.filter_number\n",
    "        self.input_size = 64 #args.input_size\n",
    "        self.embedding = 64 #args.embedding\n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = 'assets/{0}_{1}_{2}_{3}/'.format(self.project, self.dataset, self.data_opt, self.data_size)\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'models')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "        # etc.\n",
    "        if not os.path.exists('assets'):\n",
    "            os.makedirs('assets')\n",
    "        make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size, self.input_size], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.data_size, self.data_size, 3], name='y')\n",
    "        self.kt = tf.placeholder(tf.float32, name='kt')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "\n",
    "        # Generator\n",
    "        self.recon_gen = self.generator(self.x)\n",
    "\n",
    "        # Discriminator (Critic)\n",
    "        d_real = self.decoder(self.encoder(self.y))\n",
    "        d_fake = self.decoder(self.encoder(self.recon_gen, reuse=True), reuse=True)\n",
    "        self.recon_dec = self.decoder(self.x, reuse=True)\n",
    "\n",
    "        # Loss\n",
    "        self.d_real_loss = l1_loss(self.y, d_real)\n",
    "        self.d_fake_loss = l1_loss(self.recon_gen, d_fake)\n",
    "        self.d_loss = self.d_real_loss - self.kt * self.d_fake_loss\n",
    "        self.g_loss = self.d_fake_loss\n",
    "        self.m_global = self.d_real_loss + tf.abs(self.gamma * self.d_real_loss - self.d_fake_loss)\n",
    "\n",
    "        # Variables\n",
    "        g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"gen_\")\n",
    "        d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"disc_\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.opt_g = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.g_loss, var_list=g_vars)\n",
    "        self.opt_d = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.d_loss, var_list=d_vars)\n",
    "\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            self.saver.save(self.sess, self.ckpt_model_name, write_meta_graph=True)\n",
    "\n",
    "        # Summary\n",
    "        if self.flag:\n",
    "            tf.summary.scalar('loss/loss', self.d_loss + self.g_loss)\n",
    "            tf.summary.scalar('loss/g_loss', self.g_loss)\n",
    "            tf.summary.scalar('loss/d_loss', self.d_loss)\n",
    "            tf.summary.scalar('loss/d_real_loss', self.d_real_loss)\n",
    "            tf.summary.scalar('loss/d_fake_loss', self.d_fake_loss)\n",
    "            tf.summary.scalar('misc/kt', self.kt)\n",
    "            tf.summary.scalar('misc/m_global', self.m_global)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(self.project_dir, self.sess.graph)\n",
    "\n",
    "    def train(self, train_flag):\n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        print('Shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Shuffle Done')\n",
    "\n",
    "        # initial parameter\n",
    "        start_time = time.time()\n",
    "        kt = np.float32(0.)\n",
    "        lr = np.float32(self.learning_rate)\n",
    "        self.count = 0\n",
    "\n",
    "        for epoch in range(self.niter):\n",
    "            batch_idxs = len(data) // self.batch_size\n",
    "\n",
    "            for idx in range(0, batch_idxs):\n",
    "                self.count += 1\n",
    "\n",
    "                batch_x = np.random.uniform(-1., 1., size=[self.batch_size, self.input_size])\n",
    "                batch_files = data[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "                batch_data = [get_image(batch_file) for batch_file in batch_files]\n",
    "\n",
    "                # opt & feed list (different with paper)\n",
    "                g_opt = [self.opt_g, self.g_loss, self.d_real_loss, self.d_fake_loss]\n",
    "                d_opt = [self.opt_d, self.d_loss, self.merged]\n",
    "                feed_dict = {self.x: batch_x, self.y: batch_data, self.kt: kt, self.lr: lr}\n",
    "\n",
    "                # run tensorflow\n",
    "                _, loss_g, d_real_loss, d_fake_loss = self.sess.run(g_opt, feed_dict=feed_dict)\n",
    "                _, loss_d, summary = self.sess.run(d_opt, feed_dict=feed_dict)\n",
    "\n",
    "                # update kt, m_global\n",
    "                kt = np.maximum(np.minimum(1., kt + self.lamda * (self.gamma * d_real_loss - d_fake_loss)), 0.)\n",
    "                m_global = d_real_loss + np.abs(self.gamma * d_real_loss - d_fake_loss)\n",
    "                loss = loss_g + loss_d\n",
    "\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, \"\n",
    "                      \"loss: %.4f, loss_g: %.4f, loss_d: %.4f, d_real: %.4f, d_fake: %.4f, kt: %.8f, M: %.8f\"\n",
    "                      % (epoch, idx, batch_idxs, time.time() - start_time,\n",
    "                         loss, loss_g, loss_d, d_real_loss, d_fake_loss, kt, m_global))\n",
    "\n",
    "                # write train summary\n",
    "                self.writer.add_summary(summary, self.count)\n",
    "\n",
    "                # Test during Training\n",
    "                if self.count % self.niter_snapshot == (self.niter_snapshot - 1):\n",
    "                    # update learning rate\n",
    "                    lr *= 0.95\n",
    "                    # save & test\n",
    "                    self.saver.save(self.sess, self.ckpt_model_name, global_step=self.count, write_meta_graph=False)\n",
    "                    self.test(train_flag)\n",
    "\n",
    "    def test(self, train_flag=True):\n",
    "        # generate output\n",
    "        img_num = self.batch_size\n",
    "        img_size = self.data_size\n",
    "\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        im_output_dec = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "\n",
    "        test_data = np.random.uniform(-1., 1., size=[img_num, self.input_size])\n",
    "        output_gen = (self.sess.run(self.recon_gen, feed_dict={self.x: test_data}))  # generator output\n",
    "        output_dec = (self.sess.run(self.recon_dec, feed_dict={self.x: test_data}))  # decoder output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(img_num)]\n",
    "        output_dec = [inverse_image(output_dec[i]) for i in range(img_num)]\n",
    "\n",
    "        for i in range(output_f):\n",
    "            for j in range(output_f):\n",
    "                im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                    = output_gen[j + (i * output_f)]\n",
    "                im_output_dec[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                    = output_dec[j + (i * output_f)]\n",
    "\n",
    "        # output save\n",
    "        if train_flag:\n",
    "            scm.imsave(self.project_dir + '/result/' + str(self.count) + '_output.bmp', im_output_gen)\n",
    "        else:\n",
    "            now = datetime.datetime.now()\n",
    "            nowDatetime = now.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "            scm.imsave(self.project_dir + '/result_test/gen_{}_output.bmp'.format(nowDatetime), im_output_gen)\n",
    "            scm.imsave(self.project_dir + '/result_test/dec_{}_output.bmp'.format(nowDatetime), im_output_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BEGAN(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "\n",
    "    def generator(self, x, reuse=None):\n",
    "        with tf.variable_scope('gen_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            w = self.data_size\n",
    "            f = self.filter_number\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = fc(x, 8 * 8 * f, name='fc')\n",
    "            x = tf.reshape(x, [-1, 8, 8, f])\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv1_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = resize_nn(x, w / 8)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv2_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv2_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 4)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv3_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv3_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 2)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv4_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv4_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv5_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv5_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, 3], stride=1,  padding=p,name='conv6_a')\n",
    "        return x\n",
    "\n",
    "    def encoder(self, x, reuse=None):\n",
    "        with tf.variable_scope('disc_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            h = self.embedding\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = conv2d(x, [3, 3, 3, f], stride=1,  padding=p,name='conv1_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv2_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv2_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, f, 2 * f], stride=1,  padding=p,name='conv3_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 2 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 3 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = conv2d(x, [1, 1, 4 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_0')\n",
    "                x = pool(x, r=2, s=2)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "            x = fc(x, h, name='enc_fc')\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x, reuse=None):\n",
    "        with tf.variable_scope('disc_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            w = self.data_size\n",
    "            f = self.filter_number\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = fc(x, 8 * 8 * f, name='fc')\n",
    "            x = tf.reshape(x, [-1, 8, 8, f])\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = resize_nn(x, w / 8)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv2_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv2_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "                x = resize_nn(x, w / 4)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv3_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv3_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 2)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv4_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv4_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv5_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv5_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, 3], stride=1, padding=p, name='conv6_a')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"test_began\"\n",
    "train_flag = True\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = BEGAN(sess, project_name)\n",
    "\n",
    "        # TRAIN / TEST\n",
    "        if train_flag:\n",
    "            model.train(train_flag)\n",
    "        else:\n",
    "            model.test(train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
