{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy from https://github.com/Heumi/BEGAN-tensorflow/tree/master/src/function\n",
    "\n",
    "To test the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf\n",
    "\n",
    "https://github.com/nightrome/really-awesome-gan\n",
    "\n",
    "https://arxiv.org/pdf/1705.09966.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1703.05192.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1606.03657.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1704.02166.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1709.03842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "import numpy as np\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = scm.imread(img_path)/255. - 0.5\n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def get_label(path, size):\n",
    "    label = int(path[-5])\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[ label ] = 1.\n",
    "    return one_hot\n",
    "\n",
    "def inverse_image(img):\n",
    "    img = (img + 0.5) * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n",
    "\n",
    "def pair_expressions(paths):\n",
    "    subject_exprs = []\n",
    "    subject_pairs = []\n",
    "    all_pairs = []\n",
    "    last_subject = 0\n",
    "\n",
    "    # Pair all expression of a subject\n",
    "    for path in paths:\n",
    "        subject = int(path[-9:-6])\n",
    "\n",
    "        if subject != last_subject and last_subject != 0:\n",
    "            subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "            all_pairs.extend(subject_pairs)\n",
    "            subject_exprs = []\n",
    "\n",
    "        subject_exprs.append(path)\n",
    "        last_subject = subject\n",
    "\n",
    "    # Last subject\n",
    "    subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "    all_pairs.extend(subject_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "def get_shape_c(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv2d(x, filter_shape, bias=True, stride=1, padding=\"SAME\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "    pad_size = (kw - 1) / 2\n",
    "\n",
    "    if padding == \"VALID\":\n",
    "        x = tf.pad(x, [[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], \"SYMMETRIC\")\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=filter_shape, initializer=initializer)\n",
    "        x = tf.nn.conv2d(x, weight, [1, stride, stride, 1], padding=padding)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=filter_shape[-1], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"./Face_data/Faces_with_expression_label/dataset_64x64\" #args.data_dir #./Data\n",
    "        self.dataset = \"expr\" #args.dataset  # celeba\n",
    "        self.data_size = 64 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "        self.data_label_vector_size = 7 #size of one-hot-encoded label vector\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 50 #args.niter\n",
    "        self.niter_snapshot = 2440 #args.nsnapshot\n",
    "        self.max_to_keep = 5 #args.max_to_keep\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 2.0 #args.gamma\n",
    "        self.filter_number = 64 #args.filter_number\n",
    "        self.input_size = 64 #args.input_size\n",
    "        self.embedding = 64 #args.embedding\n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = 'assets/{0}_{1}_{2}_{3}/'.format(self.project, self.dataset, self.data_opt, self.data_size)\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'models')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "        # etc.\n",
    "        if not os.path.exists('assets'):\n",
    "            os.makedirs('assets')\n",
    "        make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.input_size, self.input_size, 3], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='y')\n",
    "        self.c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='c')\n",
    "        self.kt = tf.placeholder(tf.float32, name='kt')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "\n",
    "        # Generator\n",
    "        self.recon_gen = self.generator_decoder(self.generator_encoder(self.x), self.c)\n",
    "\n",
    "        # Discriminator (Critic)\n",
    "        d_real = self.discriminator_decoder(self.discriminator_encoder(self.y, self.c))\n",
    "        d_fake = self.discriminator_decoder(self.discriminator_encoder(self.recon_gen, self.c, reuse=True), reuse=True)\n",
    "        #self.recon_dec = self.discriminator_decoder(self.x, reuse=True)\n",
    "\n",
    "        # Loss\n",
    "        self.d_real_loss = l1_loss(self.y, d_real)\n",
    "        self.d_fake_loss = l1_loss(self.recon_gen, d_fake)\n",
    "        self.d_loss = self.d_real_loss - self.kt * self.d_fake_loss\n",
    "        self.g_loss = self.d_fake_loss\n",
    "        self.m_global = self.d_real_loss + tf.abs(self.gamma * self.d_real_loss - self.d_fake_loss)\n",
    "\n",
    "        # Variables\n",
    "        g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"gen_\")\n",
    "        d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"disc_\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.opt_g = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.g_loss, var_list=g_vars)\n",
    "        self.opt_d = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.d_loss, var_list=d_vars)\n",
    "\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            self.saver.save(self.sess, self.ckpt_model_name, write_meta_graph=True)\n",
    "\n",
    "        # Summary\n",
    "        if self.flag:\n",
    "            tf.summary.scalar('loss/loss', self.d_loss + self.g_loss)\n",
    "            tf.summary.scalar('loss/g_loss', self.g_loss)\n",
    "            tf.summary.scalar('loss/d_loss', self.d_loss)\n",
    "            tf.summary.scalar('loss/d_real_loss', self.d_real_loss)\n",
    "            tf.summary.scalar('loss/d_fake_loss', self.d_fake_loss)\n",
    "            tf.summary.scalar('misc/kt', self.kt)\n",
    "            tf.summary.scalar('misc/m_global', self.m_global)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(self.project_dir, self.sess.graph)\n",
    "\n",
    "    def train(self, train_flag):\n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        print('Shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Shuffle Done')\n",
    "\n",
    "        # initial parameter\n",
    "        start_time = time.time()\n",
    "        kt = np.float32(0.)\n",
    "        lr = np.float32(self.learning_rate)\n",
    "        self.count = 0\n",
    "\n",
    "        for epoch in range(self.niter):\n",
    "            batch_idxs = len(data) // self.batch_size\n",
    "\n",
    "            for idx in range(0, batch_idxs):\n",
    "                self.count += 1\n",
    "\n",
    "                batch_files = data[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "                batch_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "                batch_targets = [get_image(batch_file[1]) for batch_file in batch_files]\n",
    "                batch_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "                # opt & feed list (different with paper)\n",
    "                g_opt = [self.opt_g, self.g_loss, self.d_real_loss, self.d_fake_loss]\n",
    "                d_opt = [self.opt_d, self.d_loss, self.merged]\n",
    "                feed_dict = {self.x: batch_inputs, self.y: batch_targets, self.c: batch_target_labels,\n",
    "                             self.kt: kt, self.lr: lr}\n",
    "\n",
    "                # run tensorflow\n",
    "                _, loss_g, d_real_loss, d_fake_loss = self.sess.run(g_opt, feed_dict=feed_dict)\n",
    "                _, loss_d, summary = self.sess.run(d_opt, feed_dict=feed_dict)\n",
    "\n",
    "                # update kt, m_global\n",
    "                kt = np.maximum(np.minimum(1., kt + self.lamda * (self.gamma * d_real_loss - d_fake_loss)), 0.)\n",
    "                m_global = d_real_loss + np.abs(self.gamma * d_real_loss - d_fake_loss)\n",
    "                loss = loss_g + loss_d\n",
    "\n",
    "                if self.count % 100 == 1:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, \"\n",
    "                          \"loss: %.4f, loss_g: %.4f, loss_d: %.4f, d_real: %.4f, d_fake: %.4f, kt: %.8f, M: %.8f\"\n",
    "                          % (epoch, idx, batch_idxs, time.time() - start_time,\n",
    "                             loss, loss_g, loss_d, d_real_loss, d_fake_loss, kt, m_global))\n",
    "\n",
    "                # write train summary\n",
    "                self.writer.add_summary(summary, self.count)\n",
    "\n",
    "                # Test during Training\n",
    "                if self.count % self.niter_snapshot == (self.niter_snapshot - 1):\n",
    "                    # update learning rate\n",
    "                    lr *= 0.95\n",
    "                    # save & test\n",
    "                    self.saver.save(self.sess, self.ckpt_model_name, global_step=self.count, write_meta_graph=False)\n",
    "                    self.test(train_flag)\n",
    "\n",
    "    def test(self, train_flag=True):\n",
    "        # generate output\n",
    "        img_num =  36 #self.batch_size\n",
    "        display_img_num = int(img_num / 3)\n",
    "        img_size = self.data_size\n",
    "\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        #im_output_dec = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        \n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        print('Test data shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Test data shuffle Done')\n",
    "        \n",
    "        batch_files = data[0: display_img_num]\n",
    "        test_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "        test_inputs_o = [scm.imread((batch_file[0])) for batch_file in batch_files]\n",
    "        test_targets = [scm.imread((batch_file[1])) for batch_file in batch_files]\n",
    "        test_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "        output_gen = (self.sess.run(self.recon_gen, feed_dict={self.x: test_inputs, \n",
    "                                                               self.c: test_target_labels}))  # generator output\n",
    "        #output_dec = (self.sess.run(self.recon_dec, feed_dict={self.x: test_inputs}))  # decoder output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(display_img_num)]\n",
    "        #output_dec = [inverse_image(output_dec[i]) for i in range(display_img_num)]\n",
    "\n",
    "        for i in range(output_f): # row\n",
    "            for j in range(output_f): # col\n",
    "                if j % 3 == 0: # input img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_inputs_o[int(j / 3) + (i * int(output_f / 3))]\n",
    "                    #im_output_dec[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                    #    = test_inputs_o[int(j / 3) + (i * int(output_f / 3))]\n",
    "                elif j % 3 == 1: # output img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[int(j / 3) + (i * int(output_f / 3))]\n",
    "                    #im_output_dec[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                    #    = output_dec[int(j / 3) + (i * int(output_f / 3))]\n",
    "                else: # target img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_targets[int(j / 3) + (i * int(output_f / 3))]\n",
    "                    #im_output_dec[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                    #    = test_targets[int(j / 3) + (i * int(output_f / 3))]\n",
    "\n",
    "        labels = np.argmax(test_target_labels, axis=1)\n",
    "        label_string = ''.join(str(int(l)) for l in labels)\n",
    "        # output save\n",
    "        if train_flag:\n",
    "            scm.imsave(self.project_dir + '/result/' + str(self.count) + '_' + label_string \n",
    "                       + '_output.bmp', im_output_gen)\n",
    "        else:\n",
    "            now = datetime.datetime.now()\n",
    "            nowDatetime = now.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "            scm.imsave(self.project_dir + '/result_test/gen_{}_'.format(nowDatetime) \n",
    "                       + label_string + '_output.bmp', im_output_gen)\n",
    "            #scm.imsave(self.project_dir + '/result_test/dec_{}_'.format(nowDatetime) \n",
    "            #           + label_string + '_output.bmp' , im_output_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BEGAN(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "        \n",
    "    def generator_encoder(self, x, reuse=None):\n",
    "        with tf.variable_scope('gen_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            h = self.embedding\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = conv2d(x, [3, 3, 3, f], stride=1,  padding=p,name='conv1_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv2_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv2_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, f, 2 * f], stride=1,  padding=p,name='conv3_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 2 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 3 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = conv2d(x, [1, 1, 4 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_0')\n",
    "                x = pool(x, r=2, s=2)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "            x = fc(x, h, name='enc_fc')\n",
    "        return x\n",
    "\n",
    "    def generator_decoder(self, x, c, reuse=None):\n",
    "        with tf.variable_scope('gen_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            w = self.data_size\n",
    "            f = self.filter_number\n",
    "            p = \"SAME\"\n",
    "            \n",
    "            x = tf.concat([x, c], axis=1) # adding label information\n",
    "\n",
    "            x = fc(x, 8 * 8 * f, name='fc')\n",
    "            x = tf.reshape(x, [-1, 8, 8, f])\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv1_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = resize_nn(x, w / 8)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv2_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv2_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 4)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv3_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv3_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 2)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv4_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p, name='conv4_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv5_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv5_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, 3], stride=1,  padding=p,name='conv6_a')\n",
    "        return x\n",
    "\n",
    "    def discriminator_encoder(self, x, c, reuse=None):\n",
    "        with tf.variable_scope('disc_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            h = self.embedding\n",
    "            p = \"SAME\"\n",
    "            c_len = self.data_label_vector_size\n",
    "\n",
    "            x = conv2d(x, [3, 3, 3, f], stride=1,  padding=p,name='conv1_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            # Add y as a channel to dis1 layer as described in IcGAN paper\n",
    "            x = tf.concat([x, tf.tile(tf.reshape(c, [-1, 1, 1, get_shape_c(c)[-1]]),\\\n",
    "                                     [1, x.get_shape().as_list()[1], x.get_shape().as_list()[2], 1])],\\\n",
    "                              axis=3)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f + c_len, f], stride=1,  padding=p,name='conv2_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1,  padding=p,name='conv2_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, f, 2 * f], stride=1,  padding=p,name='conv3_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 2 * f, 2 * f], stride=1,  padding=p,name='conv3_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 2 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 3 * f, 3 * f], stride=1,  padding=p,name='conv4_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [1, 1, 3 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_0')\n",
    "            x = pool(x, r=2, s=2)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, 4 * f, 4 * f], stride=1,  padding=p,name='conv5_enc_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = conv2d(x, [1, 1, 4 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_0')\n",
    "                x = pool(x, r=2, s=2)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, 5 * f, 5 * f], stride=1,  padding=p,name='conv6_enc_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "            x = fc(x, h, name='enc_fc')\n",
    "        return x\n",
    "\n",
    "    def discriminator_decoder(self, x, reuse=None):\n",
    "        with tf.variable_scope('disc_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            w = self.data_size\n",
    "            f = self.filter_number\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = fc(x, 8 * 8 * f, name='fc')\n",
    "            x = tf.reshape(x, [-1, 8, 8, f])\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv1_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            if self.data_size == 128:\n",
    "                x = resize_nn(x, w / 8)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv2_a')\n",
    "                x = tf.nn.elu(x)\n",
    "                x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv2_b')\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "                x = resize_nn(x, w / 4)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv3_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv3_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w / 2)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv4_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv4_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = resize_nn(x, w)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv5_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv2d(x, [3, 3, f, f], stride=1, padding=p, name='conv5_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv2d(x, [3, 3, f, 3], stride=1, padding=p, name='conv6_a')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle ....\n",
      "Shuffle Done\n",
      "Epoch: [ 0] [   0/ 633] time: 1.0324, loss: 0.2429, loss_g: 0.0000, loss_d: 0.2429, d_real: 0.2429, d_fake: 0.0000, kt: 0.00048585, M: 0.72877799\n",
      "Epoch: [ 0] [ 100/ 633] time: 17.4920, loss: 0.1812, loss_g: 0.0254, loss_d: 0.1558, d_real: 0.1567, d_fake: 0.0254, kt: 0.03228274, M: 0.44457084\n",
      "Epoch: [ 0] [ 200/ 633] time: 34.0431, loss: 0.1128, loss_g: 0.0271, loss_d: 0.0857, d_real: 0.0871, d_fake: 0.0271, kt: 0.05041021, M: 0.23407784\n",
      "Epoch: [ 0] [ 300/ 633] time: 50.8094, loss: 0.1420, loss_g: 0.0657, loss_d: 0.0764, d_real: 0.0804, d_fake: 0.0657, kt: 0.06181993, M: 0.17557015\n",
      "Epoch: [ 0] [ 400/ 633] time: 67.5938, loss: 0.1578, loss_g: 0.0858, loss_d: 0.0720, d_real: 0.0780, d_fake: 0.0858, kt: 0.07129666, M: 0.14806770\n",
      "Epoch: [ 0] [ 500/ 633] time: 84.3867, loss: 0.1380, loss_g: 0.0669, loss_d: 0.0711, d_real: 0.0760, d_fake: 0.0669, kt: 0.08022035, M: 0.16096699\n",
      "Epoch: [ 0] [ 600/ 633] time: 101.2652, loss: 0.1336, loss_g: 0.0645, loss_d: 0.0691, d_real: 0.0740, d_fake: 0.0645, kt: 0.08861684, M: 0.15762058\n",
      "Epoch: [ 1] [  67/ 633] time: 118.4296, loss: 0.1310, loss_g: 0.0619, loss_d: 0.0691, d_real: 0.0747, d_fake: 0.0619, kt: 0.09725372, M: 0.16221265\n",
      "Epoch: [ 1] [ 167/ 633] time: 135.7335, loss: 0.1311, loss_g: 0.0655, loss_d: 0.0656, d_real: 0.0719, d_fake: 0.0655, kt: 0.10526639, M: 0.15018659\n",
      "Epoch: [ 1] [ 267/ 633] time: 152.9740, loss: 0.1113, loss_g: 0.0548, loss_d: 0.0565, d_real: 0.0622, d_fake: 0.0548, kt: 0.11230017, M: 0.13188723\n",
      "Epoch: [ 1] [ 367/ 633] time: 170.2878, loss: 0.1419, loss_g: 0.0830, loss_d: 0.0589, d_real: 0.0680, d_fake: 0.0830, kt: 0.11885370, M: 0.12103210\n",
      "Epoch: [ 1] [ 467/ 633] time: 187.8361, loss: 0.1173, loss_g: 0.0649, loss_d: 0.0524, d_real: 0.0598, d_fake: 0.0649, kt: 0.12438033, M: 0.11438288\n",
      "Epoch: [ 1] [ 567/ 633] time: 205.4004, loss: 0.1410, loss_g: 0.0894, loss_d: 0.0517, d_real: 0.0626, d_fake: 0.0894, kt: 0.12975571, M: 0.09837361\n",
      "Epoch: [ 2] [  34/ 633] time: 222.9884, loss: 0.1218, loss_g: 0.0696, loss_d: 0.0522, d_real: 0.0604, d_fake: 0.0696, kt: 0.13387883, M: 0.11162013\n",
      "Epoch: [ 2] [ 134/ 633] time: 240.6225, loss: 0.1456, loss_g: 0.0857, loss_d: 0.0599, d_real: 0.0709, d_fake: 0.0857, kt: 0.13770935, M: 0.12696118\n",
      "Epoch: [ 2] [ 234/ 633] time: 258.2757, loss: 0.1306, loss_g: 0.0808, loss_d: 0.0498, d_real: 0.0604, d_fake: 0.0808, kt: 0.14105079, M: 0.10044408\n",
      "Epoch: [ 2] [ 334/ 633] time: 275.9329, loss: 0.1350, loss_g: 0.0889, loss_d: 0.0462, d_real: 0.0578, d_fake: 0.0889, kt: 0.14371279, M: 0.08446807\n",
      "Epoch: [ 2] [ 434/ 633] time: 293.5900, loss: 0.1555, loss_g: 0.1107, loss_d: 0.0447, d_real: 0.0598, d_fake: 0.1107, kt: 0.14605451, M: 0.06857852\n",
      "Epoch: [ 2] [ 534/ 633] time: 311.2474, loss: 0.1578, loss_g: 0.1098, loss_d: 0.0480, d_real: 0.0637, d_fake: 0.1098, kt: 0.14508252, M: 0.08144216\n",
      "Epoch: [ 3] [   1/ 633] time: 328.9021, loss: 0.1747, loss_g: 0.1349, loss_d: 0.0398, d_real: 0.0561, d_fake: 0.1349, kt: 0.14572579, M: 0.07874392\n",
      "Epoch: [ 3] [ 101/ 633] time: 346.5613, loss: 0.0899, loss_g: 0.0433, loss_d: 0.0466, d_real: 0.0532, d_fake: 0.0433, kt: 0.14980047, M: 0.11625688\n",
      "Epoch: [ 3] [ 201/ 633] time: 364.2191, loss: 0.1008, loss_g: 0.0491, loss_d: 0.0517, d_real: 0.0578, d_fake: 0.0491, kt: 0.15563155, M: 0.12440868\n",
      "Epoch: [ 3] [ 301/ 633] time: 381.8739, loss: 0.1051, loss_g: 0.0569, loss_d: 0.0482, d_real: 0.0558, d_fake: 0.0569, kt: 0.16083241, M: 0.11040412\n",
      "Epoch: [ 3] [ 401/ 633] time: 399.4590, loss: 0.1134, loss_g: 0.0627, loss_d: 0.0507, d_real: 0.0569, d_fake: 0.0627, kt: 0.16592826, M: 0.10815583\n",
      "Epoch: [ 3] [ 501/ 633] time: 417.1197, loss: 0.1019, loss_g: 0.0544, loss_d: 0.0475, d_real: 0.0558, d_fake: 0.0544, kt: 0.17186504, M: 0.11311511\n",
      "Test data shuffle ....\n",
      "Test data shuffle Done\n",
      "Epoch: [ 3] [ 601/ 633] time: 434.9274, loss: 0.1216, loss_g: 0.0694, loss_d: 0.0522, d_real: 0.0631, d_fake: 0.0694, kt: 0.17731019, M: 0.11985896\n",
      "Epoch: [ 4] [  68/ 633] time: 452.5980, loss: 0.1421, loss_g: 0.0920, loss_d: 0.0501, d_real: 0.0643, d_fake: 0.0920, kt: 0.18188199, M: 0.10094283\n",
      "Epoch: [ 4] [ 168/ 633] time: 470.2539, loss: 0.1394, loss_g: 0.0999, loss_d: 0.0395, d_real: 0.0564, d_fake: 0.0999, kt: 0.18617480, M: 0.06936073\n",
      "Epoch: [ 4] [ 268/ 633] time: 487.9123, loss: 0.1286, loss_g: 0.0879, loss_d: 0.0407, d_real: 0.0552, d_fake: 0.0879, kt: 0.18953332, M: 0.07754493\n",
      "Epoch: [ 4] [ 368/ 633] time: 505.5739, loss: 0.1267, loss_g: 0.0789, loss_d: 0.0478, d_real: 0.0601, d_fake: 0.0789, kt: 0.19249037, M: 0.10127736\n",
      "Epoch: [ 4] [ 468/ 633] time: 523.2332, loss: 0.1391, loss_g: 0.0992, loss_d: 0.0399, d_real: 0.0557, d_fake: 0.0992, kt: 0.19562749, M: 0.06794234\n",
      "Epoch: [ 4] [ 568/ 633] time: 540.8937, loss: 0.1178, loss_g: 0.0719, loss_d: 0.0459, d_real: 0.0573, d_fake: 0.0719, kt: 0.19808332, M: 0.10013339\n",
      "Epoch: [ 5] [  35/ 633] time: 558.4588, loss: 0.1242, loss_g: 0.0783, loss_d: 0.0459, d_real: 0.0595, d_fake: 0.0783, kt: 0.19763835, M: 0.10014388\n",
      "Epoch: [ 5] [ 135/ 633] time: 576.1329, loss: 0.1355, loss_g: 0.0954, loss_d: 0.0401, d_real: 0.0558, d_fake: 0.0954, kt: 0.19933904, M: 0.07204711\n",
      "Epoch: [ 5] [ 235/ 633] time: 593.7912, loss: 0.1843, loss_g: 0.1411, loss_d: 0.0432, d_real: 0.0676, d_fake: 0.1411, kt: 0.19943823, M: 0.07351718\n",
      "Epoch: [ 5] [ 335/ 633] time: 611.4522, loss: 0.1633, loss_g: 0.1222, loss_d: 0.0411, d_real: 0.0598, d_fake: 0.1222, kt: 0.19889912, M: 0.06240446\n",
      "Epoch: [ 5] [ 435/ 633] time: 629.1166, loss: 0.1934, loss_g: 0.1467, loss_d: 0.0467, d_real: 0.0701, d_fake: 0.1467, kt: 0.19693078, M: 0.07666525\n",
      "Epoch: [ 5] [ 535/ 633] time: 646.7759, loss: 0.1400, loss_g: 0.0988, loss_d: 0.0412, d_real: 0.0577, d_fake: 0.0988, kt: 0.19445514, M: 0.07429373\n",
      "Epoch: [ 6] [   2/ 633] time: 664.4387, loss: 0.2299, loss_g: 0.1701, loss_d: 0.0598, d_real: 0.0798, d_fake: 0.1701, kt: 0.19147581, M: 0.09028669\n",
      "Epoch: [ 6] [ 102/ 633] time: 682.1002, loss: 0.1830, loss_g: 0.1439, loss_d: 0.0391, d_real: 0.0621, d_fake: 0.1439, kt: 0.18953701, M: 0.08184642\n",
      "Epoch: [ 6] [ 202/ 633] time: 699.7688, loss: 0.1392, loss_g: 0.0914, loss_d: 0.0478, d_real: 0.0634, d_fake: 0.0914, kt: 0.18529677, M: 0.09883706\n",
      "Epoch: [ 6] [ 302/ 633] time: 717.4429, loss: 0.1998, loss_g: 0.0982, loss_d: 0.1015, d_real: 0.1024, d_fake: 0.0982, kt: 0.00978201, M: 0.20893442\n",
      "Epoch: [ 6] [ 402/ 633] time: 735.1112, loss: 0.1286, loss_g: 0.0549, loss_d: 0.0737, d_real: 0.0746, d_fake: 0.0549, kt: 0.01869316, M: 0.16886076\n",
      "Epoch: [ 6] [ 502/ 633] time: 752.7822, loss: 0.1287, loss_g: 0.0662, loss_d: 0.0624, d_real: 0.0642, d_fake: 0.0662, kt: 0.02618973, M: 0.12623221\n",
      "Epoch: [ 6] [ 602/ 633] time: 770.4413, loss: 0.1273, loss_g: 0.0686, loss_d: 0.0587, d_real: 0.0609, d_fake: 0.0686, kt: 0.03204359, M: 0.11409948\n",
      "Epoch: [ 7] [  69/ 633] time: 788.0935, loss: 0.1122, loss_g: 0.0540, loss_d: 0.0582, d_real: 0.0601, d_fake: 0.0540, kt: 0.03769477, M: 0.12640550\n",
      "Epoch: [ 7] [ 169/ 633] time: 805.7521, loss: 0.1000, loss_g: 0.0431, loss_d: 0.0569, d_real: 0.0588, d_fake: 0.0431, kt: 0.04483024, M: 0.13334038\n",
      "Epoch: [ 7] [ 269/ 633] time: 823.4116, loss: 0.0922, loss_g: 0.0347, loss_d: 0.0575, d_real: 0.0593, d_fake: 0.0347, kt: 0.05237456, M: 0.14326035\n",
      "Epoch: [ 7] [ 369/ 633] time: 841.0608, loss: 0.0907, loss_g: 0.0368, loss_d: 0.0539, d_real: 0.0561, d_fake: 0.0368, kt: 0.05999698, M: 0.13138044\n",
      "Test data shuffle ....\n",
      "Test data shuffle Done\n",
      "Epoch: [ 7] [ 469/ 633] time: 858.8010, loss: 0.0851, loss_g: 0.0337, loss_d: 0.0514, d_real: 0.0537, d_fake: 0.0337, kt: 0.06736454, M: 0.12725190\n",
      "Epoch: [ 7] [ 569/ 633] time: 876.4582, loss: 0.0823, loss_g: 0.0338, loss_d: 0.0485, d_real: 0.0510, d_fake: 0.0338, kt: 0.07477311, M: 0.11911047\n",
      "Epoch: [ 8] [  36/ 633] time: 894.0658, loss: 0.0790, loss_g: 0.0306, loss_d: 0.0484, d_real: 0.0509, d_fake: 0.0306, kt: 0.08213469, M: 0.12192022\n",
      "Epoch: [ 8] [ 136/ 633] time: 911.7171, loss: 0.0849, loss_g: 0.0373, loss_d: 0.0476, d_real: 0.0509, d_fake: 0.0373, kt: 0.08919144, M: 0.11545886\n",
      "Epoch: [ 8] [ 236/ 633] time: 929.3655, loss: 0.0891, loss_g: 0.0400, loss_d: 0.0491, d_real: 0.0526, d_fake: 0.0400, kt: 0.09593555, M: 0.11784749\n",
      "Epoch: [ 8] [ 336/ 633] time: 947.0123, loss: 0.0855, loss_g: 0.0341, loss_d: 0.0514, d_real: 0.0548, d_fake: 0.0341, kt: 0.10303796, M: 0.13038173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 8] [ 436/ 633] time: 964.6594, loss: 0.0892, loss_g: 0.0381, loss_d: 0.0510, d_real: 0.0549, d_fake: 0.0381, kt: 0.11030916, M: 0.12654268\n",
      "Epoch: [ 8] [ 536/ 633] time: 982.3093, loss: 0.0871, loss_g: 0.0368, loss_d: 0.0503, d_real: 0.0541, d_fake: 0.0368, kt: 0.11732055, M: 0.12545872\n",
      "Epoch: [ 9] [   3/ 633] time: 999.9710, loss: 0.0827, loss_g: 0.0379, loss_d: 0.0448, d_real: 0.0492, d_fake: 0.0379, kt: 0.12401154, M: 0.10971176\n",
      "Epoch: [ 9] [ 103/ 633] time: 1017.6240, loss: 0.0776, loss_g: 0.0341, loss_d: 0.0435, d_real: 0.0479, d_fake: 0.0341, kt: 0.13050417, M: 0.10947919\n",
      "Epoch: [ 9] [ 203/ 633] time: 1035.2766, loss: 0.0960, loss_g: 0.0459, loss_d: 0.0501, d_real: 0.0559, d_fake: 0.0459, kt: 0.13659308, M: 0.12170314\n",
      "Epoch: [ 9] [ 303/ 633] time: 1052.9418, loss: 0.0872, loss_g: 0.0426, loss_d: 0.0446, d_real: 0.0506, d_fake: 0.0426, kt: 0.14262047, M: 0.10908695\n",
      "Epoch: [ 9] [ 403/ 633] time: 1070.5901, loss: 0.1041, loss_g: 0.0574, loss_d: 0.0466, d_real: 0.0550, d_fake: 0.0574, kt: 0.14818259, M: 0.10742718\n",
      "Epoch: [ 9] [ 503/ 633] time: 1088.2495, loss: 0.0932, loss_g: 0.0509, loss_d: 0.0423, d_real: 0.0500, d_fake: 0.0509, kt: 0.15354557, M: 0.09913580\n",
      "Epoch: [ 9] [ 603/ 633] time: 1105.9168, loss: 0.1018, loss_g: 0.0604, loss_d: 0.0414, d_real: 0.0507, d_fake: 0.0604, kt: 0.15848081, M: 0.09160579\n",
      "Epoch: [10] [  70/ 633] time: 1123.5710, loss: 0.0923, loss_g: 0.0481, loss_d: 0.0442, d_real: 0.0518, d_fake: 0.0481, kt: 0.16327233, M: 0.10738715\n",
      "Epoch: [10] [ 170/ 633] time: 1141.2236, loss: 0.1143, loss_g: 0.0755, loss_d: 0.0388, d_real: 0.0507, d_fake: 0.0755, kt: 0.16786716, M: 0.07660575\n",
      "Epoch: [10] [ 270/ 633] time: 1158.8749, loss: 0.1227, loss_g: 0.0754, loss_d: 0.0473, d_real: 0.0597, d_fake: 0.0754, kt: 0.17220351, M: 0.10381879\n",
      "Epoch: [10] [ 370/ 633] time: 1176.5338, loss: 0.1138, loss_g: 0.0708, loss_d: 0.0430, d_real: 0.0547, d_fake: 0.0708, kt: 0.17648209, M: 0.09312982\n",
      "Epoch: [10] [ 470/ 633] time: 1194.1846, loss: 0.1323, loss_g: 0.0923, loss_d: 0.0400, d_real: 0.0561, d_fake: 0.0923, kt: 0.18029471, M: 0.07589817\n",
      "Epoch: [10] [ 570/ 633] time: 1211.8304, loss: 0.1098, loss_g: 0.0680, loss_d: 0.0418, d_real: 0.0535, d_fake: 0.0680, kt: 0.18367919, M: 0.09266821\n",
      "Epoch: [11] [  37/ 633] time: 1229.4845, loss: 0.1120, loss_g: 0.0723, loss_d: 0.0398, d_real: 0.0526, d_fake: 0.0723, kt: 0.18623749, M: 0.08557827\n",
      "Epoch: [11] [ 137/ 633] time: 1247.1342, loss: 0.1135, loss_g: 0.0752, loss_d: 0.0383, d_real: 0.0517, d_fake: 0.0752, kt: 0.18944828, M: 0.07997472\n",
      "Epoch: [11] [ 237/ 633] time: 1264.7920, loss: 0.1611, loss_g: 0.1252, loss_d: 0.0359, d_real: 0.0582, d_fake: 0.1252, kt: 0.19119376, M: 0.06699980\n",
      "Epoch: [11] [ 337/ 633] time: 1282.4456, loss: 0.1342, loss_g: 0.0884, loss_d: 0.0458, d_real: 0.0619, d_fake: 0.0884, kt: 0.19326176, M: 0.09727350\n",
      "Test data shuffle ....\n",
      "Test data shuffle Done\n",
      "Epoch: [11] [ 437/ 633] time: 1300.1985, loss: 0.1130, loss_g: 0.0718, loss_d: 0.0412, d_real: 0.0544, d_fake: 0.0718, kt: 0.19459141, M: 0.09130298\n",
      "Epoch: [11] [ 537/ 633] time: 1317.8537, loss: 0.1551, loss_g: 0.1072, loss_d: 0.0479, d_real: 0.0653, d_fake: 0.1072, kt: 0.19426177, M: 0.08871187\n",
      "Epoch: [12] [   4/ 633] time: 1335.5158, loss: 0.1910, loss_g: 0.1548, loss_d: 0.0361, d_real: 0.0631, d_fake: 0.1548, kt: 0.19474725, M: 0.09173266\n",
      "Epoch: [12] [ 104/ 633] time: 1353.1954, loss: 0.1625, loss_g: 0.1255, loss_d: 0.0370, d_real: 0.0587, d_fake: 0.1255, kt: 0.19072411, M: 0.06679600\n",
      "Epoch: [12] [ 204/ 633] time: 1370.8480, loss: 0.1383, loss_g: 0.0963, loss_d: 0.0420, d_real: 0.0598, d_fake: 0.0963, kt: 0.19167775, M: 0.08301831\n",
      "Epoch: [12] [ 304/ 633] time: 1388.5093, loss: 0.2133, loss_g: 0.1865, loss_d: 0.0269, d_real: 0.0590, d_fake: 0.1865, kt: 0.19008790, M: 0.12747131\n",
      "Epoch: [12] [ 404/ 633] time: 1406.1721, loss: 0.1308, loss_g: 0.0943, loss_d: 0.0365, d_real: 0.0533, d_fake: 0.0943, kt: 0.18649040, M: 0.06561698\n",
      "Epoch: [12] [ 504/ 633] time: 1423.8220, loss: 0.1436, loss_g: 0.1027, loss_d: 0.0409, d_real: 0.0591, d_fake: 0.1027, kt: 0.18670312, M: 0.07451327\n",
      "Epoch: [12] [ 604/ 633] time: 1441.4770, loss: 0.1491, loss_g: 0.1106, loss_d: 0.0385, d_real: 0.0578, d_fake: 0.1106, kt: 0.18495422, M: 0.06268550\n",
      "Epoch: [13] [  71/ 633] time: 1459.1357, loss: 0.1317, loss_g: 0.0937, loss_d: 0.0380, d_real: 0.0526, d_fake: 0.0937, kt: 0.18180548, M: 0.06392339\n",
      "Epoch: [13] [ 171/ 633] time: 1476.7287, loss: 0.2314, loss_g: 0.1935, loss_d: 0.0379, d_real: 0.0695, d_fake: 0.1935, kt: 0.16996470, M: 0.12402876\n",
      "Epoch: [13] [ 271/ 633] time: 1494.3802, loss: 0.1172, loss_g: 0.0702, loss_d: 0.0470, d_real: 0.0587, d_fake: 0.0702, kt: 0.17159561, M: 0.10594948\n",
      "Epoch: [13] [ 371/ 633] time: 1512.0325, loss: 0.1142, loss_g: 0.0722, loss_d: 0.0420, d_real: 0.0534, d_fake: 0.0722, kt: 0.17409468, M: 0.08802792\n",
      "Epoch: [13] [ 471/ 633] time: 1529.6905, loss: 0.1126, loss_g: 0.0768, loss_d: 0.0358, d_real: 0.0486, d_fake: 0.0768, kt: 0.17217001, M: 0.06881921\n",
      "Epoch: [13] [ 571/ 633] time: 1547.3459, loss: 0.1156, loss_g: 0.0759, loss_d: 0.0397, d_real: 0.0515, d_fake: 0.0759, kt: 0.17398001, M: 0.07851500\n",
      "Epoch: [14] [  38/ 633] time: 1565.0060, loss: 0.1420, loss_g: 0.0990, loss_d: 0.0430, d_real: 0.0595, d_fake: 0.0990, kt: 0.17273381, M: 0.07953465\n",
      "Epoch: [14] [ 138/ 633] time: 1582.6638, loss: 0.5032, loss_g: 0.4541, loss_d: 0.0490, d_real: 0.0705, d_fake: 0.4541, kt: 0.16502202, M: 0.38363200\n",
      "Epoch: [14] [ 238/ 633] time: 1600.3361, loss: 0.1555, loss_g: 0.1076, loss_d: 0.0478, d_real: 0.0641, d_fake: 0.1076, kt: 0.15891003, M: 0.08458143\n",
      "Epoch: [14] [ 338/ 633] time: 1617.9943, loss: 0.1123, loss_g: 0.0694, loss_d: 0.0429, d_real: 0.0538, d_fake: 0.0694, kt: 0.16254344, M: 0.09193065\n",
      "Epoch: [14] [ 438/ 633] time: 1635.6571, loss: 0.1133, loss_g: 0.0687, loss_d: 0.0446, d_real: 0.0553, d_fake: 0.0687, kt: 0.16256273, M: 0.09721969\n",
      "Epoch: [14] [ 538/ 633] time: 1653.3206, loss: 0.1748, loss_g: 0.1398, loss_d: 0.0350, d_real: 0.0552, d_fake: 0.1398, kt: 0.15828729, M: 0.08456631\n",
      "Epoch: [15] [   5/ 633] time: 1670.9739, loss: 0.1518, loss_g: 0.1098, loss_d: 0.0419, d_real: 0.0568, d_fake: 0.1098, kt: 0.15424853, M: 0.06045850\n",
      "Epoch: [15] [ 105/ 633] time: 1688.6323, loss: 0.1521, loss_g: 0.1173, loss_d: 0.0349, d_real: 0.0504, d_fake: 0.1173, kt: 0.15437806, M: 0.06691010\n",
      "Epoch: [15] [ 205/ 633] time: 1706.2936, loss: 0.1680, loss_g: 0.1277, loss_d: 0.0403, d_real: 0.0592, d_fake: 0.1277, kt: 0.15508320, M: 0.06853542\n",
      "Test data shuffle ....\n",
      "Test data shuffle Done\n",
      "Epoch: [15] [ 305/ 633] time: 1724.0332, loss: 0.2034, loss_g: 0.1681, loss_d: 0.0352, d_real: 0.0605, d_fake: 0.1681, kt: 0.15602637, M: 0.10761901\n",
      "Epoch: [15] [ 405/ 633] time: 1741.7016, loss: 0.1750, loss_g: 0.0956, loss_d: 0.0794, d_real: 0.0911, d_fake: 0.0956, kt: 0.12981010, M: 0.17764706\n",
      "Epoch: [15] [ 505/ 633] time: 1759.3584, loss: 0.1244, loss_g: 0.0708, loss_d: 0.0535, d_real: 0.0626, d_fake: 0.0708, kt: 0.13431142, M: 0.11690378\n",
      "Epoch: [15] [ 605/ 633] time: 1777.0078, loss: 0.0927, loss_g: 0.0495, loss_d: 0.0431, d_real: 0.0499, d_fake: 0.0495, kt: 0.13957999, M: 0.10021082\n",
      "Epoch: [16] [  72/ 633] time: 1794.6558, loss: 0.0911, loss_g: 0.0490, loss_d: 0.0421, d_real: 0.0490, d_fake: 0.0490, kt: 0.14444964, M: 0.09808524\n",
      "Epoch: [16] [ 172/ 633] time: 1812.3114, loss: 0.1084, loss_g: 0.0686, loss_d: 0.0397, d_real: 0.0494, d_fake: 0.0686, kt: 0.14865211, M: 0.07969823\n",
      "Epoch: [16] [ 272/ 633] time: 1829.9628, loss: 0.1218, loss_g: 0.0774, loss_d: 0.0444, d_real: 0.0551, d_fake: 0.0774, kt: 0.15188671, M: 0.08780298\n",
      "Epoch: [16] [ 372/ 633] time: 1847.6185, loss: 0.1180, loss_g: 0.0710, loss_d: 0.0471, d_real: 0.0571, d_fake: 0.0710, kt: 0.15111973, M: 0.10027051\n",
      "Epoch: [16] [ 472/ 633] time: 1865.2841, loss: 0.3183, loss_g: 0.2744, loss_d: 0.0440, d_real: 0.0742, d_fake: 0.2744, kt: 0.13923760, M: 0.20012127\n",
      "Epoch: [16] [ 572/ 633] time: 1882.9368, loss: 0.1078, loss_g: 0.0656, loss_d: 0.0423, d_real: 0.0514, d_fake: 0.0656, kt: 0.14284808, M: 0.08856985\n",
      "Epoch: [17] [  39/ 633] time: 1900.5918, loss: 0.1375, loss_g: 0.0916, loss_d: 0.0459, d_real: 0.0569, d_fake: 0.0916, kt: 0.14555514, M: 0.07919164\n",
      "Epoch: [17] [ 139/ 633] time: 1918.2193, loss: 0.2544, loss_g: 0.2157, loss_d: 0.0387, d_real: 0.0572, d_fake: 0.2157, kt: 0.14436669, M: 0.15852312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17] [ 239/ 633] time: 1935.8757, loss: 0.1220, loss_g: 0.0826, loss_d: 0.0394, d_real: 0.0506, d_fake: 0.0826, kt: 0.14131557, M: 0.06928741\n",
      "Epoch: [17] [ 339/ 633] time: 1953.5292, loss: 0.1101, loss_g: 0.0653, loss_d: 0.0448, d_real: 0.0535, d_fake: 0.0653, kt: 0.14381904, M: 0.09521371\n",
      "Epoch: [17] [ 439/ 633] time: 1971.1733, loss: 0.1630, loss_g: 0.1264, loss_d: 0.0367, d_real: 0.0520, d_fake: 0.1264, kt: 0.14423100, M: 0.07438330\n",
      "Epoch: [17] [ 539/ 633] time: 1988.8233, loss: 0.1314, loss_g: 0.0909, loss_d: 0.0405, d_real: 0.0525, d_fake: 0.0909, kt: 0.13782526, M: 0.06671017\n",
      "Epoch: [18] [   6/ 633] time: 2006.4709, loss: 0.0869, loss_g: 0.0438, loss_d: 0.0431, d_real: 0.0488, d_fake: 0.0438, kt: 0.13629506, M: 0.10274213\n",
      "Epoch: [18] [ 106/ 633] time: 2024.1162, loss: 0.1364, loss_g: 0.0889, loss_d: 0.0475, d_real: 0.0578, d_fake: 0.0889, kt: 0.13628962, M: 0.08450207\n",
      "Epoch: [18] [ 206/ 633] time: 2041.7836, loss: 0.1379, loss_g: 0.0734, loss_d: 0.0644, d_real: 0.0729, d_fake: 0.0734, kt: 0.12348802, M: 0.14538594\n",
      "Epoch: [18] [ 306/ 633] time: 2059.4353, loss: 0.0961, loss_g: 0.0440, loss_d: 0.0521, d_real: 0.0576, d_fake: 0.0440, kt: 0.12922040, M: 0.12886035\n",
      "Epoch: [18] [ 406/ 633] time: 2077.0934, loss: 0.1877, loss_g: 0.1522, loss_d: 0.0355, d_real: 0.0537, d_fake: 0.1522, kt: 0.13138583, M: 0.09850653\n",
      "Epoch: [18] [ 506/ 633] time: 2094.7351, loss: 0.0924, loss_g: 0.0500, loss_d: 0.0424, d_real: 0.0488, d_fake: 0.0500, kt: 0.13364555, M: 0.09645296\n",
      "Epoch: [18] [ 606/ 633] time: 2112.3792, loss: 0.0891, loss_g: 0.0473, loss_d: 0.0419, d_real: 0.0480, d_fake: 0.0473, kt: 0.13943050, M: 0.09660570\n",
      "Epoch: [19] [  73/ 633] time: 2130.0209, loss: 0.0788, loss_g: 0.0343, loss_d: 0.0445, d_real: 0.0491, d_fake: 0.0343, kt: 0.14427399, M: 0.11304279\n",
      "Test data shuffle ....\n",
      "Test data shuffle Done\n",
      "Epoch: [19] [ 173/ 633] time: 2147.7572, loss: 0.0932, loss_g: 0.0573, loss_d: 0.0359, d_real: 0.0441, d_fake: 0.0573, kt: 0.14967699, M: 0.07505461\n",
      "Epoch: [19] [ 273/ 633] time: 2165.4068, loss: 0.0939, loss_g: 0.0500, loss_d: 0.0439, d_real: 0.0513, d_fake: 0.0500, kt: 0.15420058, M: 0.10398320\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'JpegImageFile' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-922635ccb6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# TRAIN / TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac2ab73240df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_flag)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mbatch_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mbatch_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mbatch_target_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_label_vector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac2ab73240df>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mbatch_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mbatch_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mbatch_target_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_label_vector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3470f713e0ce>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# rgb to bgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'JpegImageFile' and 'float'"
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"cbegan_exprs_6_gamma_2\"\n",
    "train_flag = True\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = BEGAN(sess, project_name)\n",
    "\n",
    "        # TRAIN / TEST\n",
    "        if train_flag:\n",
    "            model.train(train_flag)\n",
    "        else:\n",
    "            model.test(train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
