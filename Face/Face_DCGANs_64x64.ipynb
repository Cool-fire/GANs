{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying conditional GANs in Faces image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the face image\n",
    "\n",
    "https://github.com/leblancfg/autocrop/blob/master/autocrop/autocrop.py\n",
    "\n",
    "http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/\n",
    "\n",
    "Install opencv\n",
    "\n",
    "https://www.pyimagesearch.com/2015/07/20/install-opencv-3-0-and-python-3-4-on-ubuntu/\n",
    "\n",
    "Auto-Encoder\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb\n",
    "\n",
    "awesome python library\n",
    "https://pypi.python.org/pypi/tqdm\n",
    "\n",
    "https://github.com/carpedm20/BEGAN-tensorflow/blob/master/models.py\n",
    "\n",
    "https://arxiv.org/pdf/1703.10717.pdf\n",
    "\n",
    "loss=nan solution\n",
    "\n",
    "http://blog.csdn.net/threadroc/article/details/53967560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preparing training set\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "class CK:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] #Define emotions\n",
    "        self.file_paths = []\n",
    "        self.start_of_new_batch = 0\n",
    "        self.height = 60\n",
    "        self.width = 60\n",
    "        self.channels = 1\n",
    "        self.images_tensor=[]\n",
    "\n",
    "        for emotion in self.emotions:\n",
    "            self.file_paths.extend(glob.glob(\"./Face_data/CK+/dataset/%s/*\" %emotion))\n",
    "\n",
    "        random.shuffle(self.file_paths)\n",
    "        size = len(self.file_paths)\n",
    "        # Here generating a tensor of type string that include all the filename with png extention\n",
    "        filename_queue  = tf.train.string_input_producer(self.file_paths)\n",
    "        # Initializing a file Reader\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        # Get all the files mentioned ie filename queue and\n",
    "        # returns the  the file name and the pixelvalue in form of a tensor !\n",
    "        _,imagefile= image_reader.read(filename_queue)\n",
    "        image = tf.image.decode_png(imagefile, channels=self.channels)\n",
    "        image.set_shape((self.height,self.width,self.channels))\n",
    "        images = tf.train.shuffle_batch([image],\n",
    "                                        batch_size=size,\n",
    "                                        capacity=size,\n",
    "                                        min_after_dequeue=0)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            # Coordinate the loading of image files.\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            # Get a batch of image tensors.\n",
    "            self.images_tensor = sess.run(images)\n",
    "\n",
    "            # Finish off the filename queue coordinator.\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "        \n",
    "        self.images_tensor = self.images_tensor / 255. \n",
    "        \n",
    "#         img = mpimg.imread(paths_for_this_patch[0])\n",
    "#         print(paths_for_this_patch[0])\n",
    "#         im = images_tensor[0][0]\n",
    "#         plt.figure(1)\n",
    "#         plt.imshow(im.squeeze(), cmap='gray')\n",
    "#         plt.figure(2)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "    def next_batch(self, size):\n",
    "        \n",
    "        if size > len(self.file_paths):\n",
    "            size = len(self.file_paths)\n",
    "        if size <= 0:\n",
    "            return []\n",
    "            \n",
    "        # Get images for this batch\n",
    "        this_patch = []\n",
    "        start = self.start_of_new_batch\n",
    "        end = (start + size - 1) % len(self.file_paths)\n",
    "        if start > end:\n",
    "            this_patch = self.images_tensor[start:len(self.file_paths)]\n",
    "            this_patch = np.concatenate((this_patch,\n",
    "                                         (self.images_tensor[0:end+1])))\n",
    "        else:\n",
    "            this_patch = self.images_tensor[start:end+1]\n",
    "            \n",
    "        self.start_of_new_batch = (start + size) % len(self.file_paths)\n",
    "        \n",
    "\n",
    "        return  this_patch\n",
    "\n",
    "train_CK = CK()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_shape(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks\n",
    "\n",
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \n",
    "    # size: (out_height/(2*2*2)) * (out_width/(2*2*2)) * n\n",
    "    zP = slim.fully_connected(z,8*8*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=lrelu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,8,8,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=128,kernel_size=[4,4],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=lrelu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=64,kernel_size=[4,4],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=lrelu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=32,kernel_size=[4,4],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=lrelu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[64,64],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "\n",
    "The line of concat y with dis1 refer to:\n",
    "https://github.com/Eyyub/tensorflow-cdcgan/blob/master/model/discriminator.py\n",
    "\n",
    "Idea comes from IcGAN:\n",
    "https://arxiv.org/abs/1611.06355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)    \n",
    "    dis2 = slim.convolution2d(dis1,128,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    dis3 = slim.convolution2d(dis2,256,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    dis4 = slim.convolution2d(dis3,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv4',weights_initializer=initializer)\n",
    "    \n",
    "    d_out = slim.fully_connected(slim.flatten(dis4),1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,64,64,1],dtype=tf.float32) #Real images\n",
    "\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz, reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "# d_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(Dx,1e-10,1.0)) + tf.log(1.-tf.clip_by_value(Dg,1e-10,1.0))) #This optimizes the discriminator.\n",
    "# g_loss = -tf.reduce_mean(tf.log(tf.clip_by_value(Dg,1e-10,1.0))) #This optimizes the generator.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tim/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Gen Loss: 0.474374 Disc Loss: 1.49134\n",
      "Gen Loss: 2.42464 Disc Loss: 0.0478356\n",
      "Gen Loss: 6.94997 Disc Loss: 0.0246885\n",
      "Gen Loss: 7.40649 Disc Loss: 0.0111768\n",
      "Gen Loss: 6.95707 Disc Loss: 0.00608021\n",
      "Gen Loss: 7.80961 Disc Loss: 0.00520655\n",
      "Gen Loss: 6.67809 Disc Loss: 0.00374943\n",
      "Gen Loss: 5.998 Disc Loss: 0.00320411\n",
      "Gen Loss: 10.2151 Disc Loss: 0.00480482\n",
      "Gen Loss: 10.4406 Disc Loss: 0.00187392\n",
      "Gen Loss: 9.43026 Disc Loss: 0.00212386\n",
      "Gen Loss: 8.63427 Disc Loss: 0.00129623\n",
      "Gen Loss: 8.23524 Disc Loss: 0.00200823\n",
      "Gen Loss: 9.59312 Disc Loss: 0.00173623\n",
      "Gen Loss: 7.90111 Disc Loss: 0.00157419\n",
      "Gen Loss: 9.04184 Disc Loss: 0.00107176\n",
      "Gen Loss: 6.28197 Disc Loss: 0.00127906\n",
      "Gen Loss: 7.30457 Disc Loss: 0.0014044\n",
      "Gen Loss: 7.85243 Disc Loss: 0.00153004\n",
      "Gen Loss: 6.19726 Disc Loss: 0.000901374\n",
      "Gen Loss: 9.40575 Disc Loss: 0.00116119\n",
      "Gen Loss: 10.8523 Disc Loss: 0.00124411\n",
      "Gen Loss: 7.74235 Disc Loss: 0.00123962\n",
      "Gen Loss: 8.65796 Disc Loss: 0.000999765\n",
      "Gen Loss: 7.80082 Disc Loss: 0.000797932\n",
      "Gen Loss: 7.24222 Disc Loss: 0.000831905\n",
      "Gen Loss: 8.08641 Disc Loss: 0.000697236\n",
      "Gen Loss: 8.42649 Disc Loss: 0.000265573\n",
      "Gen Loss: 9.31264 Disc Loss: 0.000387297\n",
      "Gen Loss: 8.28075 Disc Loss: 0.00028829\n",
      "Gen Loss: 9.07386 Disc Loss: 0.000624175\n",
      "Gen Loss: 8.26463 Disc Loss: 0.000220489\n",
      "Gen Loss: 10.1716 Disc Loss: 0.000570866\n",
      "Gen Loss: 10.4669 Disc Loss: 0.000389968\n",
      "Gen Loss: 7.23543 Disc Loss: 0.00035825\n",
      "Gen Loss: 8.68465 Disc Loss: 0.000541663\n",
      "Gen Loss: 7.54694 Disc Loss: 0.000673407\n",
      "Gen Loss: 8.47306 Disc Loss: 0.000523182\n",
      "Gen Loss: 9.83876 Disc Loss: 0.000330465\n",
      "Gen Loss: 7.96576 Disc Loss: 0.000664811\n",
      "Gen Loss: 7.84519 Disc Loss: 0.00041061\n",
      "Gen Loss: 8.01327 Disc Loss: 0.000565784\n",
      "Gen Loss: 8.3315 Disc Loss: 0.000485952\n",
      "Gen Loss: 7.42692 Disc Loss: 0.000506898\n",
      "Gen Loss: 8.17315 Disc Loss: 0.000532318\n",
      "Gen Loss: 10.0255 Disc Loss: 0.000368464\n",
      "Gen Loss: 8.04142 Disc Loss: 0.000484332\n",
      "Gen Loss: 8.36189 Disc Loss: 0.000266639\n",
      "Gen Loss: 8.0779 Disc Loss: 0.000454739\n",
      "Gen Loss: 7.69563 Disc Loss: 0.000247166\n",
      "Gen Loss: 8.27849 Disc Loss: 0.000456973\n",
      "Gen Loss: 8.67791 Disc Loss: 0.00024897\n",
      "Gen Loss: 7.66755 Disc Loss: 0.00036558\n",
      "Gen Loss: 10.8172 Disc Loss: 0.00032802\n",
      "Gen Loss: 8.00001 Disc Loss: 0.000450907\n",
      "Gen Loss: 9.49144 Disc Loss: 0.00027335\n",
      "Gen Loss: 10.0221 Disc Loss: 0.000412168\n",
      "Gen Loss: 8.01004 Disc Loss: 0.000252544\n",
      "Gen Loss: 11.8466 Disc Loss: 0.000209572\n",
      "Gen Loss: 10.442 Disc Loss: 0.000390052\n",
      "Gen Loss: 7.7703 Disc Loss: 0.000472352\n",
      "Gen Loss: 9.36455 Disc Loss: 0.000247682\n",
      "Gen Loss: 8.96088 Disc Loss: 0.000408714\n",
      "Gen Loss: 9.69293 Disc Loss: 0.000224876\n",
      "Gen Loss: 11.3224 Disc Loss: 0.000261966\n",
      "Gen Loss: 11.1993 Disc Loss: 0.000211458\n",
      "Gen Loss: 8.55278 Disc Loss: 0.000246368\n",
      "Gen Loss: 8.42595 Disc Loss: 0.000343657\n",
      "Gen Loss: 11.0549 Disc Loss: 0.000246827\n",
      "Gen Loss: 8.60103 Disc Loss: 0.000363802\n",
      "Gen Loss: 8.3323 Disc Loss: 0.000263437\n",
      "Gen Loss: 10.9621 Disc Loss: 0.000247431\n",
      "Gen Loss: 8.02713 Disc Loss: 0.000322154\n",
      "Gen Loss: 8.6252 Disc Loss: 0.000126313\n",
      "Gen Loss: 8.92119 Disc Loss: 0.000122554\n",
      "Gen Loss: 11.3982 Disc Loss: 0.000112062\n",
      "Gen Loss: 8.72398 Disc Loss: 0.000246337\n",
      "Gen Loss: 9.64216 Disc Loss: 9.30162e-05\n",
      "Gen Loss: 10.2366 Disc Loss: 0.000113514\n",
      "Gen Loss: 8.74643 Disc Loss: 0.000254247\n",
      "Gen Loss: 9.8189 Disc Loss: 0.000202771\n",
      "Gen Loss: 8.84514 Disc Loss: 0.000118452\n",
      "Gen Loss: 9.2172 Disc Loss: 0.000224143\n",
      "Gen Loss: 9.65795 Disc Loss: 8.28054e-05\n",
      "Gen Loss: 8.78584 Disc Loss: 0.000166437\n",
      "Gen Loss: 12.0346 Disc Loss: 9.84974e-05\n",
      "Gen Loss: 10.4903 Disc Loss: 9.93746e-05\n",
      "Gen Loss: 9.1947 Disc Loss: 0.000261023\n",
      "Gen Loss: 9.61954 Disc Loss: 0.000181106\n",
      "Gen Loss: 8.89339 Disc Loss: 0.00019784\n",
      "Gen Loss: 8.51476 Disc Loss: 0.000141508\n",
      "Gen Loss: 9.17476 Disc Loss: 0.000138819\n",
      "Gen Loss: 9.13638 Disc Loss: 0.000192884\n",
      "Gen Loss: 10.3237 Disc Loss: 0.0001671\n",
      "Gen Loss: 8.94302 Disc Loss: 0.000113614\n",
      "Gen Loss: 8.44778 Disc Loss: 0.000143471\n",
      "Gen Loss: 9.16447 Disc Loss: 0.000260068\n",
      "Gen Loss: 9.53361 Disc Loss: 0.000227096\n",
      "Gen Loss: 9.02535 Disc Loss: 0.000140343\n",
      "Gen Loss: 8.46641 Disc Loss: 0.000206912\n",
      "Gen Loss: 8.74294 Disc Loss: 0.000101774\n",
      "Saved Model\n",
      "Gen Loss: 9.04039 Disc Loss: 0.00024249\n",
      "Gen Loss: 10.0064 Disc Loss: 0.000154318\n",
      "Gen Loss: 9.53118 Disc Loss: 0.00016076\n",
      "Gen Loss: 8.69031 Disc Loss: 0.000194708\n",
      "Gen Loss: 9.05655 Disc Loss: 0.000175526\n",
      "Gen Loss: 8.61068 Disc Loss: 0.00019996\n",
      "Gen Loss: 9.23136 Disc Loss: 0.000329682\n",
      "Gen Loss: 9.09451 Disc Loss: 0.00019547\n",
      "Gen Loss: 9.79283 Disc Loss: 0.000169112\n",
      "Gen Loss: 8.55336 Disc Loss: 0.000201512\n",
      "Gen Loss: 9.80375 Disc Loss: 8.37534e-05\n",
      "Gen Loss: 9.15199 Disc Loss: 8.87922e-05\n",
      "Gen Loss: 9.11456 Disc Loss: 0.000189176\n",
      "Gen Loss: 11.6685 Disc Loss: 0.000150038\n",
      "Gen Loss: 8.34079 Disc Loss: 0.000245528\n",
      "Gen Loss: 9.21487 Disc Loss: 0.000206301\n",
      "Gen Loss: 9.7442 Disc Loss: 6.23354e-05\n",
      "Gen Loss: 8.5015 Disc Loss: 0.000187545\n",
      "Gen Loss: 9.90859 Disc Loss: 0.000235473\n",
      "Gen Loss: 9.88026 Disc Loss: 6.1475e-05\n",
      "Gen Loss: 10.6233 Disc Loss: 0.000144831\n",
      "Gen Loss: 10.0703 Disc Loss: 0.000120895\n",
      "Gen Loss: 10.9338 Disc Loss: 9.31994e-05\n",
      "Gen Loss: 8.38289 Disc Loss: 0.000155201\n",
      "Gen Loss: 9.16617 Disc Loss: 0.000138408\n",
      "Gen Loss: 9.26401 Disc Loss: 0.000208399\n",
      "Gen Loss: 9.1038 Disc Loss: 0.000274934\n",
      "Gen Loss: 8.38914 Disc Loss: 0.000146761\n",
      "Gen Loss: 9.09677 Disc Loss: 0.000111739\n",
      "Gen Loss: 10.9314 Disc Loss: 0.00015034\n",
      "Gen Loss: 8.87717 Disc Loss: 0.000141671\n",
      "Gen Loss: 10.1772 Disc Loss: 9.97054e-05\n",
      "Gen Loss: 11.4028 Disc Loss: 0.000103373\n",
      "Gen Loss: 8.2058 Disc Loss: 0.000110889\n",
      "Gen Loss: 9.23505 Disc Loss: 0.000103145\n",
      "Gen Loss: 8.31569 Disc Loss: 8.71854e-05\n",
      "Gen Loss: 9.28821 Disc Loss: 0.000162352\n",
      "Gen Loss: 9.10648 Disc Loss: 0.000156096\n",
      "Gen Loss: 10.8363 Disc Loss: 0.000128308\n",
      "Gen Loss: 10.435 Disc Loss: 0.000100611\n",
      "Gen Loss: 9.92733 Disc Loss: 7.0756e-05\n",
      "Gen Loss: 9.67397 Disc Loss: 6.02035e-05\n",
      "Gen Loss: 10.2356 Disc Loss: 8.70594e-05\n",
      "Gen Loss: 10.0193 Disc Loss: 6.26092e-05\n",
      "Gen Loss: 11.7797 Disc Loss: 7.67139e-05\n",
      "Gen Loss: 9.50801 Disc Loss: 8.52264e-05\n",
      "Gen Loss: 10.4239 Disc Loss: 8.06146e-05\n",
      "Gen Loss: 10.3805 Disc Loss: 6.02409e-05\n",
      "Gen Loss: 10.2321 Disc Loss: 4.13627e-05\n",
      "Gen Loss: 10.8637 Disc Loss: 4.40534e-05\n",
      "Gen Loss: 10.7829 Disc Loss: 2.78576e-05\n",
      "Gen Loss: 10.7056 Disc Loss: 2.78224e-05\n",
      "Gen Loss: 11.0299 Disc Loss: 2.18096e-05\n",
      "Gen Loss: 10.013 Disc Loss: 3.05106e-05\n",
      "Gen Loss: 10.9936 Disc Loss: 2.26608e-05\n",
      "Gen Loss: 9.39348 Disc Loss: 6.78721e-05\n",
      "Gen Loss: 9.57772 Disc Loss: 8.92096e-05\n",
      "Gen Loss: 10.7146 Disc Loss: 2.23592e-05\n",
      "Gen Loss: 9.59158 Disc Loss: 7.42089e-05\n",
      "Gen Loss: 10.2054 Disc Loss: 9.46787e-05\n",
      "Gen Loss: 9.45568 Disc Loss: 5.70901e-05\n",
      "Gen Loss: 10.1676 Disc Loss: 5.8137e-05\n",
      "Gen Loss: 10.8965 Disc Loss: 8.24789e-05\n",
      "Gen Loss: 9.47248 Disc Loss: 6.40193e-05\n",
      "Gen Loss: 10.8808 Disc Loss: 7.79708e-05\n",
      "Gen Loss: 10.1881 Disc Loss: 3.96175e-05\n",
      "Gen Loss: 10.8541 Disc Loss: 0.000127241\n",
      "Gen Loss: 9.73434 Disc Loss: 6.12551e-05\n",
      "Gen Loss: 9.44293 Disc Loss: 8.8172e-05\n",
      "Gen Loss: 9.37877 Disc Loss: 0.000138533\n",
      "Gen Loss: 8.7191 Disc Loss: 6.2133e-05\n",
      "Gen Loss: 11.6565 Disc Loss: 9.01605e-05\n",
      "Gen Loss: 9.27664 Disc Loss: 6.94274e-05\n",
      "Gen Loss: 9.31869 Disc Loss: 0.000113876\n",
      "Gen Loss: 10.8344 Disc Loss: 0.00010012\n",
      "Gen Loss: 9.55799 Disc Loss: 8.22884e-05\n",
      "Gen Loss: 9.70374 Disc Loss: 0.000100499\n",
      "Gen Loss: 10.0266 Disc Loss: 0.00010199\n",
      "Gen Loss: 9.96298 Disc Loss: 7.30841e-05\n",
      "Gen Loss: 10.1981 Disc Loss: 6.61146e-05\n",
      "Gen Loss: 9.32125 Disc Loss: 7.88208e-05\n",
      "Gen Loss: 9.87279 Disc Loss: 7.1164e-05\n",
      "Gen Loss: 10.664 Disc Loss: 7.14535e-05\n",
      "Gen Loss: 11.1028 Disc Loss: 4.62303e-05\n",
      "Gen Loss: 10.167 Disc Loss: 2.33382e-05\n",
      "Gen Loss: 12.9375 Disc Loss: 4.00397e-05\n",
      "Gen Loss: 10.3965 Disc Loss: 4.61351e-05\n",
      "Gen Loss: 9.54827 Disc Loss: 0.000119864\n",
      "Gen Loss: 10.2986 Disc Loss: 3.6914e-05\n",
      "Gen Loss: 12.1244 Disc Loss: 0.000131351\n",
      "Gen Loss: 9.52271 Disc Loss: 6.34369e-05\n",
      "Gen Loss: 11.5322 Disc Loss: 0.000161557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 10.8764 Disc Loss: 0.000121261\n",
      "Gen Loss: 10.9777 Disc Loss: 4.93967e-05\n",
      "Gen Loss: 11.5192 Disc Loss: 6.16312e-05\n",
      "Gen Loss: 10.5815 Disc Loss: 5.23875e-05\n",
      "Gen Loss: 12.1294 Disc Loss: 5.81407e-05\n",
      "Gen Loss: 11.1949 Disc Loss: 9.34898e-05\n",
      "Gen Loss: 11.2224 Disc Loss: 9.11425e-05\n",
      "Gen Loss: 11.5113 Disc Loss: 8.47562e-05\n",
      "Saved Model\n",
      "Gen Loss: 10.2995 Disc Loss: 4.91452e-05\n",
      "Gen Loss: 10.515 Disc Loss: 2.42213e-05\n",
      "Gen Loss: 11.2638 Disc Loss: 1.161e-05\n",
      "Gen Loss: 9.80352 Disc Loss: 9.5321e-05\n",
      "Gen Loss: 10.7467 Disc Loss: 6.65441e-05\n",
      "Gen Loss: 9.14486 Disc Loss: 3.75824e-05\n",
      "Gen Loss: 13.4981 Disc Loss: 1.64658e-05\n",
      "Gen Loss: 9.00276 Disc Loss: 2.23388e-05\n",
      "Gen Loss: 10.4525 Disc Loss: 6.0004e-05\n",
      "Gen Loss: 8.39618 Disc Loss: 2.0929e-05\n",
      "Gen Loss: 8.82989 Disc Loss: 8.0756e-05\n",
      "Gen Loss: 10.3287 Disc Loss: 1.81689e-05\n",
      "Gen Loss: 12.2856 Disc Loss: 2.76691e-05\n",
      "Gen Loss: 8.97308 Disc Loss: 5.56765e-05\n",
      "Gen Loss: 11.379 Disc Loss: 8.69402e-05\n",
      "Gen Loss: 12.6465 Disc Loss: 3.45922e-05\n",
      "Gen Loss: 11.3425 Disc Loss: 4.60795e-05\n",
      "Gen Loss: 8.93613 Disc Loss: 7.31855e-05\n",
      "Gen Loss: 9.34739 Disc Loss: 5.98009e-05\n",
      "Gen Loss: 11.5198 Disc Loss: 6.56481e-05\n",
      "Gen Loss: 13.7597 Disc Loss: 0.000106079\n",
      "Gen Loss: 9.65244 Disc Loss: 6.36426e-05\n",
      "Gen Loss: 7.49094 Disc Loss: 0.000120649\n",
      "Gen Loss: 14.1614 Disc Loss: 8.48382e-05\n",
      "Gen Loss: 9.12743 Disc Loss: 2.56409e-05\n",
      "Gen Loss: 11.8964 Disc Loss: 4.54675e-05\n",
      "Gen Loss: 8.83875 Disc Loss: 1.31288\n",
      "Gen Loss: 1.01449 Disc Loss: 0.534467\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Saved Model\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n",
      "Gen Loss: nan Disc Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4bd9df49ba76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;31m#Transform it to be between -1 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Pad the images so the are 64x64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 50000 #Total number of iterations to use. 50000 -> 2hrs\n",
    "sample_size = 4 # Number of sample to be view\n",
    "sample_directory = './Face_DCGANs_figs' #Directory to save sample images from generator in.\n",
    "model_directory = './Face_DCGANs_models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        #zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        zs = np.random.normal(size=(batch_size, z_size))\n",
    "        xs = train_CK.next_batch(batch_size) #Draw a sample batch from dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,60,60,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 64x64\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        if i % 2 == 0:\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "            _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[sample_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            \n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:4],[4,64,64]),[2,2],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
