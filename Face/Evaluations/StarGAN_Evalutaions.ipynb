{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "import numpy as np\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path, flip=False): # [0,255] to [-1,1]\n",
    "    img = scm.imread(img_path) \n",
    "    if flip:\n",
    "        img = np.fliplr(img)\n",
    "    img = img * 2. /255. - 1.\n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def get_label(path, size):\n",
    "    label = int(path[-5])\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[ label ] = 1.0\n",
    "    one_hot[ one_hot==0 ] = 0.0\n",
    "    return one_hot\n",
    "\n",
    "def inverse_image(img): # [-1,1] to [0,255]\n",
    "    img = (img + 1.) / 2. * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n",
    "\n",
    "def pair_expressions(paths):\n",
    "    subject_exprs = []\n",
    "    subject_pairs = []\n",
    "    all_pairs = []\n",
    "    last_subject = 0\n",
    "\n",
    "    # Pair all expression of a subject\n",
    "    for path in paths:\n",
    "        subject = int(path[-10:-6])\n",
    "\n",
    "        if subject != last_subject and last_subject != 0:\n",
    "            subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "            all_pairs.extend(subject_pairs)\n",
    "            subject_exprs = []\n",
    "\n",
    "        subject_exprs.append(path)\n",
    "        last_subject = subject\n",
    "\n",
    "    # Last subject\n",
    "    subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "    all_pairs.extend(subject_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "def get_shape_c(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "def conv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(2.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                             use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def deconv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d_transpose\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d_transpose(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                                       use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(input_shape*output_shape)))\n",
    "    initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "def instance_norm(input, name='instance_norm'):\n",
    "    with tf.variable_scope(name):\n",
    "        depth = input.get_shape()[3]\n",
    "        scale = tf.get_variable('scale', [depth], initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n",
    "        offset = tf.get_variable('offset', [depth], initializer=tf.constant_initializer(0.0))\n",
    "        mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n",
    "        epsilon = 1e-5\n",
    "        inv = tf.rsqrt(variance + epsilon)\n",
    "        normalized = (input-mean)*inv\n",
    "        return scale*normalized + offset\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "def l2_loss(x, y):\n",
    "    return tf.reduce_mean(tf.square(x - y))\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))\n",
    "\n",
    "def lrelu(x, leak=0.01, name='lrelu'): #lrelu(x, leak=0.2, name='lrelu'):\n",
    "    return tf.maximum(x, leak*x)\n",
    "\n",
    "def gradient_penalty(real, fake, f):\n",
    "        def interpolate(a, b):\n",
    "            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n",
    "            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n",
    "            inter = a + alpha * (b - a)\n",
    "            inter.set_shape(a.get_shape().as_list())\n",
    "            return inter\n",
    "\n",
    "        x = interpolate(real, fake)\n",
    "        pred, _ = f(x, reuse=True)\n",
    "        gradients = tf.gradients(pred, x)[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=3))\n",
    "        gp = tf.reduce_mean((slopes - 1.)**2)\n",
    "        return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"../Face_data/Faces_with_expression_label/dataset_128x128\" #args.data_dir #./Data\n",
    "        self.dataset = \"expr\" #args.dataset  # celeba\n",
    "        self.data_size = 128 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "        self.data_label_vector_size = 7 #size of one-hot-encoded label vector\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 100 #50 #args.niter\n",
    "        self.niter_snapshot = 500 #args.nsnapshot\n",
    "        self.max_to_keep = 50 #args.max_to_keep models\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 0.5 #args.gamma\n",
    "        self.input_size = 128 #args.input_size\n",
    "        self.embedding = 128 #64 #args.embedding\n",
    "        \n",
    "        self.lambda_cls = 5.\n",
    "        self.lambda_recon = 10.\n",
    "        self.lambda_gp = 10.\n",
    "\n",
    "        \n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = './'\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'model_StarGAN')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "#         # etc.\n",
    "#         if not os.path.exists('assets_ae'):\n",
    "#             os.makedirs('assets_ae')\n",
    "#         make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='x')\n",
    "        self.x_c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='x_c')\n",
    "        self.target = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='target')\n",
    "        self.target_c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='target_c')\n",
    "        self.alpha = tf.placeholder(tf.float32, shape=[None, 1], name='alpha')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "\n",
    "        # Generator\n",
    "        self.G_f = self.generator(self.x, self.target_c)\n",
    "        self.G_recon = self.generator(self.G_f, self.x_c, reuse=True)\n",
    "        \n",
    "        self.G_test = self.generator(self.x, self.target_c, reuse=True)\n",
    "        \n",
    "        # Discriminator\n",
    "        self.D_f, self.D_f_cls = self.discriminator(self.G_f)\n",
    "        self.D_target, self.D_target_cls = self.discriminator(self.target, reuse=True) # discriminate with the target\n",
    "        \n",
    "        # Gradient Penalty\n",
    "        self.real_data = tf.reshape(self.target, [-1, self.data_size*self.data_size*3]) # interpolate with target\n",
    "        self.fake_data = tf.reshape(self.G_f, [-1, self.data_size*self.data_size*3])\n",
    "        self.diff = self.fake_data - self.real_data\n",
    "        self.interpolate = self.real_data + self.alpha*self.diff\n",
    "        \n",
    "        self.inter_reshape = tf.reshape(self.interpolate, [-1, self.data_size, self.data_size, 3])\n",
    "        self.G_inter, _ = self.discriminator(self.inter_reshape, reuse=True)\n",
    "        \n",
    "        self.grad = tf.gradients(self.G_inter, \n",
    "                                 xs=[self.inter_reshape])[0]\n",
    "        self.slopes = tf.sqrt(tf.reduce_sum(tf.square(self.grad), axis=[1,2,3]))\n",
    "        self.gp = tf.reduce_mean(tf.square(self.slopes - 1.))\n",
    "        \n",
    "        \n",
    "        # Wasserstein loss\n",
    "        self.wd = tf.reduce_mean(self.D_target) - tf.reduce_mean(self.D_f)\n",
    "        self.L_adv_D = -self.wd + self.gp * self.lambda_gp\n",
    "        self.L_adv_G = -tf.reduce_mean(self.D_f)\n",
    "        \n",
    "        self.L_D_cls = tf.reduce_mean(cross_entropy(labels=self.target_c, logits=self.D_target_cls))# discriminate with the target\n",
    "        self.L_G_cls = tf.reduce_mean(cross_entropy(labels=self.target_c, logits=self.D_f_cls))\n",
    "        self.L_G_recon = l1_loss(self.x, self.G_recon)\n",
    "                \n",
    "        self.L_D = self.L_adv_D + self.lambda_cls * self.L_D_cls\n",
    "        self.L_G = self.L_adv_G + self.lambda_cls * self.L_G_cls + self.lambda_recon * self.L_G_recon\n",
    "        \n",
    "\n",
    "        # Variables\n",
    "        D_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator\")\n",
    "        G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"generator\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.opt_D = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.L_D, var_list=D_vars)\n",
    "        self.opt_G = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.L_G, var_list=G_vars)\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            print(\"Error: Could not load the model\")\n",
    "\n",
    "    def test(self, train_flag=True):\n",
    "        print('Test Sample Generation...')\n",
    "        # generate output\n",
    "        \n",
    "        batch_size = 64\n",
    "        img_size = 128\n",
    "        label_size = self.data_label_vector_size\n",
    "        \n",
    "        # load data test\n",
    "        data_path = \"../Face_data/Celeba/dataset_128x128\"\n",
    "        data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "        \n",
    "        n_data = len(data)\n",
    "        \n",
    "        n_batch = int(n_data / batch_size)+1\n",
    "        \n",
    "        for batch_id in range(n_batch):\n",
    "        \n",
    "            clear_output()\n",
    "            print(\"Batch Number / Total Batches : [{0:5d}/{1:5d}]\".format(batch_id, n_batch))\n",
    "            \n",
    "            test_files = data[batch_id*batch_size: (batch_id+1)*batch_size]\n",
    "            test_data = [get_image(test_file) for test_file in test_files]\n",
    "            test_data = np.repeat(test_data, [label_size]*batch_size, axis=0)\n",
    "\n",
    "            # get one-hot labels\n",
    "            int_labels = list(range(label_size))\n",
    "            one_hot = np.zeros((label_size, label_size))\n",
    "            one_hot[np.arange(label_size), int_labels] = 1\n",
    "            target_labels = np.tile(one_hot, (batch_size, 1))\n",
    "\n",
    "\n",
    "            output_gen = (self.sess.run(self.G_test, feed_dict={self.x: test_data, \n",
    "                                                                self.target_c: target_labels}))  # generator output\n",
    "\n",
    "            output_gen = [inverse_image(i) for i in output_gen]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                fname = test_files[i].split(\"/\")[-1].split(\".\")[0]\n",
    "                for j in range(label_size):\n",
    "\n",
    "                    save_path = self.project_dir + './gen_celeba/' + fname + '_' + str(j) + '.jpg'\n",
    "                    # output save\n",
    "                    scm.imsave(save_path, output_gen[i*label_size+j])\n",
    "                    \n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StarGAN(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "\n",
    "\n",
    "    def generator(self, x, c, reuse=None):\n",
    "        with tf.variable_scope('generator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = 64\n",
    "            image_size = self.data_size\n",
    "            c_num = self.data_label_vector_size\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = tf.concat([x, tf.tile(tf.reshape(c, [-1, 1, 1, get_shape_c(c)[-1]]),\\\n",
    "                                      [1, x.get_shape().as_list()[1], x.get_shape().as_list()[2], 1])],\\\n",
    "                          axis=3)\n",
    "            \n",
    "            # Down-sampling\n",
    "            x = conv(x, [7, 7, 3+c_num, f], stride=1, padding=p, name='ds_1')\n",
    "            x = instance_norm(x, 'in_ds_1')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [4, 4, f, f*2], stride=2, padding=p, name='ds_2')\n",
    "            x = instance_norm(x, 'in_ds_2')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [4, 4, f*2, f*4], stride=2, padding=p, name='ds_3')\n",
    "            x = instance_norm(x, 'in_ds_3')\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Bottleneck\n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_1a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_1a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_1b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_1b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_2a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_2a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_2b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_2b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_3a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_3a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_3b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_3b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_4a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_4a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_4b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_4b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_5a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_5a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_5b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_5b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_6a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_6a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_6b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_6b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Up-sampling\n",
    "            x = deconv(x, [4, 4, f*4, f*2], stride=2, padding=p, name='us_1')\n",
    "            x = instance_norm(x, 'in_us_1')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = deconv(x, [4, 4, f*2, f], stride=2, padding=p, name='us_2')\n",
    "            x = instance_norm(x, 'in_us_2')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [7, 7, f, 3], stride=1, padding=p, name='us_3')\n",
    "\n",
    "            x = tf.nn.tanh(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def discriminator(self, x, reuse=None):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = 64\n",
    "            f_max = f*8\n",
    "            image_size = self.data_size\n",
    "            k_size = int(image_size / np.power(2, 5))            \n",
    "            c_num = self.data_label_vector_size\n",
    "            p = \"SAME\"\n",
    "            \n",
    "            x = conv(x, [4, 4, 3, f], stride=2, padding=p, name='conv_1')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f, f*2], stride=2, padding=p, name='conv_2')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*2, f*4], stride=2, padding=p, name='conv_3')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*4, f*8], stride=2, padding=p, name='conv_4')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*8, f*16], stride=2, padding=p, name='conv_5')\n",
    "            x = lrelu(x)\n",
    "            \n",
    "            if image_size == 128:\n",
    "                x = conv(x, [4, 4, f*16, f*32], stride=2, padding=p, name='conv_6')\n",
    "                x = lrelu(x)\n",
    "                f_max = f_max * 2\n",
    "                k_size = int(k_size / 2)\n",
    "                \n",
    "            out_src = conv(x, [3, 3, f_max, 1], stride=1, padding=p, name='conv_out_src')\n",
    "            out_cls = conv(x, [k_size, k_size, f_max, c_num], stride=1, name='conv_out_cls')\n",
    "        \n",
    "        return out_src, tf.squeeze(out_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"StarGAN_Face_3_\"\n",
    "train_flag = False\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = StarGAN(sess, project_name)\n",
    "        model.test(train_flag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
