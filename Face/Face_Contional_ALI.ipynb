{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy from https://github.com/Heumi/BEGAN-tensorflow/tree/master/src/function\n",
    "\n",
    "To test the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf\n",
    "\n",
    "https://github.com/nightrome/really-awesome-gan\n",
    "\n",
    "https://arxiv.org/pdf/1705.09966.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1703.05192.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1606.03657.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1704.02166.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1709.03842.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "import numpy as np\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path):\n",
    "    img = scm.imread(img_path)/255. \n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def get_label(path, size):\n",
    "    label = int(path[-5])\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[ label ] = 0.9\n",
    "    one_hot[ one_hot==0 ] = 0.1\n",
    "    return one_hot\n",
    "\n",
    "def inverse_image(img):\n",
    "    img = img * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n",
    "\n",
    "def pair_expressions(paths):\n",
    "    subject_exprs = []\n",
    "    subject_pairs = []\n",
    "    all_pairs = []\n",
    "    last_subject = 0\n",
    "\n",
    "    # Pair all expression of a subject\n",
    "    for path in paths:\n",
    "        subject = int(path[-9:-6])\n",
    "\n",
    "        if subject != last_subject and last_subject != 0:\n",
    "            subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "            all_pairs.extend(subject_pairs)\n",
    "            subject_exprs = []\n",
    "\n",
    "        subject_exprs.append(path)\n",
    "        last_subject = subject\n",
    "\n",
    "    # Last subject\n",
    "    subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "    all_pairs.extend(subject_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "def get_shape_c(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dropout = tf.layers.dropout\n",
    "bn = tf.layers.batch_normalization\n",
    "\n",
    "def conv(x, filter_shape, bias=False, stride=1, padding=\"VALID\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.01)\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                             use_bias=bias, kernel_initializer=initializer)\n",
    "    return x\n",
    "\n",
    "def deconv(x, filter_shape, bias=False, stride=1, padding=\"VALID\", name=\"conv2d_transpose\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.01)\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d_transpose(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                                       use_bias=bias, kernel_initializer=initializer)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))\n",
    "\n",
    "def add_nontied_bias(x, initializer=None):\n",
    "    with tf.variable_scope('add_nontied_bias'):\n",
    "        if initializer is not None:\n",
    "            bias = tf.get_variable('bias', shape=x.get_shape().as_list()[1:], trainable=True, initializer=initializer)\n",
    "        else:\n",
    "            bias = tf.get_variable('bias', shape=x.get_shape().as_list()[1:], trainable=True, initializer=tf.zeros_initializer())\n",
    "\n",
    "        output = x + bias\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"./Face_data/Faces_with_expression_label/dataset_64x64\" #args.data_dir #./Data\n",
    "        self.dataset = \"expr\" #args.dataset  # celeba\n",
    "        self.data_size = 64 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "        self.data_label_vector_size = 7 #size of one-hot-encoded label vector\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 5000 #50 #args.niter\n",
    "        self.niter_snapshot = 500 #args.nsnapshot\n",
    "        self.max_to_keep = 5 #args.max_to_keep\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 32#16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 0.5 #args.gamma\n",
    "        self.filter_number = 64 #args.filter_number\n",
    "        self.input_size = 64 #args.input_size\n",
    "        self.embedding = 512 #64 #args.embedding\n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = 'assets/{0}_{1}_{2}_{3}/'.format(self.project, self.dataset, self.data_opt, self.data_size)\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'models')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "        # etc.\n",
    "        if not os.path.exists('assets'):\n",
    "            os.makedirs('assets')\n",
    "        make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x1 = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='x1')\n",
    "        self.x2 = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='x2')\n",
    "        self.c1 = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='c1')\n",
    "        self.c2 = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='c2')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "        self.train_g = tf.placeholder(tf.bool, [])\n",
    "        self.train_d = tf.placeholder(tf.bool, [])\n",
    "\n",
    "        self.G_l = self.encoder(self.x2, self.c2, train=self.train_g, reuse=False)\n",
    "        self.l1 = self.encoder(self.x1, self.c1, train=False, reuse=True)\n",
    "        self.G_i = self.decoder(self.l1, self.c1, train=self.train_g, reuse=False)\n",
    "        \n",
    "        self.D_G_i, self.D_G_i_logits = self.discriminator(self.G_i, self.l1, self.c1, train=self.train_d, reuse=False)\n",
    "        self.D_G_l, self.D_G_l_logits = self.discriminator(self.x2, self.G_l, self.c2, train=self.train_d, reuse=True)\n",
    "\n",
    "        # Generate test image\n",
    "        self.l_test = self.encoder(self.x1, self.c1, train=False, reuse=True)\n",
    "        self.G_test = self.decoder(self.l_test, self.c2, train=False, reuse=True)\n",
    "\n",
    "        # Loss\n",
    "        self.d_loss = tf.reduce_mean(tf.nn.softplus(-self.D_G_l_logits) + tf.nn.softplus(self.D_G_i_logits))\n",
    "        self.g_loss = tf.reduce_mean(tf.nn.softplus(self.D_G_l_logits) + tf.nn.softplus(-self.D_G_i_logits))\n",
    "\n",
    "        # Variables\n",
    "        encoder_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"encoder_\")\n",
    "        decoder_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"decoder_\")\n",
    "        discriminator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator_\")\n",
    "\n",
    "        # Optimizer\n",
    "        # update parameters for BN first\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.opt_d = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.d_loss, var_list=discriminator_vars)\n",
    "            self.opt_g = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.g_loss, var_list=encoder_vars+decoder_vars)\n",
    "\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            self.saver.save(self.sess, self.ckpt_model_name, write_meta_graph=True)\n",
    "\n",
    "        # Summary\n",
    "        if self.flag:\n",
    "            tf.summary.scalar('loss/d_loss', self.d_loss)\n",
    "            tf.summary.scalar('loss/g_loss', self.g_loss)\n",
    "            #tf.summary.scalar('loss/test_loss', self.test_loss)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(self.project_dir, self.sess.graph)\n",
    "\n",
    "    def train(self, train_flag):\n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "        #test_data_path = \"./Face_data/Faces_with_expression_label/dataset_64x64\" # expressions\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        print('Shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Shuffle Done')\n",
    "\n",
    "        # initial parameter\n",
    "        start_time = time.time()\n",
    "        lr = np.float32(self.learning_rate)\n",
    "        self.count = 0\n",
    "\n",
    "        for epoch in range(self.niter):\n",
    "            batch_idxs = len(data) // self.batch_size\n",
    "\n",
    "            for idx in range(0, batch_idxs):\n",
    "                self.count += 1\n",
    "\n",
    "                batch_files = data[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "                batch_x1 = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "                batch_x2 = [get_image(batch_file[1]) for batch_file in batch_files]\n",
    "                batch_c1 = [get_label(batch_file[0], self.data_label_vector_size) for batch_file in batch_files]\n",
    "                batch_c2 = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "                # opt & feed list \n",
    "                opts = [self.opt_d, self.opt_g, self.d_loss, self.g_loss, self.merged]\n",
    "                feed_dict = {self.x1: batch_x1, self.x2: batch_x2, \n",
    "                             self.c1: batch_c1, self.c2: batch_c2,\n",
    "                             self.lr: lr, self.train_g: True, self.train_d: True}\n",
    "          \n",
    "                # run tensorflow\n",
    "                _, _, d_loss, g_loss, summary = self.sess.run(opts, feed_dict=feed_dict)\n",
    "                \n",
    "                if self.count % 2 == 1:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, \"\n",
    "                          \"d_loss: %.6f, g_loss: %.6f\"\n",
    "                          % (epoch, idx, batch_idxs, time.time() - start_time,\n",
    "                             d_loss, g_loss))\n",
    "\n",
    "                # write train summary\n",
    "                self.writer.add_summary(summary, self.count)\n",
    "\n",
    "                # Test during Training\n",
    "                if self.count % self.niter_snapshot == (self.niter_snapshot - 1):\n",
    "                    # save & test\n",
    "                    self.saver.save(self.sess, self.ckpt_model_name, global_step=self.count, write_meta_graph=False)\n",
    "                    self.test_expr(train_flag)\n",
    "                    #self.test_celebra(train_flag)\n",
    "\n",
    "    def test_celebra(self, train_flag=True):\n",
    "        print('Test Sample Generation...')\n",
    "        # generate output\n",
    "        img_num = 8*8\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        in_img_num = output_f\n",
    "        img_size = self.data_size\n",
    "        gen_img_num = img_num - output_f\n",
    "        label_size = self.data_label_vector_size\n",
    "        \n",
    "        # load data test\n",
    "        data_path = \"./Face_data/Celeba/dataset_64x64\" # expressions\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # shuffle test data\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "\n",
    "        test_files = data[0: output_f]\n",
    "        test_data = [get_image(test_file) for test_file in test_files]\n",
    "        test_data = np.repeat(test_data, [label_size]*in_img_num, axis=0)\n",
    "        test_data_o = [scm.imread(test_file) for test_file in test_files]\n",
    "        \n",
    "        # get one-hot labels\n",
    "        int_labels = list(range(label_size))\n",
    "        one_hot = np.zeros((label_size, label_size))\n",
    "        one_hot[np.arange(label_size), int_labels] = 1\n",
    "        target_labels = np.tile(one_hot, (output_f, 1))\n",
    "        \n",
    "        \n",
    "        output_gen = (self.sess.run(self.ae_test, feed_dict={self.x: test_data, \n",
    "                                                             self.c: target_labels}))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(gen_img_num)]\n",
    "\n",
    "        for i in range(output_f):\n",
    "            for j in range(output_f):\n",
    "                if j == 0:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_data_o[i]\n",
    "                else:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[(j-1) + (i * int(output_f-1))]\n",
    "\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_celebra_output.bmp', im_output_gen)\n",
    "        \n",
    "    def test_expr(self, train_flag=True):\n",
    "        print('Test Sample Generation...')\n",
    "        # generate output\n",
    "        img_num =  36 #self.batch_size\n",
    "        display_img_num = int(img_num / 3)\n",
    "        img_size = self.data_size\n",
    "\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        \n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # Test data shuffle\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        \n",
    "        batch_files = data[0: display_img_num]\n",
    "        test_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "        test_inputs_o = [scm.imread((batch_file[0])) for batch_file in batch_files]\n",
    "        test_targets = [scm.imread((batch_file[1])) for batch_file in batch_files]\n",
    "        test_inputs_labels = [get_label(batch_file[0], self.data_label_vector_size) for batch_file in batch_files]\n",
    "        test_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "        feed_dict={self.x1: test_inputs, self.c1: test_inputs_labels, self.c2: test_target_labels}\n",
    "        output_gen = (self.sess.run(self.G_test, feed_dict=feed_dict))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(display_img_num)]\n",
    "\n",
    "        for i in range(output_f): # row\n",
    "            for j in range(output_f): # col\n",
    "                if j % 3 == 0: # input img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_inputs_o[int(j / 3) + (i * int(output_f / 3))]\n",
    "                elif j % 3 == 1: # output img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[int(j / 3) + (i * int(output_f / 3))]\n",
    "                else: # target img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_targets[int(j / 3) + (i * int(output_f / 3))]\n",
    "                   \n",
    "\n",
    "        labels = np.argmax(test_target_labels, axis=1)\n",
    "        label_string = ''.join(str(int(l)) for l in labels)\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_' + label_string \n",
    "                   + '_output.bmp', im_output_gen)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ALI(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "\n",
    "\n",
    "    def encoder(self, x, c, reuse=None, train=True):\n",
    "        with tf.variable_scope('encoder_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            c_num = self.data_label_vector_size\n",
    "            h = self.embedding\n",
    "            p = \"VALID\"\n",
    "\n",
    "            with tf.variable_scope('block1'):\n",
    "                x = conv(x, [2, 2, 3, f], stride=1, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "\n",
    "                # Add classes as channels to the outcome of first layer as described in IcGAN paper\n",
    "                x = tf.concat([x, tf.tile(tf.reshape(c, [-1, 1, 1, get_shape_c(c)[-1]]),\\\n",
    "                                          [1, x.get_shape().as_list()[1], x.get_shape().as_list()[2], 1])],\\\n",
    "                                          axis=3)\n",
    "            \n",
    "            with tf.variable_scope('block2'):\n",
    "                x = conv(x, [7, 7, f+c_num, f*2], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block3'):\n",
    "                x = conv(x, [5, 5, f*2, f*4], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block4'):\n",
    "                x = conv(x, [7, 7, f*4, f*4], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block5'):\n",
    "                x = conv(x, [4, 4, f*8, f*8], stride=1, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block6'):\n",
    "                x = conv(x, [1, 1, f*8, h], stride=1, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block7_mu'):\n",
    "                x_mu = conv(x, [1, 1, h, h], stride=1, padding=p)\n",
    "                x_mu = add_nontied_bias(x_mu)\n",
    "\n",
    "            with tf.variable_scope('block7_sigma'):\n",
    "                x_sigma = conv(x, [1, 1, h, h], stride=1, padding=p)\n",
    "                x_sigma = add_nontied_bias(x_sigma)\n",
    "                x_sigma = tf.exp(x_sigma)\n",
    "                \n",
    "            rng = tf.random_normal(shape=tf.shape(x_mu))\n",
    "            x = (rng * x_sigma) + x_mu\n",
    "\n",
    "            #x = fc(x, h, name='fc')\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x, c, reuse=None, train=True):\n",
    "        with tf.variable_scope('decoder_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            c_num = self.data_label_vector_size\n",
    "            h = self.embedding\n",
    "            p = \"VALID\"\n",
    "            \n",
    "            c = tf.reshape(c, [-1, 1, 1, c_num])\n",
    "            x = tf.reshape(x, [-1, 1, 1, h])\n",
    "            x = tf.concat([x, c], axis=3)\n",
    "\n",
    "            with tf.variable_scope('block1'):\n",
    "                x = deconv(x, [4, 4, h+c_num, f*8], stride=1, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block2'):\n",
    "                x = deconv(x, [7, 7, f*8, f*4], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "            \n",
    "            with tf.variable_scope('block3'):\n",
    "                x = deconv(x, [5, 5, f*4, f*4], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block4'):\n",
    "                x = deconv(x, [7, 7, f*4, f*2], stride=2, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block5'):\n",
    "                x = deconv(x, [2, 2, f*2, f], stride=1, padding=p)\n",
    "                x = bn(x, axis=3, center=True, scale=True, training=train)\n",
    "                x = tf.nn.elu(x)\n",
    "                \n",
    "            with tf.variable_scope('block6'):\n",
    "                x = conv(x, [1, 1, f, 3], stride=1, bias=True, padding=p)\n",
    "                x = tf.nn.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def discriminator(self, im, latent, c, reuse=None, train=True):\n",
    "        with tf.variable_scope('discriminator_') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            \n",
    "            d_i = im\n",
    "            d_l = latent\n",
    "            f = self.filter_number\n",
    "            c_num = self.data_label_vector_size\n",
    "            h = self.embedding\n",
    "            p = \"VALID\"\n",
    "            \n",
    "            with tf.variable_scope('i_'):\n",
    "                \n",
    "                with tf.variable_scope('block1'):\n",
    "                    d_i = conv(d_i, [2, 2, 3, f], stride=1, bias=True, padding=p)\n",
    "                    d_i = bn(d_i, axis=3, center=True, scale=True, training=train)\n",
    "                    d_i = tf.nn.elu(d_i)\n",
    "                    \n",
    "                with tf.variable_scope('block2'):\n",
    "                    d_i = conv(d_i, [7, 7, f, f*2], stride=2, bias=True, padding=p)\n",
    "                    d_i = bn(d_i, axis=3, center=True, scale=True, training=train)\n",
    "                    d_i = tf.nn.elu(d_i)\n",
    "                \n",
    "                with tf.variable_scope('block3'):\n",
    "                    d_i = conv(d_i, [5, 5, f*2, f*4], stride=2, padding=p)\n",
    "                    d_i = bn(d_i, axis=3, center=True, scale=True, training=train)\n",
    "                    d_i = tf.nn.elu(d_i)\n",
    "                \n",
    "                with tf.variable_scope('block4'):\n",
    "                    d_i = conv(d_i, [7, 7, f*4, f*4], stride=2, bias=True, padding=p)\n",
    "                    d_i = bn(d_i, axis=3, center=True, scale=True, training=train)\n",
    "                    d_i = tf.nn.elu(d_i)\n",
    "                \n",
    "                with tf.variable_scope('block5'):\n",
    "                    d_i = conv(d_i, [4, 4, f*4, f*8], stride=1, bias=True, padding=p)\n",
    "                    d_i = bn(d_i, axis=3, center=True, scale=True, training=train)\n",
    "                    d_i = tf.nn.elu(d_i)\n",
    "                \n",
    "            with tf.variable_scope('l_'):\n",
    "                \n",
    "                d_l = tf.reshape(d_l, [-1, 1, 1, h])\n",
    "                \n",
    "                with tf.variable_scope('block1'):    \n",
    "                    d_l = dropout(d_l, rate=0.2, training=train)\n",
    "                    d_l = conv(d_l, [1, 1, h, f*16], bias=False, stride=1, padding=p)\n",
    "                    d_l = tf.nn.elu(d_l)\n",
    "                    \n",
    "                with tf.variable_scope('block2'):\n",
    "                    d_l = dropout(d_l, rate=0.2, training=train)\n",
    "                    d_l = conv(d_l, [1, 1, f*16, f*16], bias=False, stride=1, padding=p)\n",
    "                    d_l = tf.nn.elu(d_l)\n",
    "                    \n",
    "            with tf.variable_scope('ilc_'):\n",
    "                \n",
    "                c = tf.reshape(c, [-1, 1, 1, c_num])\n",
    "                d_ilc = tf.concat([d_i, d_l, c], axis=3)\n",
    "                \n",
    "                with tf.variable_scope('block1'):    \n",
    "                    d_ilc = dropout(d_ilc, rate=0.2, training=train)\n",
    "                    d_ilc = conv(d_ilc, [1, 1, f*8+f*16+c_num, f*32], stride=1, bias=True, padding=p)\n",
    "                    d_ilc = tf.nn.elu(d_ilc)\n",
    "                    \n",
    "                with tf.variable_scope('block2'):    \n",
    "                    d_ilc = dropout(d_ilc, rate=0.2, training=train)\n",
    "                    d_ilc = conv(d_ilc, [1, 1, f*32, f*32], stride=1, bias=True, padding=p)\n",
    "                    d_ilc = tf.nn.elu(d_ilc)\n",
    "                    \n",
    "                with tf.variable_scope('block3'):    \n",
    "                    d_ilc = dropout(d_ilc, rate=0.2, training=train)\n",
    "                    d_ilc = conv(d_ilc, [1, 1, f*32, 1], stride=1, bias=True, padding=p)\n",
    "            \n",
    "            logits = d_ilc\n",
    "            sig = tf.nn.sigmoid(d_ilc)\n",
    "            \n",
    "            return sig, logits\n",
    "                \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle ....\n",
      "Shuffle Done\n",
      "Epoch: [ 0] [   0/ 316] time: 2.2000, d_loss: 1.383710, g_loss: 1.390110\n",
      "Epoch: [ 0] [   2/ 316] time: 2.8546, d_loss: 1.375297, g_loss: 1.400406\n",
      "Epoch: [ 0] [   4/ 316] time: 3.5078, d_loss: 1.384886, g_loss: 1.390762\n",
      "Epoch: [ 0] [   6/ 316] time: 4.1624, d_loss: 1.363120, g_loss: 1.412010\n",
      "Epoch: [ 0] [   8/ 316] time: 4.8206, d_loss: 1.352259, g_loss: 1.430207\n",
      "Epoch: [ 0] [  10/ 316] time: 5.4786, d_loss: 1.310557, g_loss: 1.476757\n",
      "Epoch: [ 0] [  12/ 316] time: 6.1384, d_loss: 1.257354, g_loss: 1.537143\n",
      "Epoch: [ 0] [  14/ 316] time: 6.7927, d_loss: 1.196308, g_loss: 1.635256\n",
      "Epoch: [ 0] [  16/ 316] time: 7.4541, d_loss: 1.118630, g_loss: 1.781234\n",
      "Epoch: [ 0] [  18/ 316] time: 8.1126, d_loss: 0.983534, g_loss: 1.971413\n",
      "Epoch: [ 0] [  20/ 316] time: 8.7730, d_loss: 0.871104, g_loss: 2.346608\n",
      "Epoch: [ 0] [  22/ 316] time: 9.4296, d_loss: 0.947334, g_loss: 2.435038\n",
      "Epoch: [ 0] [  24/ 316] time: 10.0847, d_loss: 0.794357, g_loss: 2.689817\n",
      "Epoch: [ 0] [  26/ 316] time: 10.7387, d_loss: 0.830260, g_loss: 2.733917\n",
      "Epoch: [ 0] [  28/ 316] time: 11.3954, d_loss: 1.475805, g_loss: 2.178272\n",
      "Epoch: [ 0] [  30/ 316] time: 12.0492, d_loss: 0.829015, g_loss: 3.016900\n",
      "Epoch: [ 0] [  32/ 316] time: 12.7044, d_loss: 0.860757, g_loss: 3.339400\n",
      "Epoch: [ 0] [  34/ 316] time: 13.3633, d_loss: 0.509254, g_loss: 3.776459\n",
      "Epoch: [ 0] [  36/ 316] time: 14.0209, d_loss: 0.387107, g_loss: 4.038044\n",
      "Epoch: [ 0] [  38/ 316] time: 14.6807, d_loss: 1.584103, g_loss: 2.388543\n",
      "Epoch: [ 0] [  40/ 316] time: 15.3445, d_loss: 0.871164, g_loss: 2.608230\n",
      "Epoch: [ 0] [  42/ 316] time: 16.0050, d_loss: 0.559821, g_loss: 3.621977\n",
      "Epoch: [ 0] [  44/ 316] time: 16.6638, d_loss: 0.495724, g_loss: 4.617564\n",
      "Epoch: [ 0] [  46/ 316] time: 17.3236, d_loss: 0.461168, g_loss: 4.204583\n",
      "Epoch: [ 0] [  48/ 316] time: 17.9810, d_loss: 1.149117, g_loss: 3.542768\n",
      "Epoch: [ 0] [  50/ 316] time: 18.6363, d_loss: 0.891797, g_loss: 3.769441\n",
      "Epoch: [ 0] [  52/ 316] time: 19.2913, d_loss: 0.875835, g_loss: 2.859132\n",
      "Epoch: [ 0] [  54/ 316] time: 19.9508, d_loss: 0.703847, g_loss: 3.455529\n",
      "Epoch: [ 0] [  56/ 316] time: 20.6056, d_loss: 0.464463, g_loss: 4.669857\n",
      "Epoch: [ 0] [  58/ 316] time: 21.2608, d_loss: 0.460743, g_loss: 5.317104\n",
      "Epoch: [ 0] [  60/ 316] time: 21.9149, d_loss: 0.232265, g_loss: 5.756629\n",
      "Epoch: [ 0] [  62/ 316] time: 22.5741, d_loss: 0.654259, g_loss: 4.670717\n",
      "Epoch: [ 0] [  64/ 316] time: 23.2354, d_loss: 0.377065, g_loss: 5.569461\n",
      "Epoch: [ 0] [  66/ 316] time: 23.8981, d_loss: 0.367635, g_loss: 5.351228\n",
      "Epoch: [ 0] [  68/ 316] time: 24.5587, d_loss: 0.192787, g_loss: 5.907536\n",
      "Epoch: [ 0] [  70/ 316] time: 25.2202, d_loss: 0.633759, g_loss: 5.088345\n",
      "Epoch: [ 0] [  72/ 316] time: 25.8788, d_loss: 0.299660, g_loss: 6.031083\n",
      "Epoch: [ 0] [  74/ 316] time: 26.5396, d_loss: 0.267999, g_loss: 6.293489\n",
      "Epoch: [ 0] [  76/ 316] time: 27.2011, d_loss: 0.789969, g_loss: 6.227852\n",
      "Epoch: [ 0] [  78/ 316] time: 27.8594, d_loss: 0.658932, g_loss: 5.943700\n",
      "Epoch: [ 0] [  80/ 316] time: 28.5163, d_loss: 1.272681, g_loss: 4.181105\n",
      "Epoch: [ 0] [  82/ 316] time: 29.1732, d_loss: 0.822640, g_loss: 4.982302\n",
      "Epoch: [ 0] [  84/ 316] time: 29.8295, d_loss: 0.999055, g_loss: 2.908053\n",
      "Epoch: [ 0] [  86/ 316] time: 30.4861, d_loss: 1.662933, g_loss: 2.948544\n",
      "Epoch: [ 0] [  88/ 316] time: 31.1465, d_loss: 1.106632, g_loss: 3.429523\n",
      "Epoch: [ 0] [  90/ 316] time: 31.8066, d_loss: 1.214777, g_loss: 2.896347\n",
      "Epoch: [ 0] [  92/ 316] time: 32.4671, d_loss: 1.009145, g_loss: 2.841277\n",
      "Epoch: [ 0] [  94/ 316] time: 33.1291, d_loss: 0.753277, g_loss: 3.613656\n",
      "Epoch: [ 0] [  96/ 316] time: 33.7914, d_loss: 0.464528, g_loss: 4.475019\n",
      "Epoch: [ 0] [  98/ 316] time: 34.4563, d_loss: 0.488648, g_loss: 4.667844\n",
      "Epoch: [ 0] [ 100/ 316] time: 35.1187, d_loss: 0.435314, g_loss: 4.891987\n",
      "Epoch: [ 0] [ 102/ 316] time: 35.7807, d_loss: 1.558565, g_loss: 3.329216\n",
      "Epoch: [ 0] [ 104/ 316] time: 36.4427, d_loss: 0.947135, g_loss: 4.449770\n",
      "Epoch: [ 0] [ 106/ 316] time: 37.1034, d_loss: 0.675711, g_loss: 3.972846\n",
      "Epoch: [ 0] [ 108/ 316] time: 37.7672, d_loss: 0.745668, g_loss: 3.518851\n",
      "Epoch: [ 0] [ 110/ 316] time: 38.4259, d_loss: 0.641773, g_loss: 3.400641\n",
      "Epoch: [ 0] [ 112/ 316] time: 39.0871, d_loss: 0.772925, g_loss: 3.389085\n",
      "Epoch: [ 0] [ 114/ 316] time: 39.7446, d_loss: 1.486912, g_loss: 2.697242\n",
      "Epoch: [ 0] [ 116/ 316] time: 40.4051, d_loss: 0.877998, g_loss: 3.264254\n",
      "Epoch: [ 0] [ 118/ 316] time: 41.0675, d_loss: 1.123427, g_loss: 2.637131\n",
      "Epoch: [ 0] [ 120/ 316] time: 41.7289, d_loss: 0.665356, g_loss: 3.288970\n",
      "Epoch: [ 0] [ 122/ 316] time: 42.3902, d_loss: 1.084116, g_loss: 2.977491\n",
      "Epoch: [ 0] [ 124/ 316] time: 43.0501, d_loss: 0.944561, g_loss: 2.492662\n",
      "Epoch: [ 0] [ 126/ 316] time: 43.7089, d_loss: 0.952276, g_loss: 2.995762\n",
      "Epoch: [ 0] [ 128/ 316] time: 44.3708, d_loss: 1.026716, g_loss: 2.845692\n",
      "Epoch: [ 0] [ 130/ 316] time: 45.0363, d_loss: 1.068744, g_loss: 2.728624\n",
      "Epoch: [ 0] [ 132/ 316] time: 45.7012, d_loss: 0.844896, g_loss: 2.962061\n",
      "Epoch: [ 0] [ 134/ 316] time: 46.3682, d_loss: 1.562299, g_loss: 3.370492\n",
      "Epoch: [ 0] [ 136/ 316] time: 47.0372, d_loss: 0.539025, g_loss: 3.959718\n",
      "Epoch: [ 0] [ 138/ 316] time: 47.7052, d_loss: 0.799654, g_loss: 3.606758\n",
      "Epoch: [ 0] [ 140/ 316] time: 48.3714, d_loss: 0.578406, g_loss: 4.018489\n",
      "Epoch: [ 0] [ 142/ 316] time: 49.0361, d_loss: 0.823782, g_loss: 4.072044\n",
      "Epoch: [ 0] [ 144/ 316] time: 49.6985, d_loss: 0.515269, g_loss: 4.217708\n",
      "Epoch: [ 0] [ 146/ 316] time: 50.3627, d_loss: 0.469102, g_loss: 4.364810\n",
      "Epoch: [ 0] [ 148/ 316] time: 51.0250, d_loss: 1.480726, g_loss: 2.962184\n",
      "Epoch: [ 0] [ 150/ 316] time: 51.6897, d_loss: 2.456929, g_loss: 2.087752\n",
      "Epoch: [ 0] [ 152/ 316] time: 52.3522, d_loss: 1.001453, g_loss: 3.187548\n",
      "Epoch: [ 0] [ 154/ 316] time: 53.0165, d_loss: 0.892036, g_loss: 3.421118\n",
      "Epoch: [ 0] [ 156/ 316] time: 53.6819, d_loss: 0.465176, g_loss: 4.311084\n",
      "Epoch: [ 0] [ 158/ 316] time: 54.3451, d_loss: 1.297918, g_loss: 3.718274\n",
      "Epoch: [ 0] [ 160/ 316] time: 55.0043, d_loss: 118.520767, g_loss: 56.070595\n",
      "Epoch: [ 0] [ 162/ 316] time: 55.6672, d_loss: 0.553630, g_loss: 4.017348\n",
      "Epoch: [ 0] [ 164/ 316] time: 56.3293, d_loss: 0.771350, g_loss: 3.886672\n",
      "Epoch: [ 0] [ 166/ 316] time: 56.9927, d_loss: 0.644587, g_loss: 3.745233\n",
      "Epoch: [ 0] [ 168/ 316] time: 57.6539, d_loss: 0.528144, g_loss: 4.143342\n",
      "Epoch: [ 0] [ 170/ 316] time: 58.3159, d_loss: 1.660054, g_loss: 4.218877\n",
      "Epoch: [ 0] [ 172/ 316] time: 58.9778, d_loss: 0.942405, g_loss: 3.148465\n",
      "Epoch: [ 0] [ 174/ 316] time: 59.6408, d_loss: 0.651689, g_loss: 3.802562\n",
      "Epoch: [ 0] [ 176/ 316] time: 60.3037, d_loss: 1.032444, g_loss: 4.290707\n",
      "Epoch: [ 0] [ 178/ 316] time: 60.9674, d_loss: 1.069730, g_loss: 3.037708\n",
      "Epoch: [ 0] [ 180/ 316] time: 61.6309, d_loss: 0.446299, g_loss: 4.354654\n",
      "Epoch: [ 0] [ 182/ 316] time: 62.2975, d_loss: 0.637221, g_loss: 4.019545\n",
      "Epoch: [ 0] [ 184/ 316] time: 62.9656, d_loss: 0.567111, g_loss: 4.645507\n",
      "Epoch: [ 0] [ 186/ 316] time: 63.6308, d_loss: 0.342634, g_loss: 5.129745\n",
      "Epoch: [ 0] [ 188/ 316] time: 64.2966, d_loss: 1.520269, g_loss: 3.913053\n",
      "Epoch: [ 0] [ 190/ 316] time: 64.9655, d_loss: 0.882880, g_loss: 3.002722\n",
      "Epoch: [ 0] [ 192/ 316] time: 65.6362, d_loss: 0.738676, g_loss: 3.462683\n",
      "Epoch: [ 0] [ 194/ 316] time: 66.3017, d_loss: 0.690554, g_loss: 3.358125\n",
      "Epoch: [ 0] [ 196/ 316] time: 66.9696, d_loss: 0.681833, g_loss: 3.680412\n",
      "Epoch: [ 0] [ 198/ 316] time: 67.6337, d_loss: 0.535573, g_loss: 3.634972\n",
      "Epoch: [ 0] [ 200/ 316] time: 68.2991, d_loss: 2.051273, g_loss: 3.691411\n",
      "Epoch: [ 0] [ 202/ 316] time: 68.9668, d_loss: 1.210035, g_loss: 3.318047\n",
      "Epoch: [ 0] [ 204/ 316] time: 69.6320, d_loss: 0.695516, g_loss: 3.203196\n",
      "Epoch: [ 0] [ 206/ 316] time: 70.2981, d_loss: 0.992235, g_loss: 2.876734\n",
      "Epoch: [ 0] [ 208/ 316] time: 70.9633, d_loss: 1.458146, g_loss: 3.349542\n",
      "Epoch: [ 0] [ 210/ 316] time: 71.6330, d_loss: 1.158285, g_loss: 2.255112\n",
      "Epoch: [ 0] [ 212/ 316] time: 72.3009, d_loss: 0.823166, g_loss: 2.749361\n",
      "Epoch: [ 0] [ 214/ 316] time: 72.9688, d_loss: 0.811378, g_loss: 2.789095\n",
      "Epoch: [ 0] [ 216/ 316] time: 73.6365, d_loss: 1.178297, g_loss: 3.272285\n",
      "Epoch: [ 0] [ 218/ 316] time: 74.3032, d_loss: 0.593328, g_loss: 3.271812\n",
      "Epoch: [ 0] [ 220/ 316] time: 74.9703, d_loss: 0.629377, g_loss: 3.445684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0] [ 222/ 316] time: 75.6343, d_loss: 1.464056, g_loss: 2.981787\n",
      "Epoch: [ 0] [ 224/ 316] time: 76.2996, d_loss: 0.567291, g_loss: 3.826739\n",
      "Epoch: [ 0] [ 226/ 316] time: 76.9644, d_loss: 0.981439, g_loss: 2.910825\n",
      "Epoch: [ 0] [ 228/ 316] time: 77.6287, d_loss: 0.697645, g_loss: 3.927514\n",
      "Epoch: [ 0] [ 230/ 316] time: 78.2927, d_loss: 0.869495, g_loss: 2.872766\n",
      "Epoch: [ 0] [ 232/ 316] time: 78.9567, d_loss: 0.887950, g_loss: 3.397305\n",
      "Epoch: [ 0] [ 234/ 316] time: 79.6214, d_loss: 0.486024, g_loss: 3.700614\n",
      "Epoch: [ 0] [ 236/ 316] time: 80.2872, d_loss: 0.521652, g_loss: 3.884615\n",
      "Epoch: [ 0] [ 238/ 316] time: 80.9558, d_loss: 1.720816, g_loss: 4.236128\n",
      "Epoch: [ 0] [ 240/ 316] time: 81.6228, d_loss: 0.446671, g_loss: 4.202503\n",
      "Epoch: [ 0] [ 242/ 316] time: 82.2880, d_loss: 0.975312, g_loss: 3.196357\n",
      "Epoch: [ 0] [ 244/ 316] time: 82.9521, d_loss: 0.528157, g_loss: 4.325714\n",
      "Epoch: [ 0] [ 246/ 316] time: 83.6182, d_loss: 0.681957, g_loss: 3.437139\n",
      "Epoch: [ 0] [ 248/ 316] time: 84.2829, d_loss: 0.833667, g_loss: 3.071718\n",
      "Epoch: [ 0] [ 250/ 316] time: 84.9498, d_loss: 0.679585, g_loss: 3.978551\n",
      "Epoch: [ 0] [ 252/ 316] time: 85.6172, d_loss: 0.653457, g_loss: 4.789295\n",
      "Epoch: [ 0] [ 254/ 316] time: 86.2798, d_loss: 1.051898, g_loss: 3.772365\n",
      "Epoch: [ 0] [ 256/ 316] time: 86.9430, d_loss: 0.644046, g_loss: 3.852401\n",
      "Epoch: [ 0] [ 258/ 316] time: 87.6087, d_loss: 0.403291, g_loss: 5.342601\n",
      "Epoch: [ 0] [ 260/ 316] time: 88.2768, d_loss: 0.387682, g_loss: 5.140582\n",
      "Epoch: [ 0] [ 262/ 316] time: 88.9477, d_loss: 0.713154, g_loss: 5.528619\n",
      "Epoch: [ 0] [ 264/ 316] time: 89.6146, d_loss: 1.226265, g_loss: 3.939489\n",
      "Epoch: [ 0] [ 266/ 316] time: 90.2838, d_loss: 0.540275, g_loss: 4.284062\n",
      "Epoch: [ 0] [ 268/ 316] time: 90.9584, d_loss: 1.017087, g_loss: 3.379274\n",
      "Epoch: [ 0] [ 270/ 316] time: 91.6268, d_loss: 0.500379, g_loss: 4.295979\n",
      "Epoch: [ 0] [ 272/ 316] time: 92.2989, d_loss: 1.035381, g_loss: 3.509607\n",
      "Epoch: [ 0] [ 274/ 316] time: 92.9682, d_loss: 0.528052, g_loss: 4.159241\n",
      "Epoch: [ 0] [ 276/ 316] time: 93.6365, d_loss: 0.864438, g_loss: 4.386686\n",
      "Epoch: [ 0] [ 278/ 316] time: 94.3089, d_loss: 0.289043, g_loss: 5.845208\n",
      "Epoch: [ 0] [ 280/ 316] time: 94.9781, d_loss: 0.197658, g_loss: 6.134199\n",
      "Epoch: [ 0] [ 282/ 316] time: 95.6459, d_loss: 0.822261, g_loss: 4.818177\n",
      "Epoch: [ 0] [ 284/ 316] time: 96.3151, d_loss: 1.520545, g_loss: 3.438915\n",
      "Epoch: [ 0] [ 286/ 316] time: 96.9827, d_loss: 0.349448, g_loss: 4.906249\n",
      "Epoch: [ 0] [ 288/ 316] time: 97.6535, d_loss: 0.324040, g_loss: 4.796196\n",
      "Epoch: [ 0] [ 290/ 316] time: 98.3245, d_loss: 0.543815, g_loss: 3.941165\n",
      "Epoch: [ 0] [ 292/ 316] time: 98.9941, d_loss: 1.095533, g_loss: 4.191874\n",
      "Epoch: [ 0] [ 294/ 316] time: 99.6677, d_loss: 0.970793, g_loss: 4.172434\n",
      "Epoch: [ 0] [ 296/ 316] time: 100.3383, d_loss: 0.582426, g_loss: 4.171178\n",
      "Epoch: [ 0] [ 298/ 316] time: 101.0093, d_loss: 0.420478, g_loss: 4.642768\n",
      "Epoch: [ 0] [ 300/ 316] time: 101.6792, d_loss: 0.642167, g_loss: 3.979036\n",
      "Epoch: [ 0] [ 302/ 316] time: 102.3472, d_loss: 0.197027, g_loss: 5.921212\n",
      "Epoch: [ 0] [ 304/ 316] time: 103.0201, d_loss: 1.014115, g_loss: 4.340620\n",
      "Epoch: [ 0] [ 306/ 316] time: 103.6895, d_loss: 0.323245, g_loss: 5.423727\n",
      "Epoch: [ 0] [ 308/ 316] time: 104.3609, d_loss: 0.332131, g_loss: 5.804014\n",
      "Epoch: [ 0] [ 310/ 316] time: 105.0303, d_loss: 0.690916, g_loss: 3.587044\n",
      "Epoch: [ 0] [ 312/ 316] time: 105.6982, d_loss: 0.407838, g_loss: 4.304212\n",
      "Epoch: [ 0] [ 314/ 316] time: 106.3695, d_loss: 0.367320, g_loss: 4.838182\n",
      "Epoch: [ 1] [   0/ 316] time: 107.0421, d_loss: 0.878726, g_loss: 4.651361\n",
      "Epoch: [ 1] [   2/ 316] time: 107.7117, d_loss: 1.027294, g_loss: 4.191993\n",
      "Epoch: [ 1] [   4/ 316] time: 108.3827, d_loss: 0.393658, g_loss: 5.088764\n",
      "Epoch: [ 1] [   6/ 316] time: 109.0509, d_loss: 0.408316, g_loss: 4.944762\n",
      "Epoch: [ 1] [   8/ 316] time: 109.7236, d_loss: 0.373157, g_loss: 4.907925\n",
      "Epoch: [ 1] [  10/ 316] time: 110.3939, d_loss: 2.710100, g_loss: 2.836079\n",
      "Epoch: [ 1] [  12/ 316] time: 111.0678, d_loss: 0.499035, g_loss: 4.519101\n",
      "Epoch: [ 1] [  14/ 316] time: 111.7396, d_loss: 0.564993, g_loss: 4.348486\n",
      "Epoch: [ 1] [  16/ 316] time: 112.4103, d_loss: 1.030557, g_loss: 3.707235\n",
      "Epoch: [ 1] [  18/ 316] time: 113.0777, d_loss: 1.068418, g_loss: 3.559159\n",
      "Epoch: [ 1] [  20/ 316] time: 113.7486, d_loss: 0.717061, g_loss: 3.677896\n",
      "Epoch: [ 1] [  22/ 316] time: 114.4158, d_loss: 0.401524, g_loss: 4.732248\n",
      "Epoch: [ 1] [  24/ 316] time: 115.0885, d_loss: 0.510347, g_loss: 3.793649\n",
      "Epoch: [ 1] [  26/ 316] time: 115.7594, d_loss: 0.330895, g_loss: 4.745903\n",
      "Epoch: [ 1] [  28/ 316] time: 116.4287, d_loss: 1.930371, g_loss: 3.574534\n",
      "Epoch: [ 1] [  30/ 316] time: 117.0983, d_loss: 0.493827, g_loss: 4.564062\n",
      "Epoch: [ 1] [  32/ 316] time: 117.7661, d_loss: 0.114008, g_loss: 7.918379\n",
      "Epoch: [ 1] [  34/ 316] time: 118.4376, d_loss: 0.093522, g_loss: 7.922920\n",
      "Epoch: [ 1] [  36/ 316] time: 119.1086, d_loss: 0.295302, g_loss: 6.301650\n",
      "Epoch: [ 1] [  38/ 316] time: 119.7815, d_loss: 0.218686, g_loss: 7.214434\n",
      "Epoch: [ 1] [  40/ 316] time: 120.4530, d_loss: 0.158694, g_loss: 6.563399\n",
      "Epoch: [ 1] [  42/ 316] time: 121.1200, d_loss: 0.140038, g_loss: 7.170593\n",
      "Epoch: [ 1] [  44/ 316] time: 121.7904, d_loss: 0.319060, g_loss: 7.101807\n",
      "Epoch: [ 1] [  46/ 316] time: 122.4611, d_loss: 0.302336, g_loss: 5.983898\n",
      "Epoch: [ 1] [  48/ 316] time: 123.1289, d_loss: 0.168980, g_loss: 7.107612\n",
      "Epoch: [ 1] [  50/ 316] time: 123.8011, d_loss: 0.187898, g_loss: 7.008003\n",
      "Epoch: [ 1] [  52/ 316] time: 124.4711, d_loss: 0.231644, g_loss: 7.245031\n",
      "Epoch: [ 1] [  54/ 316] time: 125.1425, d_loss: 0.162987, g_loss: 7.579165\n",
      "Epoch: [ 1] [  56/ 316] time: 125.8121, d_loss: 0.061173, g_loss: 8.060112\n",
      "Epoch: [ 1] [  58/ 316] time: 126.4804, d_loss: 0.092970, g_loss: 7.947453\n",
      "Epoch: [ 1] [  60/ 316] time: 127.1525, d_loss: 0.209755, g_loss: 6.767776\n",
      "Epoch: [ 1] [  62/ 316] time: 127.8220, d_loss: 0.360350, g_loss: 6.689730\n",
      "Epoch: [ 1] [  64/ 316] time: 128.4935, d_loss: 0.573173, g_loss: 7.479533\n",
      "Epoch: [ 1] [  66/ 316] time: 129.1640, d_loss: 1.028259, g_loss: 5.043675\n",
      "Epoch: [ 1] [  68/ 316] time: 129.8316, d_loss: 0.631735, g_loss: 6.510894\n",
      "Epoch: [ 1] [  70/ 316] time: 130.5002, d_loss: 0.250133, g_loss: 6.051361\n",
      "Epoch: [ 1] [  72/ 316] time: 131.1714, d_loss: 0.389872, g_loss: 6.679141\n",
      "Epoch: [ 1] [  74/ 316] time: 131.8425, d_loss: 0.410511, g_loss: 6.867124\n",
      "Epoch: [ 1] [  76/ 316] time: 132.5132, d_loss: 0.204487, g_loss: 7.310920\n",
      "Epoch: [ 1] [  78/ 316] time: 133.1854, d_loss: 0.456331, g_loss: 5.766067\n",
      "Epoch: [ 1] [  80/ 316] time: 133.8551, d_loss: 1.617716, g_loss: 6.444354\n",
      "Epoch: [ 1] [  82/ 316] time: 134.5255, d_loss: 1.085005, g_loss: 3.759571\n",
      "Epoch: [ 1] [  84/ 316] time: 135.1928, d_loss: 0.591211, g_loss: 4.860890\n",
      "Epoch: [ 1] [  86/ 316] time: 135.8678, d_loss: 1.420273, g_loss: 3.445490\n",
      "Epoch: [ 1] [  88/ 316] time: 136.5383, d_loss: 0.581067, g_loss: 4.794278\n",
      "Epoch: [ 1] [  90/ 316] time: 137.2079, d_loss: 0.854063, g_loss: 3.820163\n",
      "Epoch: [ 1] [  92/ 316] time: 137.8758, d_loss: 0.364348, g_loss: 6.462819\n",
      "Epoch: [ 1] [  94/ 316] time: 138.5427, d_loss: 0.640729, g_loss: 4.325770\n",
      "Epoch: [ 1] [  96/ 316] time: 139.2138, d_loss: 0.881388, g_loss: 4.621922\n",
      "Epoch: [ 1] [  98/ 316] time: 139.8844, d_loss: 0.371178, g_loss: 5.330503\n",
      "Epoch: [ 1] [ 100/ 316] time: 140.5513, d_loss: 0.673026, g_loss: 4.639152\n",
      "Epoch: [ 1] [ 102/ 316] time: 141.2276, d_loss: 1.424128, g_loss: 4.536662\n",
      "Epoch: [ 1] [ 104/ 316] time: 141.8996, d_loss: 0.519791, g_loss: 4.491974\n",
      "Epoch: [ 1] [ 106/ 316] time: 142.5690, d_loss: 0.815064, g_loss: 3.662095\n",
      "Epoch: [ 1] [ 108/ 316] time: 143.2401, d_loss: 0.538940, g_loss: 4.659954\n",
      "Epoch: [ 1] [ 110/ 316] time: 143.9071, d_loss: 0.726108, g_loss: 4.304650\n",
      "Epoch: [ 1] [ 112/ 316] time: 144.5757, d_loss: 1.292264, g_loss: 3.403656\n",
      "Epoch: [ 1] [ 114/ 316] time: 145.2453, d_loss: 0.515051, g_loss: 4.651712\n",
      "Epoch: [ 1] [ 116/ 316] time: 145.9227, d_loss: 1.086236, g_loss: 5.249322\n",
      "Epoch: [ 1] [ 118/ 316] time: 146.5921, d_loss: 0.609845, g_loss: 5.299390\n",
      "Epoch: [ 1] [ 120/ 316] time: 147.2634, d_loss: 0.500225, g_loss: 5.250144\n",
      "Epoch: [ 1] [ 122/ 316] time: 147.9400, d_loss: 0.611728, g_loss: 4.699577\n",
      "Epoch: [ 1] [ 124/ 316] time: 148.6234, d_loss: 0.344411, g_loss: 6.339299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1] [ 126/ 316] time: 149.3035, d_loss: 0.216928, g_loss: 6.122698\n",
      "Epoch: [ 1] [ 128/ 316] time: 149.9857, d_loss: 0.464081, g_loss: 6.019615\n",
      "Epoch: [ 1] [ 130/ 316] time: 150.6678, d_loss: 0.470597, g_loss: 5.351743\n",
      "Epoch: [ 1] [ 132/ 316] time: 151.3510, d_loss: 2.530237, g_loss: 3.989373\n",
      "Epoch: [ 1] [ 134/ 316] time: 152.0324, d_loss: 0.591420, g_loss: 8.089653\n",
      "Epoch: [ 1] [ 136/ 316] time: 152.7151, d_loss: 0.189740, g_loss: 8.211820\n",
      "Epoch: [ 1] [ 138/ 316] time: 153.4016, d_loss: 0.186318, g_loss: 7.651402\n",
      "Epoch: [ 1] [ 140/ 316] time: 154.0888, d_loss: 0.482397, g_loss: 6.021285\n",
      "Epoch: [ 1] [ 142/ 316] time: 154.7747, d_loss: 0.158133, g_loss: 7.013131\n",
      "Epoch: [ 1] [ 144/ 316] time: 155.4591, d_loss: 0.259846, g_loss: 5.722879\n",
      "Epoch: [ 1] [ 146/ 316] time: 156.1417, d_loss: 0.759027, g_loss: 4.404072\n",
      "Epoch: [ 1] [ 148/ 316] time: 156.8231, d_loss: 2.080277, g_loss: 3.481394\n",
      "Epoch: [ 1] [ 150/ 316] time: 157.5017, d_loss: 0.447896, g_loss: 4.646549\n",
      "Epoch: [ 1] [ 152/ 316] time: 158.1829, d_loss: 0.379773, g_loss: 4.633998\n",
      "Epoch: [ 1] [ 154/ 316] time: 158.8636, d_loss: 3.240853, g_loss: 4.778299\n",
      "Epoch: [ 1] [ 156/ 316] time: 159.5451, d_loss: 0.685672, g_loss: 4.873713\n",
      "Epoch: [ 1] [ 158/ 316] time: 160.2256, d_loss: 0.766672, g_loss: 4.060032\n",
      "Epoch: [ 1] [ 160/ 316] time: 160.9039, d_loss: 0.850692, g_loss: 3.048655\n",
      "Epoch: [ 1] [ 162/ 316] time: 161.5777, d_loss: 0.686068, g_loss: 3.487668\n",
      "Epoch: [ 1] [ 164/ 316] time: 162.2507, d_loss: 1.232018, g_loss: 3.924500\n",
      "Epoch: [ 1] [ 166/ 316] time: 162.9232, d_loss: 1.203891, g_loss: 3.272475\n",
      "Epoch: [ 1] [ 168/ 316] time: 163.5992, d_loss: 0.744920, g_loss: 3.293712\n",
      "Epoch: [ 1] [ 170/ 316] time: 164.2763, d_loss: 0.924014, g_loss: 2.759579\n",
      "Epoch: [ 1] [ 172/ 316] time: 164.9523, d_loss: 0.644424, g_loss: 3.562160\n",
      "Epoch: [ 1] [ 174/ 316] time: 165.6316, d_loss: 1.122145, g_loss: 4.250585\n",
      "Epoch: [ 1] [ 176/ 316] time: 166.3134, d_loss: 0.765099, g_loss: 3.426002\n",
      "Epoch: [ 1] [ 178/ 316] time: 166.9956, d_loss: 1.127690, g_loss: 3.003094\n",
      "Epoch: [ 1] [ 180/ 316] time: 167.6765, d_loss: 0.738724, g_loss: 3.612636\n",
      "Epoch: [ 1] [ 182/ 316] time: 168.3633, d_loss: 2.046693, g_loss: 3.719058\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [ 184/ 316] time: 169.5191, d_loss: 0.455231, g_loss: 4.541286\n",
      "Epoch: [ 1] [ 186/ 316] time: 170.1947, d_loss: 0.850737, g_loss: 3.178962\n",
      "Epoch: [ 1] [ 188/ 316] time: 170.8664, d_loss: 2.539793, g_loss: 3.025468\n",
      "Epoch: [ 1] [ 190/ 316] time: 171.5420, d_loss: 0.876568, g_loss: 3.831983\n",
      "Epoch: [ 1] [ 192/ 316] time: 172.2135, d_loss: 1.102561, g_loss: 3.432449\n",
      "Epoch: [ 1] [ 194/ 316] time: 172.8930, d_loss: 0.354682, g_loss: 4.397088\n",
      "Epoch: [ 1] [ 196/ 316] time: 173.5685, d_loss: 0.830272, g_loss: 3.671964\n",
      "Epoch: [ 1] [ 198/ 316] time: 174.2398, d_loss: 0.424902, g_loss: 4.247740\n",
      "Epoch: [ 1] [ 200/ 316] time: 174.9112, d_loss: 0.490224, g_loss: 3.897952\n",
      "Epoch: [ 1] [ 202/ 316] time: 175.6098, d_loss: 0.523645, g_loss: 4.028581\n",
      "Epoch: [ 1] [ 204/ 316] time: 176.3076, d_loss: 0.493548, g_loss: 4.686394\n",
      "Epoch: [ 1] [ 206/ 316] time: 177.0078, d_loss: 2.291159, g_loss: 2.633514\n",
      "Epoch: [ 1] [ 208/ 316] time: 177.7082, d_loss: 0.279967, g_loss: 5.238589\n",
      "Epoch: [ 1] [ 210/ 316] time: 178.4071, d_loss: 0.786394, g_loss: 3.677235\n",
      "Epoch: [ 1] [ 212/ 316] time: 179.1082, d_loss: 1.029682, g_loss: 2.709001\n",
      "Epoch: [ 1] [ 214/ 316] time: 179.8079, d_loss: 0.784381, g_loss: 3.476627\n",
      "Epoch: [ 1] [ 216/ 316] time: 180.5104, d_loss: 0.662292, g_loss: 3.883498\n",
      "Epoch: [ 1] [ 218/ 316] time: 181.2082, d_loss: 0.305603, g_loss: 4.887509\n",
      "Epoch: [ 1] [ 220/ 316] time: 181.9057, d_loss: 0.513896, g_loss: 4.880886\n",
      "Epoch: [ 1] [ 222/ 316] time: 182.5912, d_loss: 0.239797, g_loss: 5.539442\n",
      "Epoch: [ 1] [ 224/ 316] time: 183.2724, d_loss: 0.440222, g_loss: 4.343886\n",
      "Epoch: [ 1] [ 226/ 316] time: 183.9512, d_loss: 1.318375, g_loss: 2.303959\n",
      "Epoch: [ 1] [ 228/ 316] time: 184.6290, d_loss: 0.479954, g_loss: 4.760004\n",
      "Epoch: [ 1] [ 230/ 316] time: 185.3035, d_loss: 2.701105, g_loss: 3.013792\n",
      "Epoch: [ 1] [ 232/ 316] time: 185.9764, d_loss: 2.061888, g_loss: 3.005167\n",
      "Epoch: [ 1] [ 234/ 316] time: 186.6508, d_loss: 3.528723, g_loss: 1.851133\n",
      "Epoch: [ 1] [ 236/ 316] time: 187.3230, d_loss: 0.776582, g_loss: 3.839807\n",
      "Epoch: [ 1] [ 238/ 316] time: 187.9984, d_loss: 0.540521, g_loss: 4.411420\n",
      "Epoch: [ 1] [ 240/ 316] time: 188.6719, d_loss: 0.399827, g_loss: 5.075370\n",
      "Epoch: [ 1] [ 242/ 316] time: 189.3462, d_loss: 0.316424, g_loss: 5.392735\n",
      "Epoch: [ 1] [ 244/ 316] time: 190.0297, d_loss: 0.549188, g_loss: 4.204632\n",
      "Epoch: [ 1] [ 246/ 316] time: 190.7311, d_loss: 0.834652, g_loss: 3.626208\n",
      "Epoch: [ 1] [ 248/ 316] time: 191.4318, d_loss: 1.330810, g_loss: 2.303108\n",
      "Epoch: [ 1] [ 250/ 316] time: 192.1309, d_loss: 0.595741, g_loss: 4.310968\n",
      "Epoch: [ 1] [ 252/ 316] time: 192.8307, d_loss: 0.512230, g_loss: 4.270607\n",
      "Epoch: [ 1] [ 254/ 316] time: 193.5299, d_loss: 0.555223, g_loss: 3.873423\n",
      "Epoch: [ 1] [ 256/ 316] time: 194.2200, d_loss: 1.932673, g_loss: 3.431475\n",
      "Epoch: [ 1] [ 258/ 316] time: 194.9091, d_loss: 0.612616, g_loss: 4.130665\n",
      "Epoch: [ 1] [ 260/ 316] time: 195.5922, d_loss: 1.215618, g_loss: 4.312148\n",
      "Epoch: [ 1] [ 262/ 316] time: 196.2736, d_loss: 0.667771, g_loss: 3.592244\n",
      "Epoch: [ 1] [ 264/ 316] time: 196.9573, d_loss: 0.496645, g_loss: 3.893009\n",
      "Epoch: [ 1] [ 266/ 316] time: 197.6430, d_loss: 0.774376, g_loss: 3.584347\n",
      "Epoch: [ 1] [ 268/ 316] time: 198.3259, d_loss: 0.662689, g_loss: 4.509965\n",
      "Epoch: [ 1] [ 270/ 316] time: 199.0112, d_loss: 0.298230, g_loss: 4.990978\n",
      "Epoch: [ 1] [ 272/ 316] time: 199.6968, d_loss: 0.460938, g_loss: 4.707410\n",
      "Epoch: [ 1] [ 274/ 316] time: 200.3798, d_loss: 0.540731, g_loss: 5.072810\n",
      "Epoch: [ 1] [ 276/ 316] time: 201.0616, d_loss: 0.752304, g_loss: 4.386547\n",
      "Epoch: [ 1] [ 278/ 316] time: 201.7433, d_loss: 0.409591, g_loss: 4.578342\n",
      "Epoch: [ 1] [ 280/ 316] time: 202.4215, d_loss: 0.749486, g_loss: 5.185668\n",
      "Epoch: [ 1] [ 282/ 316] time: 203.1009, d_loss: 0.457894, g_loss: 4.366400\n",
      "Epoch: [ 1] [ 284/ 316] time: 203.7825, d_loss: 0.481910, g_loss: 5.276867\n",
      "Epoch: [ 1] [ 286/ 316] time: 204.4633, d_loss: 1.578023, g_loss: 4.077332\n",
      "Epoch: [ 1] [ 288/ 316] time: 205.1399, d_loss: 0.365620, g_loss: 5.872643\n",
      "Epoch: [ 1] [ 290/ 316] time: 205.8258, d_loss: 0.368844, g_loss: 4.431997\n",
      "Epoch: [ 1] [ 292/ 316] time: 206.5198, d_loss: 1.169950, g_loss: 2.749292\n",
      "Epoch: [ 1] [ 294/ 316] time: 207.2153, d_loss: 0.779391, g_loss: 3.316073\n",
      "Epoch: [ 1] [ 296/ 316] time: 207.9135, d_loss: 0.862801, g_loss: 3.769277\n",
      "Epoch: [ 1] [ 298/ 316] time: 208.6111, d_loss: 0.422798, g_loss: 4.688182\n",
      "Epoch: [ 1] [ 300/ 316] time: 209.3039, d_loss: 0.573142, g_loss: 4.928662\n",
      "Epoch: [ 1] [ 302/ 316] time: 209.9984, d_loss: 2.848317, g_loss: 3.887092\n",
      "Epoch: [ 1] [ 304/ 316] time: 210.6908, d_loss: 0.569014, g_loss: 4.557036\n",
      "Epoch: [ 1] [ 306/ 316] time: 211.3854, d_loss: 0.737148, g_loss: 3.904586\n",
      "Epoch: [ 1] [ 308/ 316] time: 212.0823, d_loss: 0.585679, g_loss: 3.691245\n",
      "Epoch: [ 1] [ 310/ 316] time: 212.7777, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 1] [ 312/ 316] time: 213.4388, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 1] [ 314/ 316] time: 214.0914, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [   0/ 316] time: 214.7470, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [   2/ 316] time: 215.3996, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [   4/ 316] time: 216.0507, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [   6/ 316] time: 216.7012, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [   8/ 316] time: 217.3543, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  10/ 316] time: 218.0047, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  12/ 316] time: 218.6595, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  14/ 316] time: 219.3158, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  16/ 316] time: 219.9733, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  18/ 316] time: 220.6341, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  20/ 316] time: 221.2944, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  22/ 316] time: 221.9530, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  24/ 316] time: 222.6100, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  26/ 316] time: 223.2707, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  28/ 316] time: 223.9308, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  30/ 316] time: 224.5873, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  32/ 316] time: 225.2456, d_loss: nan, g_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 2] [  34/ 316] time: 225.9044, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  36/ 316] time: 226.5629, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  38/ 316] time: 227.2200, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  40/ 316] time: 227.8775, d_loss: nan, g_loss: nan\n",
      "Epoch: [ 2] [  42/ 316] time: 228.5341, d_loss: nan, g_loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e9a4a8a3e9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# TRAIN / TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e5f46bdfd6d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_flag)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;31m# run tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"ConditionalALI_Face_2\"\n",
    "train_flag = True\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = ALI(sess, project_name)\n",
    "\n",
    "        # TRAIN / TEST\n",
    "        if train_flag:\n",
    "            model.train(train_flag)\n",
    "        else:\n",
    "            model.test(train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
