{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StarGAN implementation v5 (Final version)\n",
    "\n",
    "Based on v3, 128x128 data\n",
    "\n",
    "Parameter trying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1711.09020\n",
    "https://github.com/goldkim92/StarGAN-tensorflow/blob/master/\n",
    "https://github.com/ly-atdawn/StarGAN-Tensorflow\n",
    "https://github.com/igul222/improved_wgan_training/blob/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "import numpy as np\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path, flip=False): # [0,255] to [-1,1]\n",
    "    img = scm.imread(img_path) \n",
    "    if flip:\n",
    "        img = np.fliplr(img)\n",
    "    img = img * 2. /255. - 1.\n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def get_label(path, size):\n",
    "    label = int(path[-5])\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[ label ] = 1.0\n",
    "    one_hot[ one_hot==0 ] = 0.0\n",
    "    return one_hot\n",
    "\n",
    "def inverse_image(img): # [-1,1] to [0,255]\n",
    "    img = (img + 1.) / 2. * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n",
    "\n",
    "def pair_expressions(paths):\n",
    "    subject_exprs = []\n",
    "    subject_pairs = []\n",
    "    all_pairs = []\n",
    "    last_subject = 0\n",
    "\n",
    "    # Pair all expression of a subject\n",
    "    for path in paths:\n",
    "        subject = int(path[-10:-6])\n",
    "\n",
    "        if subject != last_subject and last_subject != 0:\n",
    "            subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "            all_pairs.extend(subject_pairs)\n",
    "            subject_exprs = []\n",
    "\n",
    "        subject_exprs.append(path)\n",
    "        last_subject = subject\n",
    "\n",
    "    # Last subject\n",
    "    subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "    all_pairs.extend(subject_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "def get_shape_c(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "def conv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(2.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                             use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def deconv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d_transpose\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d_transpose(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                                       use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(input_shape*output_shape)))\n",
    "    initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "def instance_norm(input, name='instance_norm'):\n",
    "    with tf.variable_scope(name):\n",
    "        depth = input.get_shape()[3]\n",
    "        scale = tf.get_variable('scale', [depth], initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n",
    "        offset = tf.get_variable('offset', [depth], initializer=tf.constant_initializer(0.0))\n",
    "        mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n",
    "        epsilon = 1e-5\n",
    "        inv = tf.rsqrt(variance + epsilon)\n",
    "        normalized = (input-mean)*inv\n",
    "        return scale*normalized + offset\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "def l2_loss(x, y):\n",
    "    return tf.reduce_mean(tf.square(x - y))\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))\n",
    "\n",
    "def lrelu(x, leak=0.01, name='lrelu'): #lrelu(x, leak=0.2, name='lrelu'):\n",
    "    return tf.maximum(x, leak*x)\n",
    "\n",
    "def gradient_penalty(real, fake, f):\n",
    "        def interpolate(a, b):\n",
    "            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n",
    "            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n",
    "            inter = a + alpha * (b - a)\n",
    "            inter.set_shape(a.get_shape().as_list())\n",
    "            return inter\n",
    "\n",
    "        x = interpolate(real, fake)\n",
    "        pred, _ = f(x, reuse=True)\n",
    "        gradients = tf.gradients(pred, x)[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=3))\n",
    "        gp = tf.reduce_mean((slopes - 1.)**2)\n",
    "        return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"./Face_data/Faces_with_expression_label/dataset_128x128\" #args.data_dir #./Data\n",
    "        self.dataset = \"expr\" #args.dataset  # celeba\n",
    "        self.data_size = 128 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "        self.data_label_vector_size = 7 #size of one-hot-encoded label vector\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 100 #50 #args.niter\n",
    "        self.niter_snapshot = 500 #args.nsnapshot\n",
    "        self.max_to_keep = 50 #args.max_to_keep models\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 0.5 #args.gamma\n",
    "        self.input_size = 128 #args.input_size\n",
    "        self.embedding = 128 #64 #args.embedding\n",
    "        \n",
    "        self.lambda_cls = 5.\n",
    "        self.lambda_recon = 10.\n",
    "        self.lambda_gp = 10.\n",
    "\n",
    "        \n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = 'assets_ae/{0}_{1}_{2}_{3}/'.format(self.project, self.dataset, self.data_opt, self.data_size)\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'models')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "        # etc.\n",
    "        if not os.path.exists('assets_ae'):\n",
    "            os.makedirs('assets_ae')\n",
    "        make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='x')\n",
    "        self.x_c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='x_c')\n",
    "        self.target = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='target')\n",
    "        self.target_c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='target_c')\n",
    "        self.alpha = tf.placeholder(tf.float32, shape=[None, 1], name='alpha')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "\n",
    "        # Generator\n",
    "        self.G_f = self.generator(self.x, self.target_c)\n",
    "        self.G_recon = self.generator(self.G_f, self.x_c, reuse=True)\n",
    "        \n",
    "        self.G_test = self.generator(self.x, self.target_c, reuse=True)\n",
    "        \n",
    "        # Discriminator\n",
    "        self.D_f, self.D_f_cls = self.discriminator(self.G_f)\n",
    "        self.D_target, self.D_target_cls = self.discriminator(self.target, reuse=True) # discriminate with the target\n",
    "        \n",
    "        # Gradient Penalty\n",
    "        self.real_data = tf.reshape(self.target, [-1, self.data_size*self.data_size*3]) # interpolate with target\n",
    "        self.fake_data = tf.reshape(self.G_f, [-1, self.data_size*self.data_size*3])\n",
    "        self.diff = self.fake_data - self.real_data\n",
    "        self.interpolate = self.real_data + self.alpha*self.diff\n",
    "        \n",
    "        self.inter_reshape = tf.reshape(self.interpolate, [-1, self.data_size, self.data_size, 3])\n",
    "        self.G_inter, _ = self.discriminator(self.inter_reshape, reuse=True)\n",
    "        \n",
    "        self.grad = tf.gradients(self.G_inter, \n",
    "                                 xs=[self.inter_reshape])[0]\n",
    "        self.slopes = tf.sqrt(tf.reduce_sum(tf.square(self.grad), axis=[1,2,3]))\n",
    "        self.gp = tf.reduce_mean(tf.square(self.slopes - 1.))\n",
    "        \n",
    "        \n",
    "        # Wasserstein loss\n",
    "        self.wd = tf.reduce_mean(self.D_target) - tf.reduce_mean(self.D_f)\n",
    "        self.L_adv_D = -self.wd + self.gp * self.lambda_gp\n",
    "        self.L_adv_G = -tf.reduce_mean(self.D_f)\n",
    "        \n",
    "        self.L_D_cls = tf.reduce_mean(cross_entropy(labels=self.target_c, logits=self.D_target_cls))# discriminate with the target\n",
    "        self.L_G_cls = tf.reduce_mean(cross_entropy(labels=self.target_c, logits=self.D_f_cls))\n",
    "        self.L_G_recon = l1_loss(self.x, self.G_recon)\n",
    "                \n",
    "        self.L_D = self.L_adv_D + self.lambda_cls * self.L_D_cls\n",
    "        self.L_G = self.L_adv_G + self.lambda_cls * self.L_G_cls + self.lambda_recon * self.L_G_recon\n",
    "        \n",
    "\n",
    "        # Variables\n",
    "        D_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator\")\n",
    "        G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"generator\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.opt_D = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.L_D, var_list=D_vars)\n",
    "        self.opt_G = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.L_G, var_list=G_vars)\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            self.saver.save(self.sess, self.ckpt_model_name, write_meta_graph=True)\n",
    "\n",
    "        # Summary\n",
    "        if self.flag:\n",
    "            tf.summary.scalar('loss/d_loss', self.L_D)\n",
    "            tf.summary.scalar('loss/g_loss', self.L_G)\n",
    "            tf.summary.scalar('loss/d_loss_cls', self.L_D_cls)\n",
    "            tf.summary.scalar('loss/g_loss_recon', self.L_G_recon)\n",
    "            tf.summary.scalar('loss/g_loss_cls', self.L_G_cls)\n",
    "            tf.summary.scalar('loss/wd', self.wd)\n",
    "\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(self.project_dir, self.sess.graph)\n",
    "\n",
    "    def train(self, train_flag):\n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "            \n",
    "        print('Shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Shuffle Done')\n",
    "\n",
    "        # initial parameter\n",
    "        start_time = time.time()\n",
    "        self.count = 0 # -------------------------------------------!!\n",
    "\n",
    "        for epoch in range(self.niter): #-------------------------------!!\n",
    "            batch_idxs = len(data) // self.batch_size\n",
    "            lr = np.float32(self.learning_rate)\n",
    "            \n",
    "            # learning rate decay\n",
    "            if epoch >= self.niter / 2.0:\n",
    "                lr_decay = (self.niter - epoch) / (self.niter / 2.0)\n",
    "                lr = lr * lr_decay\n",
    "            \n",
    "            for idx in range(0, batch_idxs):\n",
    "                self.count += 1\n",
    "\n",
    "                # Flip the batch with 0.5 prob to increase training data\n",
    "                if random.uniform(0, 1) < 0.5:\n",
    "                    flip = True\n",
    "                else:\n",
    "                    flip = False\n",
    "                \n",
    "                batch_files = data[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "                batch_inputs = [get_image(batch_file[0], flip) for batch_file in batch_files]\n",
    "                batch_target = [get_image(batch_file[1], flip) for batch_file in batch_files]\n",
    "                batch_inputs_labels = [get_label(batch_file[0], self.data_label_vector_size) for batch_file in batch_files]\n",
    "                batch_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "                batch_alpha = np.random.uniform(low=0., high=1.0, size=[self.batch_size, 1]).astype(np.float32)\n",
    "                                \n",
    "                # feed list \n",
    "                D_fetches = [self.opt_D, self.L_D, self.L_D_cls]\n",
    "                G_fetches = [self.opt_G, self.L_G, self.L_G_cls, self.L_G_recon, self.wd]\n",
    "                feed_dict = {self.x: batch_inputs, self.x_c: batch_inputs_labels, \n",
    "                             self.target: batch_target, self.target_c: batch_target_labels, \n",
    "                             self.alpha: batch_alpha,\n",
    "                             self.lr: lr}\n",
    "                \n",
    "\n",
    "                # run tensorflow\n",
    "                for i in range(5):\n",
    "                    _, d_loss, d_loss_cls = self.sess.run(D_fetches, feed_dict=feed_dict)\n",
    "                    \n",
    "                _, g_loss, g_loss_cls, g_loss_recon, wd = self.sess.run(G_fetches, feed_dict=feed_dict)\n",
    "                  \n",
    "                if self.count % 100 == 1:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, \"\n",
    "                          \"d_loss: %.6f, d_loss_cls: %.6f, g_loss: %.6f, g_loss_cls: %.6f, g_loss_recon: %.6f, \"\n",
    "                          \"wd: %.6f\"\n",
    "                          % (epoch, idx, batch_idxs, time.time() - start_time,\n",
    "                             d_loss, d_loss_cls, g_loss, g_loss_cls, g_loss_recon, wd))\n",
    "\n",
    "                # write train summary\n",
    "                summary = self.sess.run(self.merged, feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary, self.count)\n",
    "\n",
    "                # Test during Training\n",
    "                if self.count % self.niter_snapshot == (self.niter_snapshot - 1):\n",
    "                    # save & test\n",
    "                    self.saver.save(self.sess, self.ckpt_model_name, global_step=self.count, write_meta_graph=False)\n",
    "                    self.test_expr(train_flag)\n",
    "                    self.test_celebra(train_flag)\n",
    "\n",
    "    def test_celebra(self, train_flag=True):\n",
    "        print('Test Sample Generation...')\n",
    "        # generate output\n",
    "        img_num = 8*8\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        in_img_num = output_f\n",
    "        img_size = self.data_size\n",
    "        gen_img_num = img_num - output_f\n",
    "        label_size = self.data_label_vector_size\n",
    "        \n",
    "        # load data test\n",
    "        data_path = \"./Face_data/Celeba/dataset_128x128\"\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # shuffle test data\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "\n",
    "        test_files = data[0: output_f]\n",
    "        test_data = [get_image(test_file) for test_file in test_files]\n",
    "        test_data = np.repeat(test_data, [label_size]*in_img_num, axis=0)\n",
    "        test_data_o = [scm.imread(test_file) for test_file in test_files]\n",
    "        \n",
    "        # get one-hot labels\n",
    "        int_labels = list(range(label_size))\n",
    "        one_hot = np.zeros((label_size, label_size))\n",
    "        one_hot[np.arange(label_size), int_labels] = 1\n",
    "        target_labels = np.tile(one_hot, (output_f, 1))\n",
    "        \n",
    "        \n",
    "        output_gen = (self.sess.run(self.G_test, feed_dict={self.x: test_data, \n",
    "                                                            self.target_c: target_labels}))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(gen_img_num)]\n",
    "\n",
    "        for i in range(output_f):\n",
    "            for j in range(output_f):\n",
    "                if j == 0:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_data_o[i]\n",
    "                else:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[(j-1) + (i * int(output_f-1))]\n",
    "\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_celebra_output.bmp', im_output_gen)\n",
    "        \n",
    "    def test_expr(self, train_flag=True):\n",
    "        print('Train Sample Generation...')\n",
    "        # generate output\n",
    "        img_num =  36 #self.batch_size\n",
    "        display_img_num = int(img_num / 3)\n",
    "        img_size = self.data_size\n",
    "\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        \n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # Test data shuffle\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        \n",
    "        batch_files = data[0: display_img_num]\n",
    "        test_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "        test_inputs_o = [scm.imread((batch_file[0])) for batch_file in batch_files]\n",
    "        test_targets = [scm.imread((batch_file[1])) for batch_file in batch_files]\n",
    "        test_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "        output_gen = (self.sess.run(self.G_test, feed_dict={self.x: test_inputs, \n",
    "                                                            self.target_c: test_target_labels}))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(display_img_num)]\n",
    "\n",
    "        for i in range(output_f): # row\n",
    "            for j in range(output_f): # col\n",
    "                if j % 3 == 0: # input img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_inputs_o[int(j / 3) + (i * int(output_f / 3))]\n",
    "                elif j % 3 == 1: # output img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[int(j / 3) + (i * int(output_f / 3))]\n",
    "                else: # target img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_targets[int(j / 3) + (i * int(output_f / 3))]\n",
    "                   \n",
    "\n",
    "        labels = np.argmax(test_target_labels, axis=1)\n",
    "        label_string = ''.join(str(int(l)) for l in labels)\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_' + label_string \n",
    "                   + '_expr_output.bmp', im_output_gen)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StarGAN(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "\n",
    "\n",
    "    def generator(self, x, c, reuse=None):\n",
    "        with tf.variable_scope('generator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = 64\n",
    "            image_size = self.data_size\n",
    "            c_num = self.data_label_vector_size\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = tf.concat([x, tf.tile(tf.reshape(c, [-1, 1, 1, get_shape_c(c)[-1]]),\\\n",
    "                                      [1, x.get_shape().as_list()[1], x.get_shape().as_list()[2], 1])],\\\n",
    "                          axis=3)\n",
    "            \n",
    "            # Down-sampling\n",
    "            x = conv(x, [7, 7, 3+c_num, f], stride=1, padding=p, name='ds_1')\n",
    "            x = instance_norm(x, 'in_ds_1')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [4, 4, f, f*2], stride=2, padding=p, name='ds_2')\n",
    "            x = instance_norm(x, 'in_ds_2')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [4, 4, f*2, f*4], stride=2, padding=p, name='ds_3')\n",
    "            x = instance_norm(x, 'in_ds_3')\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Bottleneck\n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_1a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_1a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_1b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_1b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_2a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_2a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_2b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_2b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_3a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_3a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_3b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_3b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_4a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_4a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_4b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_4b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_5a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_5a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_5b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_5b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x_r = conv(x, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_6a')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_6a')\n",
    "            x_r = tf.nn.relu(x_r)\n",
    "            x_r = conv(x_r, [3, 3, f*4, f*4], stride=1, padding=p, name='bneck_6b')\n",
    "            x_r = instance_norm(x_r, 'in_bneck_6b')\n",
    "            x = x + x_r\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Up-sampling\n",
    "            x = deconv(x, [4, 4, f*4, f*2], stride=2, padding=p, name='us_1')\n",
    "            x = instance_norm(x, 'in_us_1')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = deconv(x, [4, 4, f*2, f], stride=2, padding=p, name='us_2')\n",
    "            x = instance_norm(x, 'in_us_2')\n",
    "            x = tf.nn.relu(x)\n",
    "            x = conv(x, [7, 7, f, 3], stride=1, padding=p, name='us_3')\n",
    "\n",
    "            x = tf.nn.tanh(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def discriminator(self, x, reuse=None):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = 64\n",
    "            f_max = f*8\n",
    "            image_size = self.data_size\n",
    "            k_size = int(image_size / np.power(2, 5))            \n",
    "            c_num = self.data_label_vector_size\n",
    "            p = \"SAME\"\n",
    "            \n",
    "            x = conv(x, [4, 4, 3, f], stride=2, padding=p, name='conv_1')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f, f*2], stride=2, padding=p, name='conv_2')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*2, f*4], stride=2, padding=p, name='conv_3')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*4, f*8], stride=2, padding=p, name='conv_4')\n",
    "            x = lrelu(x)\n",
    "            x = conv(x, [4, 4, f*8, f*16], stride=2, padding=p, name='conv_5')\n",
    "            x = lrelu(x)\n",
    "            \n",
    "            if image_size == 128:\n",
    "                x = conv(x, [4, 4, f*16, f*32], stride=2, padding=p, name='conv_6')\n",
    "                x = lrelu(x)\n",
    "                f_max = f_max * 2\n",
    "                k_size = int(k_size / 2)\n",
    "                \n",
    "            out_src = conv(x, [3, 3, f_max, 1], stride=1, padding=p, name='conv_out_src')\n",
    "            out_cls = conv(x, [k_size, k_size, f_max, c_num], stride=1, name='conv_out_cls')\n",
    "        \n",
    "        return out_src, tf.squeeze(out_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle ....\n",
      "Shuffle Done\n",
      "Epoch: [ 0] [   0/1776] time: 4.3352, d_loss: 7.256486, d_loss_cls: 0.471521, g_loss: 9.668207, g_loss_cls: 0.422335, g_loss_recon: 0.589320, wd: 2.402757\n",
      "Epoch: [ 0] [ 100/1776] time: 320.6996, d_loss: -4.634955, d_loss_cls: 0.213228, g_loss: 10.831045, g_loss_cls: 0.614057, g_loss_recon: 0.207274, wd: 7.096758\n",
      "Epoch: [ 0] [ 200/1776] time: 640.3687, d_loss: -3.512026, d_loss_cls: 0.063718, g_loss: 7.494265, g_loss_cls: 0.216107, g_loss_recon: 0.174221, wd: 4.839296\n",
      "Epoch: [ 0] [ 300/1776] time: 960.8502, d_loss: -4.421745, d_loss_cls: 0.016681, g_loss: 2.445868, g_loss_cls: 0.226492, g_loss_recon: 0.164453, wd: 5.408619\n",
      "Epoch: [ 0] [ 400/1776] time: 1280.7647, d_loss: -4.254221, d_loss_cls: 0.217345, g_loss: 6.779167, g_loss_cls: 0.084539, g_loss_recon: 0.167296, wd: 6.123425\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 0] [ 500/1776] time: 1601.9002, d_loss: -4.483073, d_loss_cls: 0.018882, g_loss: 5.330348, g_loss_cls: 0.083865, g_loss_recon: 0.163754, wd: 5.675584\n",
      "Epoch: [ 0] [ 600/1776] time: 1929.2781, d_loss: -4.343927, d_loss_cls: 0.008526, g_loss: 8.077256, g_loss_cls: 0.085274, g_loss_recon: 0.145818, wd: 5.364122\n",
      "Epoch: [ 0] [ 700/1776] time: 2250.3797, d_loss: -4.845246, d_loss_cls: 0.007405, g_loss: 6.016008, g_loss_cls: 0.017619, g_loss_recon: 0.141885, wd: 5.771045\n",
      "Epoch: [ 0] [ 800/1776] time: 2570.1677, d_loss: -3.595556, d_loss_cls: 0.009733, g_loss: 3.204299, g_loss_cls: 0.036708, g_loss_recon: 0.131826, wd: 4.365256\n",
      "Epoch: [ 0] [ 900/1776] time: 2890.2279, d_loss: -4.964317, d_loss_cls: 0.017907, g_loss: 3.257511, g_loss_cls: 0.051264, g_loss_recon: 0.134407, wd: 6.161693\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 0] [1000/1776] time: 3211.4434, d_loss: -3.706058, d_loss_cls: 0.004569, g_loss: 0.066141, g_loss_cls: 0.002578, g_loss_recon: 0.124960, wd: 4.530795\n",
      "Epoch: [ 0] [1100/1776] time: 3529.7827, d_loss: -2.869257, d_loss_cls: 0.005362, g_loss: -0.907248, g_loss_cls: 0.009440, g_loss_recon: 0.117112, wd: 3.457504\n",
      "Epoch: [ 0] [1200/1776] time: 3849.1364, d_loss: -3.117813, d_loss_cls: 0.010993, g_loss: 1.664732, g_loss_cls: 0.055891, g_loss_recon: 0.119874, wd: 3.729258\n",
      "Epoch: [ 0] [1300/1776] time: 4167.4548, d_loss: -3.330488, d_loss_cls: 0.034033, g_loss: -3.129886, g_loss_cls: 0.099906, g_loss_recon: 0.134281, wd: 4.243461\n",
      "Epoch: [ 0] [1400/1776] time: 4486.7686, d_loss: -2.704793, d_loss_cls: 0.013693, g_loss: 2.476733, g_loss_cls: 0.018211, g_loss_recon: 0.116048, wd: 3.284353\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 0] [1500/1776] time: 4805.2213, d_loss: -2.425159, d_loss_cls: 0.004218, g_loss: 2.630053, g_loss_cls: 0.006668, g_loss_recon: 0.116249, wd: 2.888022\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [ 224/1776] time: 6420.3603, d_loss: -2.701343, d_loss_cls: 0.004131, g_loss: 0.417173, g_loss_cls: 0.037121, g_loss_recon: 0.098321, wd: 3.146180\n",
      "Epoch: [ 1] [ 324/1776] time: 6735.7037, d_loss: -2.895727, d_loss_cls: 0.002271, g_loss: 3.681328, g_loss_cls: 0.002807, g_loss_recon: 0.112007, wd: 3.544232\n",
      "Epoch: [ 1] [ 424/1776] time: 7050.7923, d_loss: -3.255669, d_loss_cls: 0.003009, g_loss: -2.280639, g_loss_cls: 0.038391, g_loss_recon: 0.111511, wd: 3.890403\n",
      "Epoch: [ 1] [ 524/1776] time: 7365.6832, d_loss: -2.563163, d_loss_cls: 0.002631, g_loss: -0.016742, g_loss_cls: 0.001355, g_loss_recon: 0.111975, wd: 3.040351\n",
      "Epoch: [ 1] [ 624/1776] time: 7680.8804, d_loss: -2.802379, d_loss_cls: 0.003463, g_loss: 0.749199, g_loss_cls: 0.014568, g_loss_recon: 0.108163, wd: 3.302621\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [ 724/1776] time: 7997.0734, d_loss: -2.652231, d_loss_cls: 0.008652, g_loss: 0.273488, g_loss_cls: 0.004582, g_loss_recon: 0.120733, wd: 3.263451\n",
      "Epoch: [ 1] [ 824/1776] time: 8312.1690, d_loss: -3.331519, d_loss_cls: 0.018596, g_loss: 0.022232, g_loss_cls: 0.008389, g_loss_recon: 0.104390, wd: 4.178719\n",
      "Epoch: [ 1] [ 924/1776] time: 8627.2902, d_loss: -2.465484, d_loss_cls: 0.003651, g_loss: 1.166683, g_loss_cls: 0.090950, g_loss_recon: 0.104957, wd: 3.072254\n",
      "Epoch: [ 1] [1024/1776] time: 8942.4147, d_loss: -3.725647, d_loss_cls: 0.003091, g_loss: 4.065336, g_loss_cls: 0.021153, g_loss_recon: 0.126097, wd: 4.531466\n",
      "Epoch: [ 1] [1124/1776] time: 9257.5919, d_loss: -3.086570, d_loss_cls: 0.000881, g_loss: 6.020446, g_loss_cls: 0.002447, g_loss_recon: 0.111166, wd: 3.872319\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [1224/1776] time: 9573.7241, d_loss: -3.011375, d_loss_cls: 0.001115, g_loss: -7.322902, g_loss_cls: 0.005369, g_loss_recon: 0.116925, wd: 3.589298\n",
      "Epoch: [ 1] [1324/1776] time: 9888.8360, d_loss: -3.492120, d_loss_cls: 0.002289, g_loss: -4.235898, g_loss_cls: 0.008353, g_loss_recon: 0.101436, wd: 4.278416\n",
      "Epoch: [ 1] [1424/1776] time: 10203.9947, d_loss: -2.694347, d_loss_cls: 0.003400, g_loss: 3.411184, g_loss_cls: 0.001220, g_loss_recon: 0.107797, wd: 3.293787\n",
      "Epoch: [ 1] [1524/1776] time: 10519.0252, d_loss: -4.308805, d_loss_cls: 0.002980, g_loss: 4.603341, g_loss_cls: 0.010260, g_loss_recon: 0.114971, wd: 5.344808\n",
      "Epoch: [ 1] [1624/1776] time: 10834.0987, d_loss: -3.244344, d_loss_cls: 0.000952, g_loss: 1.370198, g_loss_cls: 0.000554, g_loss_recon: 0.106537, wd: 3.895466\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [1724/1776] time: 11150.1933, d_loss: -3.145636, d_loss_cls: 0.002346, g_loss: -0.594218, g_loss_cls: 0.031122, g_loss_recon: 0.106169, wd: 3.916348\n",
      "Epoch: [ 2] [  48/1776] time: 11465.3184, d_loss: -2.408569, d_loss_cls: 0.001450, g_loss: -1.594447, g_loss_cls: 0.007246, g_loss_recon: 0.106976, wd: 2.862345\n",
      "Epoch: [ 2] [ 148/1776] time: 11780.4425, d_loss: -2.736395, d_loss_cls: 0.000642, g_loss: 0.611857, g_loss_cls: 0.002473, g_loss_recon: 0.093483, wd: 3.367496\n",
      "Epoch: [ 2] [ 248/1776] time: 12095.7308, d_loss: -3.476211, d_loss_cls: 0.001903, g_loss: 5.385262, g_loss_cls: 0.002595, g_loss_recon: 0.104296, wd: 4.299658\n",
      "Epoch: [ 2] [ 348/1776] time: 12410.8574, d_loss: -3.878980, d_loss_cls: 0.000882, g_loss: -2.447505, g_loss_cls: 0.006681, g_loss_recon: 0.110959, wd: 4.681179\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 2] [ 448/1776] time: 12726.9464, d_loss: -3.152807, d_loss_cls: 0.001440, g_loss: 1.508514, g_loss_cls: 0.018782, g_loss_recon: 0.107466, wd: 3.831089\n",
      "Epoch: [ 2] [ 548/1776] time: 13041.9565, d_loss: -2.588090, d_loss_cls: 0.001966, g_loss: 2.979559, g_loss_cls: 0.004458, g_loss_recon: 0.100898, wd: 3.100428\n",
      "Epoch: [ 2] [ 648/1776] time: 13357.2877, d_loss: -3.504827, d_loss_cls: 0.001097, g_loss: -8.210339, g_loss_cls: 0.014602, g_loss_recon: 0.107336, wd: 4.106574\n",
      "Epoch: [ 2] [ 748/1776] time: 13672.3065, d_loss: -2.368163, d_loss_cls: 0.002045, g_loss: 2.888353, g_loss_cls: 0.001361, g_loss_recon: 0.094423, wd: 2.857038\n",
      "Epoch: [ 2] [ 848/1776] time: 13987.3944, d_loss: -2.692377, d_loss_cls: 0.000634, g_loss: 1.107929, g_loss_cls: 0.000977, g_loss_recon: 0.109612, wd: 3.337496\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 2] [ 948/1776] time: 14303.4371, d_loss: -3.121274, d_loss_cls: 0.000896, g_loss: 0.734282, g_loss_cls: 0.005593, g_loss_recon: 0.103518, wd: 3.830787\n",
      "Epoch: [ 2] [1048/1776] time: 14618.5206, d_loss: -2.014832, d_loss_cls: 0.001191, g_loss: 0.150482, g_loss_cls: 0.008218, g_loss_recon: 0.111405, wd: 2.490995\n",
      "Epoch: [ 2] [1148/1776] time: 14933.8797, d_loss: -2.275084, d_loss_cls: 0.002021, g_loss: -0.742258, g_loss_cls: 0.003212, g_loss_recon: 0.111053, wd: 2.879627\n",
      "Epoch: [ 2] [1248/1776] time: 15249.0421, d_loss: -3.313339, d_loss_cls: 0.001070, g_loss: 10.063492, g_loss_cls: 0.011112, g_loss_recon: 0.097238, wd: 4.025709\n",
      "Epoch: [ 2] [1348/1776] time: 15564.0926, d_loss: -3.774217, d_loss_cls: 0.000595, g_loss: 5.166356, g_loss_cls: 0.006552, g_loss_recon: 0.106235, wd: 4.617189\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 2] [1448/1776] time: 15880.3054, d_loss: -2.214432, d_loss_cls: 0.001735, g_loss: 0.566824, g_loss_cls: 0.000511, g_loss_recon: 0.099977, wd: 2.667269\n",
      "Epoch: [ 2] [1548/1776] time: 16195.6645, d_loss: -2.408859, d_loss_cls: 0.001090, g_loss: 3.138040, g_loss_cls: 0.000852, g_loss_recon: 0.107821, wd: 3.027951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 2] [1648/1776] time: 16510.9367, d_loss: -2.661995, d_loss_cls: 0.000996, g_loss: 1.464761, g_loss_cls: 0.000027, g_loss_recon: 0.103259, wd: 3.119321\n",
      "Epoch: [ 2] [1748/1776] time: 16826.0263, d_loss: -3.140516, d_loss_cls: 0.004989, g_loss: 2.795908, g_loss_cls: 0.002689, g_loss_recon: 0.102642, wd: 3.978564\n",
      "Epoch: [ 3] [  72/1776] time: 17141.2712, d_loss: -3.317130, d_loss_cls: 0.000711, g_loss: 1.658351, g_loss_cls: 0.000251, g_loss_recon: 0.096602, wd: 4.043356\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 3] [ 172/1776] time: 17457.3839, d_loss: -2.040513, d_loss_cls: 0.000920, g_loss: 3.550213, g_loss_cls: 0.028849, g_loss_recon: 0.123141, wd: 2.689764\n",
      "Epoch: [ 3] [ 272/1776] time: 17772.4587, d_loss: -2.819413, d_loss_cls: 0.000465, g_loss: 9.273970, g_loss_cls: 0.014537, g_loss_recon: 0.093240, wd: 3.515535\n",
      "Epoch: [ 3] [ 372/1776] time: 18087.5173, d_loss: -2.072664, d_loss_cls: 0.000681, g_loss: 3.858003, g_loss_cls: 0.073164, g_loss_recon: 0.086322, wd: 2.570980\n",
      "Epoch: [ 3] [ 472/1776] time: 18402.7545, d_loss: -2.598236, d_loss_cls: 0.000566, g_loss: 3.487851, g_loss_cls: 0.000273, g_loss_recon: 0.101415, wd: 3.189796\n",
      "Epoch: [ 3] [ 572/1776] time: 18717.9044, d_loss: -2.098768, d_loss_cls: 0.001039, g_loss: 1.103065, g_loss_cls: 0.021922, g_loss_recon: 0.093427, wd: 2.525246\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 3] [ 672/1776] time: 19034.0248, d_loss: -2.776340, d_loss_cls: 0.000759, g_loss: 3.506500, g_loss_cls: 0.000696, g_loss_recon: 0.095279, wd: 3.384500\n",
      "Epoch: [ 3] [ 772/1776] time: 19349.1374, d_loss: -2.486618, d_loss_cls: 0.001120, g_loss: 6.494453, g_loss_cls: 0.000296, g_loss_recon: 0.097561, wd: 3.080366\n",
      "Epoch: [ 3] [ 872/1776] time: 19664.3006, d_loss: -2.835453, d_loss_cls: 0.000272, g_loss: 3.494157, g_loss_cls: 0.003962, g_loss_recon: 0.090383, wd: 3.453667\n",
      "Epoch: [ 3] [ 972/1776] time: 19979.4466, d_loss: -3.396545, d_loss_cls: 0.000643, g_loss: 3.524588, g_loss_cls: 0.001897, g_loss_recon: 0.106020, wd: 4.089166\n",
      "Epoch: [ 3] [1072/1776] time: 20294.6985, d_loss: -2.375118, d_loss_cls: 0.001530, g_loss: 3.669178, g_loss_cls: 0.000052, g_loss_recon: 0.094791, wd: 3.060909\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 3] [1172/1776] time: 20610.7758, d_loss: -2.219165, d_loss_cls: 0.001774, g_loss: 1.951303, g_loss_cls: 0.010758, g_loss_recon: 0.094847, wd: 2.784533\n",
      "Epoch: [ 3] [1272/1776] time: 20925.8657, d_loss: -2.919837, d_loss_cls: 0.000706, g_loss: 4.351729, g_loss_cls: 0.024506, g_loss_recon: 0.094862, wd: 3.580836\n",
      "Epoch: [ 3] [1372/1776] time: 21240.9157, d_loss: -3.308076, d_loss_cls: 0.000204, g_loss: 10.923988, g_loss_cls: 0.001833, g_loss_recon: 0.097011, wd: 4.005664\n",
      "Epoch: [ 3] [1472/1776] time: 21556.0399, d_loss: -2.582110, d_loss_cls: 0.002251, g_loss: 2.128980, g_loss_cls: 0.042586, g_loss_recon: 0.097431, wd: 3.333033\n",
      "Epoch: [ 3] [1572/1776] time: 21871.1458, d_loss: -2.077844, d_loss_cls: 0.000580, g_loss: 4.122346, g_loss_cls: 0.001199, g_loss_recon: 0.087005, wd: 2.486687\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 3] [1672/1776] time: 22187.2162, d_loss: -2.745816, d_loss_cls: 0.000041, g_loss: 3.675989, g_loss_cls: 0.000928, g_loss_recon: 0.103336, wd: 3.508079\n",
      "Epoch: [ 3] [1772/1776] time: 22502.4118, d_loss: -2.646092, d_loss_cls: 0.000520, g_loss: 1.660955, g_loss_cls: 0.000090, g_loss_recon: 0.092708, wd: 3.207308\n",
      "Epoch: [ 4] [  96/1776] time: 22817.4706, d_loss: -3.703101, d_loss_cls: 0.000505, g_loss: -6.121182, g_loss_cls: 0.020554, g_loss_recon: 0.107947, wd: 4.610034\n",
      "Epoch: [ 4] [ 196/1776] time: 23132.7466, d_loss: -2.672616, d_loss_cls: 0.000837, g_loss: 6.189439, g_loss_cls: 0.032699, g_loss_recon: 0.097538, wd: 3.408771\n",
      "Epoch: [ 4] [ 296/1776] time: 23447.8711, d_loss: -3.204043, d_loss_cls: 0.001118, g_loss: 4.318848, g_loss_cls: 0.007248, g_loss_recon: 0.088725, wd: 3.900694\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 4] [ 396/1776] time: 23763.9984, d_loss: -2.608704, d_loss_cls: 0.001478, g_loss: 2.332928, g_loss_cls: 0.033222, g_loss_recon: 0.092417, wd: 3.221501\n",
      "Epoch: [ 4] [ 496/1776] time: 24079.0498, d_loss: -2.699944, d_loss_cls: 0.000374, g_loss: 0.615411, g_loss_cls: 0.000318, g_loss_recon: 0.091181, wd: 3.331059\n",
      "Epoch: [ 4] [ 596/1776] time: 24394.0677, d_loss: -2.895920, d_loss_cls: 0.000340, g_loss: 7.712347, g_loss_cls: 0.000045, g_loss_recon: 0.097207, wd: 3.541452\n",
      "Epoch: [ 4] [ 696/1776] time: 24709.0677, d_loss: -2.337537, d_loss_cls: 0.000479, g_loss: -0.236549, g_loss_cls: 0.001207, g_loss_recon: 0.083763, wd: 2.872584\n",
      "Epoch: [ 4] [ 796/1776] time: 25024.2173, d_loss: -2.635646, d_loss_cls: 0.000065, g_loss: 3.401786, g_loss_cls: 0.000023, g_loss_recon: 0.088725, wd: 3.169021\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 4] [ 896/1776] time: 25340.2623, d_loss: -2.900823, d_loss_cls: 0.000423, g_loss: 0.971147, g_loss_cls: 0.000086, g_loss_recon: 0.099888, wd: 3.580641\n",
      "Epoch: [ 4] [ 996/1776] time: 25655.3470, d_loss: -2.629530, d_loss_cls: 0.000651, g_loss: 6.792696, g_loss_cls: 0.021408, g_loss_recon: 0.087315, wd: 3.315042\n",
      "Epoch: [ 4] [1096/1776] time: 25970.4764, d_loss: -2.705858, d_loss_cls: 0.000259, g_loss: 0.819640, g_loss_cls: 0.018146, g_loss_recon: 0.089006, wd: 3.408921\n",
      "Epoch: [ 4] [1196/1776] time: 26285.6566, d_loss: -2.389523, d_loss_cls: 0.000275, g_loss: -1.367727, g_loss_cls: 0.014814, g_loss_recon: 0.094129, wd: 3.152648\n",
      "Epoch: [ 4] [1296/1776] time: 26600.8072, d_loss: -1.763470, d_loss_cls: 0.002045, g_loss: 11.432278, g_loss_cls: 0.012412, g_loss_recon: 0.086146, wd: 2.302269\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 4] [1396/1776] time: 26916.9332, d_loss: -3.053266, d_loss_cls: 0.000752, g_loss: 5.665977, g_loss_cls: 0.000270, g_loss_recon: 0.090227, wd: 3.736027\n",
      "Epoch: [ 4] [1496/1776] time: 27232.0509, d_loss: -2.810881, d_loss_cls: 0.000402, g_loss: 0.893875, g_loss_cls: 0.001641, g_loss_recon: 0.092278, wd: 3.643689\n",
      "Epoch: [ 4] [1596/1776] time: 27547.1576, d_loss: -2.184460, d_loss_cls: 0.001130, g_loss: 4.206386, g_loss_cls: 0.000234, g_loss_recon: 0.081649, wd: 2.682402\n",
      "Epoch: [ 4] [1696/1776] time: 27862.3927, d_loss: -2.611480, d_loss_cls: 0.000430, g_loss: -4.165934, g_loss_cls: 0.016910, g_loss_recon: 0.085939, wd: 3.204546\n",
      "Epoch: [ 5] [  20/1776] time: 28177.5402, d_loss: -2.222412, d_loss_cls: 0.001038, g_loss: 6.521973, g_loss_cls: 0.000414, g_loss_recon: 0.091497, wd: 2.659846\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 5] [ 120/1776] time: 28493.6071, d_loss: -3.303906, d_loss_cls: 0.000378, g_loss: 9.756536, g_loss_cls: 0.000186, g_loss_recon: 0.087709, wd: 4.010776\n",
      "Epoch: [ 5] [ 220/1776] time: 28808.5515, d_loss: -3.081501, d_loss_cls: 0.000746, g_loss: 2.016884, g_loss_cls: 0.000902, g_loss_recon: 0.088840, wd: 3.756278\n",
      "Epoch: [ 5] [ 320/1776] time: 29123.3350, d_loss: -3.356807, d_loss_cls: 0.000715, g_loss: 3.141629, g_loss_cls: 0.001974, g_loss_recon: 0.090735, wd: 4.203463\n",
      "Epoch: [ 5] [ 420/1776] time: 29438.2907, d_loss: -2.914733, d_loss_cls: 0.001749, g_loss: 8.412345, g_loss_cls: 0.293292, g_loss_recon: 0.116905, wd: 3.485758\n"
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"StarGAN_Face_3_\"\n",
    "train_flag = True\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = StarGAN(sess, project_name)\n",
    "\n",
    "        # TRAIN / TEST\n",
    "        if train_flag:\n",
    "            model.train(train_flag)\n",
    "        else:\n",
    "            model.test(train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
